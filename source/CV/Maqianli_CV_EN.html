<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<style>
@media print {
      @page {
            size: A4 portrait;
            /* zoom: 60%; */
      }
      body {
            background-color: white;
      }
}
body {
      width: 21cm;
      /* width: 29cm; */
      display: block;
      margin: 0 auto;
      padding: 10px 25px;
      margin-bottom: 0.5cm;
      Box-shadow: 0 0 0.5cm rgba(0,0.5);
      border: 1px;
      overflow-y: scroll;
      Box-sizing: border-Box;
      font-family:'Times New Roman', Times, serif;
}
*{
      margin: 1px;
}
.experience {
      display:flex;
      flex-direction:row;
      justify-content:space-between;
      font-size: 15px;
}
.subexperience {
      display:flex;
      flex-direction:row;
      justify-content:space-between;
      font-weight: bold;
      font-size: 13px;
}
.name{
      text-align:center;
}
.contact{
      text-align:center;
      font-size: 12px;
}
.li{
      display:flex;
      flex-direction:row;
      justify-content:flex-start;
      font-size: 13px;
      border: 0px;
      margin: 0px;
}
p{
      font-size: 13px;
      text-indent: 2em; /* 首行缩进两格 */
}
</style>
<script src="https://kit.fontawesome.com/ef536a7fe9.js" crossorigin="anonymous"></script>
</head>

<body>
<!--马千里-->
<div class="name">
<h2>Qianli Ma</h2>
</div>

<!--联系方式-->
<div class="contact">
      <i class="fa-solid fa-phone" style="color: #000000"><b> (+86)17855801919</b></i> 
      <i class="fa-brands fa-weixin" style="color: #000000"><b> Fazzie17855801919</b></i> 
      <a href="mailto:1240419984@qq.com"><i class="fa-solid fa-envelope" style="color: #000000"><b> fazzie@qq.com</b></i></a>

      <!-- <img alt="mobile" align="center" src="https://img.shields.io/badge/mobile-(+86)17855801919-25D366?logo=whatsapp" />  -->
      <!-- <img alt="wechat" align="center" src="https://img.shields.io/badge/wechat-Fazzie17855801919-07C160?logo=wechat&amp" /> -->
      <!-- <a href="https://fazzie-key.cool/CV/Maqianli_CV.html"><img alt="CV" align="center" src="https://img.shields.io/badge/CV-View on Browser-4285F4?logo=Google Chrome" /></a>  -->
<br>
      <a href="https://github.com/Fazziekey"><i class="fa-brands fa-github" style="color: #000000"><b> Fazziekey 370+ followers</b></i></a>
      <a href="https://fazzie-key.cool"><i class="fa-solid fa-blog" style="color: #000000"><b> fazzie-key.cool</b></i></a> 
      <a href="https://www.linkedin.com/in/qianli-ma-fazzie-313272207/"><i class="fa-brands fa-linkedin" style="color: #000000"><b> QianliMa</b></i></a>
      <a href="https://www.zhihu.com/people/fazzie"><i class="fa-brands fa-zhihu" style="color: #000000"><b>fazzie</b></i></a>
      <!-- <a href="mailto:1240419984@qq.com"><img alt="mail" align="center" src="https://img.shields.io/badge/mail-fazzie@qq.com-F06B66?logo=mailgun" /></a>  -->
      <!-- <a href="https://github.com/Fazziekey"><img alt="github" align="center" src="https://img.shields.io/badge/github-Fazziekey-181717?logo=github" /></a>  -->
      <!-- <a href="https://fazzie-key.cool/nn/"><img alt="blog" align="center" src="https://img.shields.io/badge/blog-fazzie--key.cool-FFA500?logo=rss" /></a> -->
      <!-- <a href="https://www.linkedin.com/in/qianli-ma-fazzie-313272207/"><img alt="linkin" align="center" src="https://img.shields.io/badge/Linkin-Qianli Ma-0A66C2?logo=linkedin"/></a> -->
</div>
<!--联系方式-->

<p>
I possess extensive engineering experience across both Machine Learning Systems (MLSys) and Large Language Model Algorithms. 
My goal is to advance next-generation AGI systems in order to create larger and better models.
I am deeply passionate about the latest technologies and actively contribute to the open-source community as a core contributor to several popular
open-source AI projects.
</p>


<!--教育背景-->
<div>

<h4>EDUCATION BACKGROUND</h4>
<HR style="FILTER: alpha(opacity=100,finishopacity=0,style=1)" width="87%" color=#1d1a1a SIZE=1>

<div class="experience"> 
<div><b>National University of Singapore</b></div>
<div>2022.8 - 2024.1</div>   
</div>
<div class="subexperience">   
      <div>&nbsp<i>Master of Computer Science</i></div>
      <div>Singapore</div>   
</div>
<div class="li">
<ul>
      <li>Focus on Machine Learning System in HPC-AI lab</li>
      <li>Dissertation: Maximizing parallelism in Distribted training for diffusion model</li>
</ul>
</div>
           
<div class="experience"> 
<div><b>Zhejiang University </b></div>
<div>2018.9 - 2022.7</div>   
</div>
<div class="subexperience">   
      <div>&nbsp<i>B.Eng&nbspin Electronic Science and Technology</i></div>
      <div>Hangzhou, China </div>   
</div>

<div class="li">
      <ul>
            <li>Shannon elite class of Information Science and Electronic Engineer College</li>
            <li>Minor in Intensive training Program of Innovation and Entrepreneeyrship(ITP) in Chu Kechen Honors College</li>
            <li>Honors: First Class Scholarship of Zhejiang University, Excellent Graduate of Zhejiang University</li>
      </ul>
<br>
</p>
</div>

<div class="li">
<b>Other Honors</b>: Second prize in the National High school Mathematics Competition(2017)
</div>
<!--教育背景over-->

<!--工作与实习经历-->
<div>

<h4>WORK EXPERIENCE</h4>
<HR style="FILTER: alpha(opacity=100,finishopacity=0,style=1)" width="87%" color=#1d1a1a SIZE=1>

<div class="experience">   
      <div><b>ByteDance</b>&nbsp&nbspSeed</div>
      <div>2023.12 - Present</div>   
</div>
      <div class="subexperience">   
            <div>&nbspSenior Research Engineer</div>
            <div>Shanghai</div>   
      </div>
<p>
      As one of the earliest members of the Seed Team, I focusing on the AI infrastructure, optimizing training performance for LLMs & multimodal understanding and generation models (with thousands of GPUs), 
      from pre-training to post-training. 
      In particular, I led a small team to develop VeOmni(an open-source multimodal training system).
      I was deeply involved in the research and development of core models such as Seed-Thinking 1.5 and UI-TARS.
</p>  

<div class="subexperience">   
      <div>&nbspProjects Highlights</div>
</div> 

<div class="li">
      <ul>
            <li><b>VeOmni</b> is a PyTorch-native training framework purpose-built for both multi-modal pre-training and post-training.
                  <a href="https://github.com/ByteDance-Seed/VeOmni"><i class="fa-brands fa-github" style="color: #000000"></i></a></li>
                  It natively supports <b>DeviceMesh</b> and <b>DTensor</b>, and integrates cutting-edge features including FSDP2, expert parallelism, sequence parallelism, and etc.
                  In Seed, Leveraged VeOmni to develop training infrastructure supporting diverse initiatives, including UI-TARS, model architecture exploration, and unified generative-understanding model research.


            <li><b>veScale</b> is a PyTorch-native LLM Training Framework with Dtenser-based ND Parallelism and Eager Mode Execution
                  <a href="https://github.com/volcengine/veScale"><i class="fa-brands fa-github" style="color: #000000"></i></a></li>
            <li><b>verl</b> is a flexible, efficient and production-ready RL training library for large language models (LLMs).<a href="https://github.com/volcengine/verl"><i class="fa-brands fa-github" style="color: #000000"></i></a></li>
            <li>
                  <b>UI-TARS</b> is an open-source multimodal agent built upon a powerful vision-language model. It is capable of effectively performing diverse tasks within virtual worlds.<a href="https://github.com/bytedance/UI-TARS/tree/main"><i class="fa-brands fa-github" style="color: #000000"></i></a>
            </li> 
      </ul>
</div>

<div class="experience">   
      <div><b>ByteDance</b>&nbsp&nbspAML</div>
      <div>2023.6 - 2023.12</div>   
      </div>
      <div class="subexperience">   
            <div>&nbsp LLMs Research Intern at Seed-Project</div>
            <div>Shanghai</div>   
      </div>
<p>
&nbspConduct research about MLsys Learning system, Process-Supervised Reward Model (PRM), 
SFT Data Selection, Agent for Data Analysis


</p>
 
      <div class="subexperience">   
      <div>&nbspProject Highlights</div>
      </div> 
      <div class="li">
      <ul>
            <li>  <b>PRM</b>: I built a complete pipeline for data processing, PRM model training, and evaluation, 
                  and proposed a heuristic greedy search algorithm based on Process-Supervised Reward Models (PRM) (HGS-PRM),
                  which uses step-level feedback from PRM to optimize the reasoning paths of large language models; compared with the Chain-of-Thought (CoT) method, 
                  this algorithm has improved the model's capabilities in mathematical reasoning and code generation.
            </li>
            <li>
                  <b>SFT Data Selection</b>: Developed DavIR, a model-centric data selection method that enables LLMs (LLaMA, Gemma) to outperform full-dataset training with only 6% of Alpaca data; extended it to DavIR-DPO,
                  boosting Zephyr-7B-SFT's alignment performance by 8% on AlpacaEval.
            </li>
            <li>
                  <b>Agent for Data Analysis</b>: built InfiAgent-DABench, a benchmark for evaluating agents on data analysis tasks. 
                  I've developed Agent infrastructure components such as LLMs API call integration, vLLM-powered inference engines, Python sandboxes, as well as model training infrastructure.
            </li>
      </ul>
      </div>
</div> 
</div>

<div class="experience">   
      <div><b>HPC-AI Technology</b></div>
      <div>2022.7 - 2023.5</div>   
      </div>
      <div class="subexperience">   
            <div>&nbsp Machine Learning System Engineeer</div>
            <div>Singapore</div>   
      </div>
      <p>
            Joined as Employee #15, completing my master's degree while supporting the company's growth from Seed to Series A.
            As a key developer on ColossalAI—the company's core training framework,
            I also led the projects including Colossal Chat and ColoDiffusion, 
            driving GitHub stars from 0 to 20k+. 
            Beyond R&D, I contributed to commercialization strategies, grew the open-source community, 
            and participated in cloud product design.
      <p>
      <div class="subexperience">   
            <div>&nbspProject Highlights</div>
            </div> 
      <div class="li">
            <ul>
                  <li>
                        <b>Colossal Chat</b><a href="https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat"><i class="fa-brands fa-github" style="color: #000000"></i></a>
                  </li>
                  <ul>
                        <li>
                              I took on the core development work of training code for <b>Coati</b> (Colossal AI talking intelligence)large language model, and designed an entire training pipeline including instruction data collection, data preprocessing, distributed training and acceleration of the model, model alignment tuning, etc. 
                              We also open-sourced the Coati7B and Coati13B large language models.
                        </li>
                        <li>
                              After open-sourcing Coati, which helped <b>ColossalAI</b>, it ranked first on the Github trending list for three consecutive days (but was eventually overtaken by <a href="https://github.com/twitter/the-algorithm"><i class="fa-brands fa-github" style="color: #000000"></i></a> <b>The Algorithm,</b> 
                              Twitter's open source project released by Elon Musk). This had a huge impact on the community and
                              caused <b>ColossalAI</b> to gain more than <b>10k</b> stars,
                              Becoming one of the fastest growing AI open source projects in the first quarter of 2023 
                              <a href="https://www.decibel.vc/content/launching-the-generative-ai-open-source-genos-index"><i class="fa-solid fa-link" style="color: #000000"></i></a>
                        </li>
                  </ul>
                  <li><b>ColossalAI</b>, A Unified Deep Learning System for Big Model Era<a href="https://github.com/hpcaitech/ColossalAI"><i class="fa-brands fa-github" style="color: #000000"></i></a></li>
                  <ul>
                        <li>
                              Developed in the core feature of <b>ColossalAI</b>, including heterogeneous memory management, pipeline parallelism, distributed model saving.
                        </li>
                        <li>
                              Participating in the development and support of <b>ColossalAI</b> as a distributed backend for PyTorch Lightning enables <b>ColossalAI</b> to integrate more easily with PyTorch Lightning
                        </li>
                  </ul>
                  <li>Leading the development of the AIGC Big Model training solution: <b>ColoDiffusion</b><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion"><i class="fa-brands fa-github" style="color: #000000"></i></a></li>
                  <ul>
                        <li>
                              As the core developer, built Diffusion training framework based on pytorch-lightning + ColosaalAI, which supports multiple training modes, which was officially reposted by Pytorch
                              <a href="https://twitter.com/PyTorch/status/1628076104626974732?t=y656lK4VtI4EC3WkgaKPbQ&s=19"><i class="fa-solid fa-link" style="color: #000000"></i></a>
                        </li>
                        <li>
                              Use zero optimizer, auto chunk, flash-attention, cpu offload and other technologies to break the memory wall, support large bacth acceleration training
                        </li>
                  </ul>
                  <li><b>Fastfold</b>(Optimizing AlphaFold Training and Inference on GPU Clusters)<a href="https://github.com/hpcaitech/ColossalAI"><i class="fa-brands fa-github" style="color: #000000"></i></a></li>
                  I Support the data pre-processing Parallel(Triple the speed) for <b>Fastfold</b> by <b>Ray</b></a>,
                              solve the core bottleneck of MSA feature search and long pre-processing time for inference training</li>

                  <li>Technology Stack:Python,C++,Cuda,Pytorch,Ray,colossal-AI, Pytorch-lightning,TensorRT,DeepSpeed,Huggingface</li>
            </ul>
</div>

<div class="experience">   
<div><b>SenseTime</b>&nbsp&nbsp Large model training</div>
<div>2021.12 - 2022.6</div>   
</div>
<div class="subexperience">   
      <div>&nbsp AI Researcher Internship</div>
      <div>Hangzhou</div>   
</div>
<p>I participated in the development of large-scale distributed machine learning training framework - <b>Sensetime Spring</b> of SenseTime, and research related to machine learning systems</p>
<div class="li">
      <ul>
            <li>Advancing large models of target detection to the ground (Vision Transformer, Swin Transformer, etc.), Support SenseTime's general detection framework - <b>POD</b> using pytorch distributed data parallel training and mixed-presicion training</li>
            <li>Involved in MLops related work, machine learning cloud platform development, supporting model lifecycle management database</li>
            <li>Technology Stack:Python,C++,Cuda,Pytorch,go,Nebula DB</li>
      </ul>
</div>

<div class="experience">   
      <div><b>Huawei 2012 Lab</b>&nbsp&nbsp Distributed Parallel Lab</div>
      <div>2021.7 - 2021.12</div>   
      </div>
      <div class="subexperience">   
            <div>&nbsp AI Engineering Internship</div>
            <div>Hangzhou</div>   
      </div>
<p>I Contributed to <b>Mindspore</b>, a full-scene deep learning framework; Developed three new features for <b>Mindspore Lite</b><p>
      
      <div class="li">
            <ul>
                  <li>Completed <b>Mindspore Lite</b> OpenGL texture converting core code, as the key feature of <b>Mindspore Lite</b> 1.6
                        <a href="https://gitee.com/Fazzie/mindspore"><i class="fa-brands fa-github" style="color: #000000"></i></a></li>
                  <li>Accomplished the OpenCL beckend support of x86 platform ofr <b>Mindspore Lite</b>, developed GPU operators of Mindspore core</li>
                  <li>Implemented and iterated the Log system of Mindspore on x86 platform based on Glog</li>
                  <li>Technology Stack:C++,OpenCL,OpenGL,Cmake,Python</li>
            </ul>
      </div>

</div>

<!--工作与实习经历over-->

<!--文章-->
<div>
<h4>PUBLICATION</h4>
      <HR style="FILTER: alpha(opacity=100,finishopacity=0,style=1)" width="87%" color=#1d1a1a SIZE=1>
      <div class="li">
            <ul>
                  <li>Hu, Xueyu., <strong>Qianli Ma</strong>., et al. (2024) "InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks." ICML, 2024 (Published)<a href="https://arxiv.org/abs/2401.05507">[Paper]</a> <a href="https://github.com/InfiAgent/InfiAgent"><i class="fa-brands fa-github" style="color: #000000"></i></a></li>
                  <li><strong>Ma, Q</strong>., Zhou, H, et al.(2023). Let's reward step by step: Step-Level reward model as the Navigators for Reasoning. <a href="https://arxiv.org/abs/2310.10080">[Paper]</a></li>
                  <li>Zhou, H., Liu, T., <strong>Ma, Q.</strong>, et al. (2023). DavIR: Data Selection via Implicit Reward for Large Language Models. ACL 2025. (Published)<a href="https://arxiv.org/abs/2310.13008">[Paper]</a></li>
                  <li>Qin, Y. ... <strong>Ma, Q.</strong>, . . .  Shi, G. (2025). UI-TARS: Pioneering Automated GUI Interaction with Native Agents.<a href="https://arxiv.org/abs/2501.12326">[Paper]</a> <a href="https://github.com/bytedance/UI-TARS?tab=readme-ov-file"><i class="fa-brands fa-github" style="color: #000000"></i></a></li>
                  <li>Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning</li>
            </ul>
</div>



</div>
<!--比赛和项目经历over-->


<!--知识储备和专业技能-->
<div>
      <h4>KNOWLEDGE & SKILLS</h4>
      <HR style="FILTER: alpha(opacity=100,finishopacity=0,style=1)" width="87%" color=#1d1a1a SIZE=1>
      
      <div class="li">
      <ul>
            <li>
                  Program Language: Python, C++/C, Golang, Javascript
            </li>
            <li>
                  MLSys Full Stack: ColossalAI, VeOmni, DeepSpeed, Ray, Megatron-LM, verl, vllm, transformers, triton
            </li>
            <li>
                  LLMs Full Stack: Pre-train, SFT, RLHF, RLVR, Vision Language Model, Omni Model, Agentic
            </li>
            <li>
                  Tools:Linux,Vim,shell,Git,Docker,Cmake 
            </li>
            </ul>
      </div>
<!--知识储备和专业技能over-->

<!--社团和组织经历-->
<div>

<h4>CLUBS & ORGANISATIONAL EXPERIENCE</h4>
<HR style="FILTER: alpha(opacity=100,finishopacity=0,style=1)" width="87%" color=#1d1a1a SIZE=1>

<div class="experience" style="font-size:11;">   
<div><b>Zhejiang University Internet Society</b>&nbsp&nbsp Technology department&nbsp AI lab</div>
<div>2021.10 - 2022.8</div>   
</div>
<div class="experience" style="font-size:11;">   
      <div><b>String Program</b>&nbsp&nbsp Technology department &nbsp Member of the machine learning subdepartment</div>
      <div>2020.7 - Present</div>   
</div>
<div class="experience" style="font-size:11;">   
      <div><b>Zhejiang University Electroacoustic Orchestra</b>&nbsp&nbsp Drummer of Six o'clock studio band</div>
      <div>2018.11 - 2021.2</div>   
</div>
</div>
<!--社团和组织经历-->

</body>
</html>