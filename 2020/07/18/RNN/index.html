<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>RNN | 摸黑干活</title><meta name="keywords" content="Deep learning,NLP"><meta name="author" content="Fazzie"><meta name="copyright" content="Fazzie"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="循环序列模型为什么选择序列模型">
<meta property="og:type" content="article">
<meta property="og:title" content="RNN">
<meta property="og:url" content="https://fazziekey.github.io/2020/07/18/RNN/index.html">
<meta property="og:site_name" content="摸黑干活">
<meta property="og:description" content="循环序列模型为什么选择序列模型">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2020-07-18T08:00:31.000Z">
<meta property="article:modified_time" content="2024-05-28T06:13:36.212Z">
<meta property="article:author" content="Fazzie">
<meta property="article:tag" content="Deep learning">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="https://fazziekey.github.io/2020/07/18/RNN/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-05-28 14:13:36'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">69</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">41</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/index.html"><i class="fa-fw fas fas fa-id-card"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-file"></i><span> CV</span><i class="fas fa-chevron-down expand hide"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/CV/Maqianli_CV.html"><i class="fa-fw fas fa-file-alt"></i><span> CN</span></a></li><li><a class="site-page" href="/CV/Maqianli_CV_EN.html"><i class="fa-fw far fa-file-alt"></i><span> EN</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/nn/"><i class="fa-fw fas fa-network-wired"></i><span> Super Intelligence</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">摸黑干活</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/index.html"><i class="fa-fw fas fas fa-id-card"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-file"></i><span> CV</span><i class="fas fa-chevron-down expand hide"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/CV/Maqianli_CV.html"><i class="fa-fw fas fa-file-alt"></i><span> CN</span></a></li><li><a class="site-page" href="/CV/Maqianli_CV_EN.html"><i class="fa-fw far fa-file-alt"></i><span> EN</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/nn/"><i class="fa-fw fas fa-network-wired"></i><span> Super Intelligence</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">RNN</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-07-18T08:00:31.000Z" title="发表于 2020-07-18 16:00:31">2020-07-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-05-28T06:13:36.212Z" title="更新于 2024-05-28 14:13:36">2024-05-28</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E3%80%8A%E7%82%BC%E9%87%91%E6%9C%AF%E5%A3%AB%E4%BF%AE%E7%82%BC%E6%89%8B%E5%86%8C%E3%80%8B/">《炼金术士修炼手册》</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="循环序列模型"><a href="#循环序列模型" class="headerlink" title="循环序列模型"></a>循环序列模型</h1><h2 id="为什么选择序列模型"><a href="#为什么选择序列模型" class="headerlink" title="为什么选择序列模型"></a>为什么选择序列模型</h2><p><img src="http://www.ai-start.com/dl2017/images/ae2970d80a119cd341ef31c684bfac49.png" alt=""><br><span id="more"></span></p>
<h2 id="数学符号"><a href="#数学符号" class="headerlink" title="数学符号"></a>数学符号</h2><ul>
<li>输入$x^{<t>}$</li>
<li>输出$y^{<t>}$</li>
<li>序列长度$T_x$</li>
<li>词向量one-hot编码<br><img src="http://www.ai-start.com/dl2017/images/8deca8a84f06466155d2d8d53d26e05d.png" alt=""></li>
</ul>
<h2 id="RNN模型"><a href="#RNN模型" class="headerlink" title="RNN模型"></a>RNN模型</h2><h3 id="用普通神经网络的问题"><a href="#用普通神经网络的问题" class="headerlink" title="用普通神经网络的问题"></a>用普通神经网络的问题</h3><ul>
<li>输出和输入维度不对应</li>
<li>序列前后不存在联系</li>
</ul>
<h3 id="解决，引入RNN"><a href="#解决，引入RNN" class="headerlink" title="解决，引入RNN"></a>解决，引入RNN</h3><p><img src="http://www.ai-start.com/dl2017/images/cb041c33b65e17600842ebf87174c4f2.png" alt=""></p>
<script type="math/tex; mode=display">
a^{<1>}=g_1(w_{aa}a^{<0>}+w_{<ax>x^{1}}+b_a)</script><script type="math/tex; mode=display">
\hat{y}=g_2(w_{ya}a^{<1>}+b_y)</script><h3 id="更一般情况"><a href="#更一般情况" class="headerlink" title="更一般情况"></a>更一般情况</h3><script type="math/tex; mode=display">
a^{<t>}=g_1(w_{aa}a^{<t-1>}+w_{<ax>x^{t}}+b_a)</script><script type="math/tex; mode=display">
\hat{y}=g_2(w_{ya}a^{<t>}+b_y)</script><h3 id="进一步化简"><a href="#进一步化简" class="headerlink" title="进一步化简"></a>进一步化简</h3><script type="math/tex; mode=display">
a^{<t>}=g_1(w_{a}[a^{<t-1>},x^{t}]+b_a)</script><script type="math/tex; mode=display">
\hat{y}=g_2(w_{ya}a^{<t>}+b_y)</script><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p><img src="http://www.ai-start.com/dl2017/images/rnn-f.png" alt=""></p>
<h3 id="RNN常用激活函数"><a href="#RNN常用激活函数" class="headerlink" title="RNN常用激活函数"></a>RNN常用激活函数</h3><ul>
<li>tanh</li>
<li>ReLU</li>
</ul>
<h2 id="通过时间的反向传播"><a href="#通过时间的反向传播" class="headerlink" title="通过时间的反向传播"></a>通过时间的反向传播</h2><h3 id="单个单元Loss-Function"><a href="#单个单元Loss-Function" class="headerlink" title="单个单元Loss Function"></a>单个单元Loss Function</h3><script type="math/tex; mode=display">
L^{<t>}(\hat{y}^{<t>},y^{<t>})=
-y^{<t>}log\hat{y}^{<t>}-(1-y^{<t>})log(1-\hat{y}^{<t>})</script><h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><script type="math/tex; mode=display">
L(\hat{y}^{<t>},y^{<t>})=
\sum_{t=1}^{T_n}L^{<t>}(\hat{y}^{<t>},y^{<t>})</script><h3 id="RNN反向传播计算"><a href="#RNN反向传播计算" class="headerlink" title="RNN反向传播计算"></a>RNN反向传播计算</h3><p><img src="http://www.ai-start.com/dl2017/images/rnn_cell_backprop.png" alt=""></p>
<h2 id="不同类型的RNN结构"><a href="#不同类型的RNN结构" class="headerlink" title="不同类型的RNN结构"></a>不同类型的RNN结构</h2><p><img src="http://www.ai-start.com/dl2017/images/329b0748f7282efc206ea8de6a709833.png" alt=""></p>
<h3 id="多对一"><a href="#多对一" class="headerlink" title="多对一"></a>多对一</h3><p><img src="http://www.ai-start.com/dl2017/images/14e1df0a7a8cdd1584b2e92e87e23aa7.png" alt=""></p>
<h3 id="一对一"><a href="#一对一" class="headerlink" title="一对一"></a>一对一</h3><p><img src="http://www.ai-start.com/dl2017/images/14e1df0a7a8cdd1584b2e92e87e23aa7.png" alt=""></p>
<h3 id="一对多"><a href="#一对多" class="headerlink" title="一对多"></a>一对多</h3><p><img src="http://www.ai-start.com/dl2017/images/db580f1dfd6095d672fc62cce74ce5e2.png" alt=""></p>
<h2 id="语言模型和序列生成"><a href="#语言模型和序列生成" class="headerlink" title="语言模型和序列生成"></a>语言模型和序列生成</h2><p>语言模型所做的就是，它会告诉你某个特定的句子它出现的概率是多少</p>
<h3 id="语言模型建立步骤"><a href="#语言模型建立步骤" class="headerlink" title="语言模型建立步骤"></a>语言模型建立步骤</h3><ul>
<li>获取大量数据集</li>
<li>句子标记化</li>
<li>构建RNN</li>
</ul>
<h2 id="RNN梯度消失的问题"><a href="#RNN梯度消失的问题" class="headerlink" title="RNN梯度消失的问题"></a>RNN梯度消失的问题</h2><p><img src="http://www.ai-start.com/dl2017/images/8fb1c4afe30b7a0ede26522b355068ba.png" alt=""></p>
<h3 id="RNN存在问题"><a href="#RNN存在问题" class="headerlink" title="RNN存在问题"></a>RNN存在问题</h3><ul>
<li>缺乏长期记忆，如cat和were</li>
<li>梯度爆炸</li>
</ul>
<h3 id="如何解决？"><a href="#如何解决？" class="headerlink" title="如何解决？"></a>如何解决？</h3><ul>
<li>GRU</li>
<li>LSTM</li>
</ul>
<h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><h3 id="cell单元"><a href="#cell单元" class="headerlink" title="cell单元"></a>cell单元</h3><ul>
<li>功能：记忆<script type="math/tex; mode=display">
\hat{c}^{<t>}=tanh(W_c[c^{<t-1>},x^{<t>}]+b_c)</script><h3 id="记忆门"><a href="#记忆门" class="headerlink" title="记忆门"></a>记忆门</h3><script type="math/tex; mode=display">
\Gamma_u=\sigma(W_u[c^{<t-1>},x^{<t>}]+b_u)</script></li>
</ul>
<h3 id="GRU简化模型"><a href="#GRU简化模型" class="headerlink" title="GRU简化模型"></a>GRU简化模型</h3><script type="math/tex; mode=display">
c^{<t>}=\Gamma \hat{c}^{<t>}+(1-\Gamma_u)c^{<t-1>}</script><h3 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h3><script type="math/tex; mode=display">
\Gamma_r=\sigma(W_r[c^{<t-1>},x^{<t>}]+b_r)</script><script type="math/tex; mode=display">
\hat{c}^{<t>}=tanh(W_c[\Gamma_r*c^{<t-1>},x^{<t>}]+b_c)</script><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><h3 id="LSTM模型图"><a href="#LSTM模型图" class="headerlink" title="LSTM模型图"></a>LSTM模型图</h3><p><img src="http://www.ai-start.com/dl2017/images/LSTM.png" alt=""></p>
<h3 id="LSTM单元"><a href="#LSTM单元" class="headerlink" title="LSTM单元"></a>LSTM单元</h3><ul>
<li>更新门</li>
<li>遗忘门</li>
<li>输出门</li>
<li>激活函数</li>
<li>相关性门</li>
</ul>
<h3 id="LSTM的前向传播"><a href="#LSTM的前向传播" class="headerlink" title="LSTM的前向传播"></a>LSTM的前向传播</h3><p><img src="http://www.ai-start.com/dl2017/images/LSTM_rnn.png" alt=""></p>
<h3 id="LSTM反向传播"><a href="#LSTM反向传播" class="headerlink" title="LSTM反向传播"></a>LSTM反向传播</h3><h4 id="门求偏导"><a href="#门求偏导" class="headerlink" title="门求偏导"></a>门求偏导</h4><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200712162521.png" alt=""></p>
<h4 id="参数求偏导"><a href="#参数求偏导" class="headerlink" title="参数求偏导"></a>参数求偏导</h4><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200712162520.png" alt=""></p>
<h4 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h4><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200712162522.png" alt=""></p>
<h2 id="双向RNN"><a href="#双向RNN" class="headerlink" title="双向RNN"></a>双向RNN</h2><p><img src="http://www.ai-start.com/dl2017/images/48c787912f7f8daee638dd311583d6cf.png" alt=""></p>
<h2 id="深层RNN"><a href="#深层RNN" class="headerlink" title="深层RNN"></a>深层RNN</h2><p><img src="http://www.ai-start.com/dl2017/images/8378c2bfe73e1ac9f85d6aa79b71b5eb.png" alt=""></p>
<hr>
<h1 id="自然语言处理和词嵌入"><a href="#自然语言处理和词嵌入" class="headerlink" title="自然语言处理和词嵌入"></a>自然语言处理和词嵌入</h1><p><img src="http://www.ai-start.com/dl2017/images/68d7c930146724f39782cb57d33161e9.png" alt=""></p>
<h2 id="词汇表征"><a href="#词汇表征" class="headerlink" title="词汇表征"></a>词汇表征</h2><h3 id="one-hot编码的缺点"><a href="#one-hot编码的缺点" class="headerlink" title="one-hot编码的缺点"></a>one-hot编码的缺点</h3><ul>
<li>每个词孤立，泛化能力不强</li>
<li>数据量太大</li>
</ul>
<h3 id="如何改进？"><a href="#如何改进？" class="headerlink" title="如何改进？"></a>如何改进？</h3><h4 id="应用特征向量"><a href="#应用特征向量" class="headerlink" title="应用特征向量"></a>应用特征向量</h4><p><img src="http://www.ai-start.com/dl2017/images/ce30c9ae7912bdb3562199bf85eca1cd.png" alt=""></p>
<h3 id="t-SNE-算法"><a href="#t-SNE-算法" class="headerlink" title="t-SNE 算法"></a>t-SNE 算法</h3><p>将一维词向量转化为二维可视化<br><img src="http://www.ai-start.com/dl2017/images/59fb45cfdf7faa53571ec7b921b78358.png" alt=""></p>
<h2 id="使用词嵌入"><a href="#使用词嵌入" class="headerlink" title="使用词嵌入"></a>使用词嵌入</h2><p><img src="http://www.ai-start.com/dl2017/images/b4bf4b0cdcef0c9d021707c47d5aecda.png" alt=""></p>
<h3 id="词嵌入做迁移学习的方法"><a href="#词嵌入做迁移学习的方法" class="headerlink" title="词嵌入做迁移学习的方法"></a>词嵌入做迁移学习的方法</h3><ul>
<li>先从大量的文本集中学习词嵌入。一个非常大的文本集，或者下载预训练好的词嵌入模型</li>
<li><p>用这些词嵌入模型把它迁移到新的只有少量标注训练集的任务中，比如说用这个300维的词嵌入来表示的单词。这样做的一个好处就是可以用更低维度的特征向量代替原来的10000维的one-hot向量，现在你可以用一个300维更加紧凑的向量。尽管one-hot向量很快计算，而学到的用于词嵌入的300维的向量会更加紧凑。</p>
</li>
<li><p>在你的任务上训练模型时，在你的命名实体识别任务上，只有少量的标记数据集上，你可以自己选择要不要继续微调，用新的数据调整词嵌入。实际中，只有这个第二步中有很大的数据集你才会这样做，如果你标记的数据集不是很大，通常我不会在微调词嵌入上费力气。</p>
</li>
</ul>
<h2 id="词嵌入的特性"><a href="#词嵌入的特性" class="headerlink" title="词嵌入的特性"></a>词嵌入的特性</h2><ul>
<li>类比推理</li>
</ul>
<script type="math/tex; mode=display">
e_{man}-e_{women}=
\begin{bmatrix}
-1\\
0.01\\
0.03
\\0.09
\end{bmatrix}
-
\begin{bmatrix}
-1\\
0.02\\
0.02
\\0.01
\end{bmatrix}

=
\begin{bmatrix}
-2\\
-0.01\\
0.01
\\0.08
\end{bmatrix}
\approx
\begin{bmatrix}
-2\\
0\\
0
\\0
\end{bmatrix}</script><script type="math/tex; mode=display">
e_{king}-e_{queen}=
\begin{bmatrix}
-0.95\\
0.093\\
0.70
\\0.09
\end{bmatrix}
-
\begin{bmatrix}
0.97\\
0.95\\
0.69
\\0.01
\end{bmatrix}

=
\begin{bmatrix}
-1.92\\
-0.02\\
0.01
\\0.01
\end{bmatrix}
\approx
\begin{bmatrix}
-2\\
0\\
0
\\0
\end{bmatrix}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/16/db12137b26fb4e84e5261b2dcb49ddf9.png" alt=""></p>
<script type="math/tex; mode=display">
e_{man}-e_{woman}\approx e_{king}-e_{queen}</script><h3 id="如何反映相似度？"><a href="#如何反映相似度？" class="headerlink" title="如何反映相似度？"></a>如何反映相似度？</h3><p><strong>余弦相似度</strong></p>
<script type="math/tex; mode=display">
sim(u,v)=\frac{u^Tv}{|u|_2|v|_2}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/16/b71d0cddd12a41ab5c9212b24497a92c.png" alt=""></p>
<h2 id="嵌入矩阵"><a href="#嵌入矩阵" class="headerlink" title="嵌入矩阵"></a>嵌入矩阵</h2><p>学习词嵌入其实是学习一个嵌入矩阵<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/16/f319dc21af8e248c2e7418883f160097.png" alt=""></p>
<script type="math/tex; mode=display">
E\cdot o_i=e_{i}</script><h2 id="学习词嵌入"><a href="#学习词嵌入" class="headerlink" title="学习词嵌入"></a>学习词嵌入</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/16/b6b25908f6ab1c94d6d102191377160f.png" alt=""></p>
<h3 id="skip-gram"><a href="#skip-gram" class="headerlink" title="skip-gram"></a>skip-gram</h3><p><strong>输入：预测的词汇前n个单词，上下文</strong></p>
<p><strong>输出：softmax的rank</strong></p>
<ul>
<li>随机生成一个嵌入矩阵</li>
<li>学习嵌入矩阵参数</li>
</ul>
<h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>学习映射关系</p>
<script type="math/tex; mode=display">
o_c \to E \to e_c\to o_{softmax}\to \hat{y}</script><ul>
<li>随机选择目标单词上下文的另一个单词</li>
<li>输入one-hot向量</li>
<li>监督学习</li>
</ul>
<blockquote>
<p>softmax:</p>
</blockquote>
<script type="math/tex; mode=display">
p(t|c)=\frac{e^{\theta_t^T}e_c}{\sum_{j=1}^{10000}e^{\theta_t^T}e_c}</script><h3 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h3><p>softmax运算量太大</p>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><ul>
<li>分级</li>
<li>负采样</li>
</ul>
<h3 id="CBOM和skip-gram"><a href="#CBOM和skip-gram" class="headerlink" title="CBOM和skip-gram"></a>CBOM和skip-gram</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/16/774dd0cfce8313c273f7155aa064869a.png" alt=""></p>
<h2 id="负采样（Negative-Sampling）"><a href="#负采样（Negative-Sampling）" class="headerlink" title="负采样（Negative Sampling）"></a>负采样（Negative Sampling）</h2><ul>
<li>给定一对单词和target</li>
<li>使用逻辑回归</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/16/1b1dd8a7cbddd05425e6b728cbbac957.png" alt=""></p>
<script type="math/tex; mode=display">
P(y=1|c,t)=\sigma(\theta_t^Te_c)</script><ul>
<li>输入one-hot向量</li>
<li>传递给嵌入矩阵</li>
<li>得到10000个逻辑回归问题</li>
<li>只选取其中K个进行训练</li>
<li>转化为K个二分类问题</li>
</ul>
<h2 id="Glove-global-vectors-for-word-represengtation"><a href="#Glove-global-vectors-for-word-represengtation" class="headerlink" title="Glove(global vectors for word represengtation)"></a>Glove(global vectors for word represengtation)</h2><ul>
<li>$x_{ij}$:单词i和单词j在上下文出现的次数</li>
</ul>
<script type="math/tex; mode=display">
minimize\sum_{i=1}^{10000}\sum_{j=1}^{10000}f(x_{ij})(\theta_i^Te_j+b_i-b_j+\log x_{ij})^2</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/17/c649b0538387f6bac50010d96b8ca019.png" alt=""></p>
<h2 id="情感分类"><a href="#情感分类" class="headerlink" title="情感分类"></a>情感分类</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/17/af7d6b80483a8a5ad04a78ebaabeca7d.png" alt=""></p>
<p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/17/134af70c48588b247e437f8a5cab420e.png" alt=""></p>
<h2 id="词嵌入除偏（Debiasing-Word-Embeddings）"><a href="#词嵌入除偏（Debiasing-Word-Embeddings）" class="headerlink" title="词嵌入除偏（Debiasing Word Embeddings）"></a>词嵌入除偏（Debiasing Word Embeddings）</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/17/7640e02d6663dd97ec7cf1e152041e1e.png" alt=""></p>
<ul>
<li>求平均</li>
<li>中和</li>
<li>均衡步</li>
</ul>
<h1 id="序列模型和注意力机制（Sequence-models-amp-Attention-mechanism"><a href="#序列模型和注意力机制（Sequence-models-amp-Attention-mechanism" class="headerlink" title="序列模型和注意力机制（Sequence models &amp; Attention mechanism"></a>序列模型和注意力机制（Sequence models &amp; Attention mechanism</h1><h2 id="基础模型"><a href="#基础模型" class="headerlink" title="基础模型"></a>基础模型</h2><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><ul>
<li>机器翻译</li>
<li>语言识别</li>
</ul>
<h3 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/4e15947a9ec31763d8545715e20e707c.png" alt=""></p>
<ul>
<li>输入法语，输出英语</li>
<li>网络结构：LSTM或GR</li>
</ul>
<h3 id="图片描述"><a href="#图片描述" class="headerlink" title="图片描述"></a>图片描述</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/2683ae07c70d727208561dc32a64d43a.png" alt=""></p>
<ul>
<li>输入图片</li>
<li>进入卷积神经网络如AlexNet</li>
<li>把softmax单元更换为RNN结构</li>
<li>输出描述图片的序列</li>
</ul>
<h2 id="选择最可能的句子（Picking-the-most-likely-sentence）"><a href="#选择最可能的句子（Picking-the-most-likely-sentence）" class="headerlink" title="选择最可能的句子（Picking the most likely sentence）"></a>选择最可能的句子（Picking the most likely sentence）</h2><h3 id="语言模型和机器翻译模型的区别"><a href="#语言模型和机器翻译模型的区别" class="headerlink" title="语言模型和机器翻译模型的区别"></a>语言模型和机器翻译模型的区别</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/d6b739666d9d7af497ce06ea4a3b14fc.png" alt=""></p>
<ul>
<li>语言模型以零向量开始，翻译模型从句子开始</li>
<li>翻译模型为条件语言模型</li>
</ul>
<p><strong>最大化以下式子</strong></p>
<script type="math/tex; mode=display">
P(y^{<1>},...,t^{<T_y>}|x)</script><h3 id="为什么不用Greed-search"><a href="#为什么不用Greed-search" class="headerlink" title="为什么不用Greed search"></a>为什么不用Greed search</h3><ul>
<li>我们需要一次性输出整个句子而不是一个单词</li>
<li>运算量太大</li>
</ul>
<blockquote>
<p>贪心搜索是一种来自计算机科学的算法，生成第一个词的分布以后，它将会根据你的条件语言模型挑选出最有可能的第一个词进入你的机器翻译模型中，在挑选出第一个词之后它将会继续挑选出最有可能的第二个词，然后继续挑选第三个最有可能的词，这种算法就叫做贪心搜索。</p>
</blockquote>
<h2 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h2><ul>
<li>beam width$B=3$,选择softmax前三个最大值</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/a620354d4ceafdca5ca0f9ee8b5b658b.png" alt=""></p>
<ul>
<li>输入句子，如“Jane visite l’Afrique en Septembre”</li>
<li>选择第一个单词，保留前三的可能性</li>
<li>确认单词对的最大可能性</li>
<li>继续重复该步骤</li>
</ul>
<h2 id="Beam-search-改进"><a href="#Beam-search-改进" class="headerlink" title="Beam search 改进"></a>Beam search 改进</h2><h3 id="Length-normalization"><a href="#Length-normalization" class="headerlink" title="Length normalization"></a>Length normalization</h3><script type="math/tex; mode=display">
arg max\prod_{t=1}^
{T_y}P(y^{<t>}|x,y^{<1>},...,y^{<t-1>})</script><p>加入log函数</p>
<script type="math/tex; mode=display">
arg max\sum_{t=1}^
{T_y}\log P(y^{<t>}|x,y^{<1>},...,y^{<t-1>})</script><p>加入参数$\alpha$</p>
<script type="math/tex; mode=display">
\frac{1}{T_y^{\alpha}}arg max\sum_{t=1}^
{T_y}\log P(y^{<t>}|x,y^{<1>},...,y^{<t-1>})</script><h3 id="how-to-choose-Beam-width-B"><a href="#how-to-choose-Beam-width-B" class="headerlink" title="how to choose Beam width B"></a>how to choose Beam width B</h3><ul>
<li>B=1贪心算法</li>
<li>B过大，计算量大，效果没有显著提升</li>
</ul>
<h2 id="Beam-search-误差分析"><a href="#Beam-search-误差分析" class="headerlink" title="Beam search 误差分析"></a>Beam search 误差分析</h2><ul>
<li>$P(y^*|x)&gt;P(\hat{y}|x)$束搜索算法出错</li>
<li>$P(y^*|x)&lt;P(\hat{y}|x)$RNN模型出错</li>
</ul>
<blockquote>
<p>RNN实际上是一个编码器合解码器</p>
</blockquote>
<h2 id="如何评价一个翻译系统的好坏"><a href="#如何评价一个翻译系统的好坏" class="headerlink" title="如何评价一个翻译系统的好坏"></a>如何评价一个翻译系统的好坏</h2><ul>
<li>与人工翻译进行对比</li>
<li>观察输出是否在参考中</li>
<li>每个单词加入计分上限</li>
</ul>
<h3 id="Blue-score"><a href="#Blue-score" class="headerlink" title="Blue score"></a>Blue score</h3><p>考虑词组得分</p>
<p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/143d162f91004d79ff47338ad85be5d7.png" alt=""></p>
<h2 id="注意力模型（Attention-Model）"><a href="#注意力模型（Attention-Model）" class="headerlink" title="注意力模型（Attention Model）"></a>注意力模型（Attention Model）</h2><h3 id="长句子的问题"><a href="#长句子的问题" class="headerlink" title="长句子的问题"></a>长句子的问题</h3><p>blue score降低<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/d1ab68289592565fa9abf0df235039e3.png" alt=""></p>
<h3 id="注意力权重-alpha-lt-t-t’-gt"><a href="#注意力权重-alpha-lt-t-t’-gt" class="headerlink" title="注意力权重$\alpha&lt;t,t’&gt;$"></a>注意力权重$\alpha&lt;t,t’&gt;$</h3><p>第t个输出单词对应第t‘个输入单词的权重<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/c629b0432b83a9875e5a80ebb2558484.png" alt=""></p>
<script type="math/tex; mode=display">
c^{<t>}=\sum_{t'}\alpha^{<t,t'>}a^{<t'>}</script><script type="math/tex; mode=display">
\alpha^{<t,t'>}=\frac{exp(e^{<t,t'>})}{\sum_{t'=1}^{T_x}exp(e^{<t,t'>})}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/b6ff6fb462ce466873c06b366060a817.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/4560eb03dfac3d28c6e1e831295d64e7.png" alt=""></p>
<h3 id="网络结构梳理"><a href="#网络结构梳理" class="headerlink" title="网络结构梳理"></a>网络结构梳理</h3><script type="math/tex; mode=display">
activate：x^t \to a^t</script><script type="math/tex; mode=display">
a^t,s^{t-1}\to e^t</script><script type="math/tex; mode=display">
softmax:e \to \alpha</script><script type="math/tex; mode=display">
\alpha,a\to c</script><script type="math/tex; mode=display">
activate：c \to s</script><script type="math/tex; mode=display">
s^t\to y^t</script><h2 id="语音识别"><a href="#语音识别" class="headerlink" title="语音识别"></a>语音识别</h2><ul>
<li>输入音频</li>
<li>输出文字序列</li>
</ul>
<h3 id="phonemes方法"><a href="#phonemes方法" class="headerlink" title="phonemes方法"></a>phonemes方法</h3><p>将声音分解为最基本的音位</p>
<h3 id="CTC-cost（Connectionist-temporal-classification）"><a href="#CTC-cost（Connectionist-temporal-classification）" class="headerlink" title="CTC cost（Connectionist temporal classification）"></a>CTC cost（Connectionist temporal classification）</h3><ul>
<li>折叠重复字符<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/f16abb639f18e3ab3674c1db485590c8.png" alt=""></li>
</ul>
<h2 id="触发字检测（Trigger-Word-Detection）"><a href="#触发字检测（Trigger-Word-Detection）" class="headerlink" title="触发字检测（Trigger Word Detection）"></a>触发字检测（Trigger Word Detection）</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/16d66c2dd1d9b048e93013002beef77e.png" alt=""></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Fazzie</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://fazziekey.github.io/2020/07/18/RNN/">https://fazziekey.github.io/2020/07/18/RNN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://Fazziekey.github.io" target="_blank">摸黑干活</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Deep-learning/">Deep learning</a><a class="post-meta__tags" href="/tags/NLP/">NLP</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/07/18/note-of-deeplearning/"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">note of deeplearning </div></div></a></div><div class="next-post pull-right"><a href="/2020/07/18/CNN%E2%80%9C/"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">CNN“</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/07/18/CNN“/" title="CNN“"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-18</div><div class="title">CNN“</div></div></a></div><div><a href="/2021/12/13/Lite-Transformer/" title="论文解读：Lite Transformer"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-13</div><div class="title">论文解读：Lite Transformer</div></div></a></div><div><a href="/2021/03/14/deep-learning-sum/" title="深度学习激活函数和优化"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-14</div><div class="title">深度学习激活函数和优化</div></div></a></div><div><a href="/2020/07/18/note-of-deeplearning/" title="note of deeplearning "><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-18</div><div class="title">note of deeplearning </div></div></a></div><div><a href="/2020/08/04/pytorch-basic/" title="pytorch basic"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-04</div><div class="title">pytorch basic</div></div></a></div><div><a href="/2021/11/29/TFIDF/" title="基于TF-IDF和线性回归的简单文本分类"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-29</div><div class="title">基于TF-IDF和线性回归的简单文本分类</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Fazzie</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">69</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">41</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Fazziekey"><i class="fab fa-github"></i><span>Fork Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="mailto:3180104413@zju.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://www.linkedin.com/in/qianli-ma-313272207/" target="_blank" title=""><i class="fab fa-linkedin"></i></a><a class="social-icon" href="https://www.zhihu.com/people/fazzie" target="_blank" title=""><i class="fab fa-zhihu"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">talk is cheap, let's go fish</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">循环序列模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%80%89%E6%8B%A9%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.</span> <span class="toc-text">为什么选择序列模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7"><span class="toc-number">1.2.</span> <span class="toc-text">数学符号</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.</span> <span class="toc-text">RNN模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8%E6%99%AE%E9%80%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">1.3.1.</span> <span class="toc-text">用普通神经网络的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%EF%BC%8C%E5%BC%95%E5%85%A5RNN"><span class="toc-number">1.3.2.</span> <span class="toc-text">解决，引入RNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9B%B4%E4%B8%80%E8%88%AC%E6%83%85%E5%86%B5"><span class="toc-number">1.3.3.</span> <span class="toc-text">更一般情况</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%8C%96%E7%AE%80"><span class="toc-number">1.3.4.</span> <span class="toc-text">进一步化简</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.3.5.</span> <span class="toc-text">前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.6.</span> <span class="toc-text">RNN常用激活函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E6%97%B6%E9%97%B4%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.4.</span> <span class="toc-text">通过时间的反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E4%B8%AA%E5%8D%95%E5%85%83Loss-Function"><span class="toc-number">1.4.1.</span> <span class="toc-text">单个单元Loss Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Loss-function"><span class="toc-number">1.4.2.</span> <span class="toc-text">Loss function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%A1%E7%AE%97"><span class="toc-number">1.4.3.</span> <span class="toc-text">RNN反向传播计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84RNN%E7%BB%93%E6%9E%84"><span class="toc-number">1.5.</span> <span class="toc-text">不同类型的RNN结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%AF%B9%E4%B8%80"><span class="toc-number">1.5.1.</span> <span class="toc-text">多对一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E5%AF%B9%E4%B8%80"><span class="toc-number">1.5.2.</span> <span class="toc-text">一对一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E5%AF%B9%E5%A4%9A"><span class="toc-number">1.5.3.</span> <span class="toc-text">一对多</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90"><span class="toc-number">1.6.</span> <span class="toc-text">语言模型和序列生成</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.6.1.</span> <span class="toc-text">语言模型建立步骤</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">1.7.</span> <span class="toc-text">RNN梯度消失的问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN%E5%AD%98%E5%9C%A8%E9%97%AE%E9%A2%98"><span class="toc-number">1.7.1.</span> <span class="toc-text">RNN存在问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%EF%BC%9F"><span class="toc-number">1.7.2.</span> <span class="toc-text">如何解决？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GRU"><span class="toc-number">1.8.</span> <span class="toc-text">GRU</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#cell%E5%8D%95%E5%85%83"><span class="toc-number">1.8.1.</span> <span class="toc-text">cell单元</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%B0%E5%BF%86%E9%97%A8"><span class="toc-number">1.8.2.</span> <span class="toc-text">记忆门</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GRU%E7%AE%80%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.8.3.</span> <span class="toc-text">GRU简化模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E6%80%A7"><span class="toc-number">1.8.4.</span> <span class="toc-text">相关性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM"><span class="toc-number">1.9.</span> <span class="toc-text">LSTM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM%E6%A8%A1%E5%9E%8B%E5%9B%BE"><span class="toc-number">1.9.1.</span> <span class="toc-text">LSTM模型图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM%E5%8D%95%E5%85%83"><span class="toc-number">1.9.2.</span> <span class="toc-text">LSTM单元</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM%E7%9A%84%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.9.3.</span> <span class="toc-text">LSTM的前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.9.4.</span> <span class="toc-text">LSTM反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%A8%E6%B1%82%E5%81%8F%E5%AF%BC"><span class="toc-number">1.9.4.1.</span> <span class="toc-text">门求偏导</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E6%B1%82%E5%81%8F%E5%AF%BC"><span class="toc-number">1.9.4.2.</span> <span class="toc-text">参数求偏导</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0"><span class="toc-number">1.9.4.3.</span> <span class="toc-text">参数更新</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8C%E5%90%91RNN"><span class="toc-number">1.10.</span> <span class="toc-text">双向RNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%B1%82RNN"><span class="toc-number">1.11.</span> <span class="toc-text">深层RNN</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%92%8C%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="toc-number">2.</span> <span class="toc-text">自然语言处理和词嵌入</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%8D%E6%B1%87%E8%A1%A8%E5%BE%81"><span class="toc-number">2.1.</span> <span class="toc-text">词汇表征</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#one-hot%E7%BC%96%E7%A0%81%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="toc-number">2.1.1.</span> <span class="toc-text">one-hot编码的缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%94%B9%E8%BF%9B%EF%BC%9F"><span class="toc-number">2.1.2.</span> <span class="toc-text">如何改进？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">应用特征向量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#t-SNE-%E7%AE%97%E6%B3%95"><span class="toc-number">2.1.3.</span> <span class="toc-text">t-SNE 算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="toc-number">2.2.</span> <span class="toc-text">使用词嵌入</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%81%9A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">2.2.1.</span> <span class="toc-text">词嵌入做迁移学习的方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5%E7%9A%84%E7%89%B9%E6%80%A7"><span class="toc-number">2.3.</span> <span class="toc-text">词嵌入的特性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%8F%8D%E6%98%A0%E7%9B%B8%E4%BC%BC%E5%BA%A6%EF%BC%9F"><span class="toc-number">2.3.1.</span> <span class="toc-text">如何反映相似度？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5%E7%9F%A9%E9%98%B5"><span class="toc-number">2.4.</span> <span class="toc-text">嵌入矩阵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="toc-number">2.5.</span> <span class="toc-text">学习词嵌入</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#skip-gram"><span class="toc-number">2.5.1.</span> <span class="toc-text">skip-gram</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Word2Vec"><span class="toc-number">2.6.</span> <span class="toc-text">Word2Vec</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%98%E5%9C%A8%E9%97%AE%E9%A2%98"><span class="toc-number">2.6.1.</span> <span class="toc-text">存在问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="toc-number">2.6.2.</span> <span class="toc-text">解决方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CBOM%E5%92%8Cskip-gram"><span class="toc-number">2.6.3.</span> <span class="toc-text">CBOM和skip-gram</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9F%E9%87%87%E6%A0%B7%EF%BC%88Negative-Sampling%EF%BC%89"><span class="toc-number">2.7.</span> <span class="toc-text">负采样（Negative Sampling）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Glove-global-vectors-for-word-represengtation"><span class="toc-number">2.8.</span> <span class="toc-text">Glove(global vectors for word represengtation)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB"><span class="toc-number">2.9.</span> <span class="toc-text">情感分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5%E9%99%A4%E5%81%8F%EF%BC%88Debiasing-Word-Embeddings%EF%BC%89"><span class="toc-number">2.10.</span> <span class="toc-text">词嵌入除偏（Debiasing Word Embeddings）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88Sequence-models-amp-Attention-mechanism"><span class="toc-number">3.</span> <span class="toc-text">序列模型和注意力机制（Sequence models &amp; Attention mechanism</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text">基础模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E7%94%A8"><span class="toc-number">3.1.1.</span> <span class="toc-text">应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91"><span class="toc-number">3.1.2.</span> <span class="toc-text">机器翻译</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E7%89%87%E6%8F%8F%E8%BF%B0"><span class="toc-number">3.1.3.</span> <span class="toc-text">图片描述</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E6%9C%80%E5%8F%AF%E8%83%BD%E7%9A%84%E5%8F%A5%E5%AD%90%EF%BC%88Picking-the-most-likely-sentence%EF%BC%89"><span class="toc-number">3.2.</span> <span class="toc-text">选择最可能的句子（Picking the most likely sentence）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">3.2.1.</span> <span class="toc-text">语言模型和机器翻译模型的区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%94%A8Greed-search"><span class="toc-number">3.2.2.</span> <span class="toc-text">为什么不用Greed search</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Beam-Search"><span class="toc-number">3.3.</span> <span class="toc-text">Beam Search</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Beam-search-%E6%94%B9%E8%BF%9B"><span class="toc-number">3.4.</span> <span class="toc-text">Beam search 改进</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Length-normalization"><span class="toc-number">3.4.1.</span> <span class="toc-text">Length normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-to-choose-Beam-width-B"><span class="toc-number">3.4.2.</span> <span class="toc-text">how to choose Beam width B</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Beam-search-%E8%AF%AF%E5%B7%AE%E5%88%86%E6%9E%90"><span class="toc-number">3.5.</span> <span class="toc-text">Beam search 误差分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E4%B8%80%E4%B8%AA%E7%BF%BB%E8%AF%91%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%A5%BD%E5%9D%8F"><span class="toc-number">3.6.</span> <span class="toc-text">如何评价一个翻译系统的好坏</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Blue-score"><span class="toc-number">3.6.1.</span> <span class="toc-text">Blue score</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B%EF%BC%88Attention-Model%EF%BC%89"><span class="toc-number">3.7.</span> <span class="toc-text">注意力模型（Attention Model）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%95%BF%E5%8F%A5%E5%AD%90%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">3.7.1.</span> <span class="toc-text">长句子的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9D%83%E9%87%8D-alpha-lt-t-t%E2%80%99-gt"><span class="toc-number">3.7.2.</span> <span class="toc-text">注意力权重$\alpha&lt;t,t’&gt;$</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E6%A2%B3%E7%90%86"><span class="toc-number">3.7.3.</span> <span class="toc-text">网络结构梳理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB"><span class="toc-number">3.8.</span> <span class="toc-text">语音识别</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#phonemes%E6%96%B9%E6%B3%95"><span class="toc-number">3.8.1.</span> <span class="toc-text">phonemes方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CTC-cost%EF%BC%88Connectionist-temporal-classification%EF%BC%89"><span class="toc-number">3.8.2.</span> <span class="toc-text">CTC cost（Connectionist temporal classification）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A6%E5%8F%91%E5%AD%97%E6%A3%80%E6%B5%8B%EF%BC%88Trigger-Word-Detection%EF%BC%89"><span class="toc-number">3.9.</span> <span class="toc-text">触发字检测（Trigger Word Detection）</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/05/20/big-model/" title="超大模型加载转换Trick"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="超大模型加载转换Trick"/></a><div class="content"><a class="title" href="/2024/05/20/big-model/" title="超大模型加载转换Trick">超大模型加载转换Trick</a><time datetime="2024-05-20T06:15:55.000Z" title="发表于 2024-05-20 14:15:55">2024-05-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/02/21/MLsys/" title="ML system 入坑指南"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ML system 入坑指南"/></a><div class="content"><a class="title" href="/2023/02/21/MLsys/" title="ML system 入坑指南">ML system 入坑指南</a><time datetime="2023-02-21T07:49:43.000Z" title="发表于 2023-02-21 15:49:43">2023-02-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/02/21/pipeline-paralism/" title="流水线并行论文总结"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="流水线并行论文总结"/></a><div class="content"><a class="title" href="/2023/02/21/pipeline-paralism/" title="流水线并行论文总结">流水线并行论文总结</a><time datetime="2023-02-21T07:49:43.000Z" title="发表于 2023-02-21 15:49:43">2023-02-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/03/11/amp/" title="Mixed Precision Training"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Mixed Precision Training"/></a><div class="content"><a class="title" href="/2022/03/11/amp/" title="Mixed Precision Training">Mixed Precision Training</a><time datetime="2022-03-11T15:47:22.000Z" title="发表于 2022-03-11 23:47:22">2022-03-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/23/OPTIMIZER-FUSION/" title="论文解读：OPTIMIZER FUSION - EFFICIENT TRAINING WITH BETTER LOCALITY AND PARALLELISM"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文解读：OPTIMIZER FUSION - EFFICIENT TRAINING WITH BETTER LOCALITY AND PARALLELISM"/></a><div class="content"><a class="title" href="/2022/01/23/OPTIMIZER-FUSION/" title="论文解读：OPTIMIZER FUSION - EFFICIENT TRAINING WITH BETTER LOCALITY AND PARALLELISM">论文解读：OPTIMIZER FUSION - EFFICIENT TRAINING WITH BETTER LOCALITY AND PARALLELISM</a><time datetime="2022-01-23T06:53:08.000Z" title="发表于 2022-01-23 14:53:08">2022-01-23</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Fazzie</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi,  welcome  to  my  <a  target="_blank" rel="noopener" href="https://fazzie-key.cool/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    let initData = {
      el: '#vcomment',
      appId: 'fxFSTfaEl7rHVkn2JI1Ygud7-gzGzoHsz',
      appKey: 'l5dJNsod5s2Yf9XDYqWuFPwh',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
    }

    if (true) { 
      initData.requiredFields= ('nick,mail'.split(','))
    }
    
    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    const valine = new Valine(initData)
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><div class="aplayer no-destroy" data-id="7909210123" data-server="tencent" data-type="playlist" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="none" data-autoplay="false" muted></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>