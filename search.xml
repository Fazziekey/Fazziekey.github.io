<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>超大模型加载转换Trick</title>
      <link href="2024/05/20/big-model/"/>
      <url>2024/05/20/big-model/</url>
      
        <content type="html"><![CDATA[<p>在深度学习领域，大模型的训练和推理通常需要消耗大量的计算和内存。如何高效地加载和使用大模型是一个相当关键的问题。在这篇博客中，我将分享一些关于更快加载大模型和减少内存的技巧.</p><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>假设现在我们有一个236B 超大模型的原始权重的 <code>checkpoint.pth</code> 文件, 比如 <a href="https://huggingface.co/deepseek-ai/DeepSeek-V2-Chat">DeepSeek Chat V2</a>, 以BF16 格式存储, 一个标准的加载流程如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">state_dict = torch.load(checkpoint_file)</span><br><span class="line">my_model = BigModelClass(...)</span><br><span class="line">my_model.load_state_dict(state_dict)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这段代码的中, <code>my_model = BigModelClass(...)</code> 会初始化一个模型, <code>torch.load(checkpoint_file)</code>函数会将模型权重从磁盘加载到内存中。然后，<code>my_model.load_state_dict(state_dict)</code>函数会将权重从内存加载到模型的参数中。这两个步骤都可能会消耗大量的时间和内存。理想情况下, 一个236B BF16格式的模型需要占据 472GB 的内存, 上面的代码会有两个模型副本, 这意味着峰值需要944GB 内存, 接近1T ,这是非常夸张的也是不可接受的.</p><p>我们用一段简单的代码来验证上面的推断, 首先初始化一个 1B size 的模型并存下来,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_parameters</span>(<span class="params">model</span>):</span><br><span class="line">    total_params =  <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line">    <span class="keyword">return</span> total_params / <span class="number">1e9</span> </span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model_memory_size_in_megabytes</span>(<span class="params">model</span>):</span><br><span class="line">    param_size = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">        param_size += param.numel() * param.element_size()  </span><br><span class="line">        </span><br><span class="line">    bytes_in_gb = <span class="number">1024</span> * <span class="number">1024</span> * <span class="number">1024</span> </span><br><span class="line">    <span class="keyword">return</span> param_size / bytes_in_gb</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BigModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.linears = nn.ModuleList([nn.Linear(size, size) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linears(x)</span><br><span class="line"></span><br><span class="line">size = <span class="number">10000</span></span><br><span class="line">model = BigModel(size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印模型的参数量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> B trainable parameters&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The model&#x27;s memory size is approximately <span class="subst">&#123;model_memory_size_in_megabytes(model):<span class="number">.2</span>f&#125;</span> GB.&quot;</span>)</span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;checkpoint.pth&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>The model has 1.0001 B trainable parameters</p><p>The model’s memory size is approximately 3.73 GB.</p></blockquote><p>然后 按照上面的方式加载模型, 并统计cpu 内存占用, torch 默认是FP32 格式, 1B模型占用约 4GB 内存(实际为3.73GB左右), 下面代码验证后基本符合预期</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_usage</span>():</span><br><span class="line">    pid = os.getpid()</span><br><span class="line">    py = psutil.Process(pid)</span><br><span class="line">    memory_use = py.memory_info()[<span class="number">0</span>] / <span class="number">2.</span> ** <span class="number">30</span>  <span class="comment"># memory use in GB...I think</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;memory: <span class="subst">&#123;memory_use:<span class="number">.2</span>f&#125;</span> GB&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;CPU percent:&#x27;</span>, psutil.cpu_percent())</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Before Load the state_dict:&#x27;</span>)</span><br><span class="line">print_usage()</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>Before Load the state_dict:</p><p>memory: 0.34 GB</p><p>CPU percent: 8.5</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">start_time = time.time()</span><br><span class="line">state_dict = torch.load(<span class="string">&#x27;checkpoint.pth&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Loading the state_dict took <span class="subst">&#123;time.time() - start_time:<span class="number">.2</span>f&#125;</span> seconds&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;After Load the state_dict:&#x27;</span>)</span><br><span class="line">print_usage()</span><br></pre></td></tr></table></figure><blockquote><p>Loading the state_dict took 2.09 seconds</p><p>After Load the state_dict:</p><p>memory: 4.06 GB</p><p>CPU percent: 7.0</p></blockquote><p>4.06 - 0.34 = 3.72基本一致</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line">model = BigModel(size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Init the model took <span class="subst">&#123;time.time() - start_time:<span class="number">.2</span>f&#125;</span> seconds&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;After Init the model:&#x27;</span>)</span><br><span class="line">print_usage()</span><br></pre></td></tr></table></figure><blockquote><p>Init the model took 7.23 seconds</p><p>After Init the model:</p><p>memory: 7.79 GB</p><p>CPU percent: 7.6</p></blockquote><p>7.79 - 4.06 = 3.73 基本一致</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">start_time = time.time()</span><br><span class="line">model.load_state_dict(state_dict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Loading the state_dict to model took <span class="subst">&#123;time.time() - start_time:<span class="number">.2</span>f&#125;</span> seconds&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;After Load the state_dict to model:&#x27;</span>)</span><br><span class="line">print_usage()</span><br></pre></td></tr></table></figure><blockquote><p>Loading the state_dict to model took 2.63 seconds</p><p>After Load the state_dict to model:</p><p>memory: 7.79 GB</p><p>CPU percent: 16.4</p></blockquote><h2 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h2><p>分析清楚在加载和初始化环节中各个流程的开销, 我们来看看可以如何加速每个过程.</p><h3 id="使用torch-load-mmap-True"><a href="#使用torch-load-mmap-True" class="headerlink" title="使用torch.load(mmap=True)"></a>使用<code>torch.load(mmap=True)</code></h3><p>首先，让我们考虑一下当我们使用 加载检查点时会发生什么<code>torch.load</code>。当我们使用 保存检查点时<code>torch.save</code>，张量存储会使用保存它们的设备进行标记。使用<code>torch.load</code>，张量存储将加载到它们标记的设备（除非使用标志覆盖此行为 <code>map_location</code>）。为了便于解释，我们假设张量保存在 CPU 上。这意味着在第一行，所有张量存储都将加载到 CPU RAM 中，这在以下情况下可能是不可行的：</p><ul><li>CPU RAM 小于检查点的大小。</li><li>等待整个检查点加载到 RAM 中，然后再执行某些按张量处理等操作。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">start_time = time.time()</span><br><span class="line">state_dict = torch.load(<span class="string">&#x27;checkpoint.pth&#x27;</span>)</span><br><span class="line">end_time = time.time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;loading time without mmap=<span class="subst">&#123;end_time - start_time&#125;</span>&quot;</span>)</span><br><span class="line">print_usage()</span><br></pre></td></tr></table></figure><blockquote><p>loading time without mmap=2.0737619400024414</p><p>memory: 4.06 GB</p><p>CPU percent: 8.7</p></blockquote><p><code>torch.load</code>中的<code>mmap</code>参数图解决上述两个问题。顾名思义，<code>mmap</code>关键字参数 to<code>torch.load</code> 使用<a href="https://man7.org/linux/man-pages/man2/mmap.2.html">mmap 调用</a> ，将磁盘上的文件映射到虚拟内存，并让操作系统自动处理到物理内存的加载和卸载。当这个标志被传递时，张量存储将被内存映射。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">start_time = time.time()</span><br><span class="line">state_dict = torch.load(<span class="string">&#x27;checkpoint.pth&#x27;</span>, mmap=<span class="literal">True</span>)</span><br><span class="line">end_time = time.time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;loading time with mmap=<span class="subst">&#123;end_time - start_time&#125;</span>&quot;</span>)</span><br><span class="line">print_usage()</span><br></pre></td></tr></table></figure><blockquote><p>loading time with mmap=0.003424406051635742</p><p>memory: 0.34 GB</p></blockquote><p>通过上面对比,我们可以发现 使用mmap可以加速模型加载并减少内存占用, 对于236B的模型, 我们实际上并不需要 1TB的 CPU内存来完成转换</p><h3 id="使用-torch-device-39-meta-39"><a href="#使用-torch-device-39-meta-39" class="headerlink" title="使用 torch.device(&#39;meta&#39;)"></a>使用 <code>torch.device(&#39;meta&#39;)</code></h3><p>当模型size 巨大时, 模型初始化也需要巨大时间, 我们扩大一下模型size到25B, 初始化一个模型就需要接近3分钟.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">size = <span class="number">50000</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">model = BigModel(size)</span><br><span class="line">end_time = time.time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;init time=<span class="subst">&#123;end_time - start_time&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> B trainable parameters&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The model&#x27;s memory size is approximately <span class="subst">&#123;model_memory_size_in_megabytes(model):<span class="number">.2</span>f&#125;</span> GB.&quot;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>init time=184.56671452522278</p><p>The model has 25.0005 B trainable parameters</p><p>The model’s memory size is approximately 93.13 GB.</p></blockquote><p>但在load 模型时, 初始化这一步是多余的, 我们实际上只需要知道模型的所有 key 和 对应的 shape,</p><p>这个时候, <code>torch.device(&#39;meta&#39;)</code> 这个 上下文就可以发挥作用了, <a href="https://pytorch.org/docs/main/tensor_attributes.html#torch-device"><code>torch.device()</code></a> 上下文管理器确保工厂调用将像它们被传递了指定的”device”作为参数一样执行。在 <code>torch.device(&#39;meta&#39;)</code> 上的张量不携带数据。然而，它们具有张量所具有的所有其他元数据，例如<code>.size()</code>、<code>.stride()</code>、<code>.requires_grad</code>等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.device(<span class="string">&#x27;meta&#x27;</span>):</span><br><span class="line">   model = BigModel(size)</span><br><span class="line">model.load_state_dict(state_dict, assign=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="keyword">assert</span> p.device.<span class="built_in">type</span> != <span class="string">&quot;meta&quot;</span>, <span class="string">f&quot;<span class="subst">&#123;n&#125;</span> has not been loaded!&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>注意, 在使用 <code>torch.device(&#39;meta&#39;)</code>后, 我们需要加上 <code>assign=True</code>参数来让参数被加载. 最后一段代码可以check 所有参数被正确加载了, 加载后的参数的 device应该不再是 <code>meta</code> 了.</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>最后, 我们直接上一个100B size大小的大模型来对比, 是否使用 <code>torch.load(mmap=True)</code> 和<code>torch.device(&#39;meta&#39;)</code> 速度差别.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">size = <span class="number">100000</span></span><br><span class="line">model = BigModel(size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印模型的参数量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> B trainable parameters&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The model&#x27;s memory size is approximately <span class="subst">&#123;model_memory_size_in_megabytes(model):<span class="number">.2</span>f&#125;</span> GB.&quot;</span>)</span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;checkpoint.pth&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>The model has 100.001 B trainable parameters</p><p>The model’s memory size is approximately 186.27 GB.</p></blockquote><h3 id="加速前"><a href="#加速前" class="headerlink" title="加速前"></a>加速前</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">start_time = time.time()</span><br><span class="line">state_dict = torch.load(<span class="string">&#x27;checkpoint.pth&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Loading the state_dict took <span class="subst">&#123;time.time() - start_time:<span class="number">.2</span>f&#125;</span> seconds&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;After Load the state_dict:&#x27;</span>)</span><br><span class="line">print_usage()</span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line">model = BigModel(size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Init the model took <span class="subst">&#123;time.time() - start_time:<span class="number">.2</span>f&#125;</span> seconds&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;After Init the model:&#x27;</span>)</span><br><span class="line">print_usage()</span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line">model.load_state_dict(state_dict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Loading the state_dict to model took <span class="subst">&#123;time.time() - start_time:<span class="number">.2</span>f&#125;</span> seconds&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;After Load the state_dict to model:&#x27;</span>)</span><br><span class="line">print_usage()</span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, size)</span><br><span class="line">output = model(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;One time forward <span class="subst">&#123;time.time() - start_time:<span class="number">.2</span>f&#125;</span> seconds&#x27;</span>)</span><br><span class="line">print_usage()</span><br></pre></td></tr></table></figure><blockquote><p>Before Load the state_dict:</p><p>memory: 0.34 GB</p><p>CPU percent: 9.1</p><p>Loading the state_dict took 852.06 seconds</p><p>After Load the state_dict:</p><p>memory: 372.87 GB</p><p>CPU percent: 5.0</p><p>Init the model took 518.15 seconds</p><p>After Init the model:</p><p>memory: 745.41 GB</p><p>CPU percent: 4.9</p><p>Loading the state_dict to model took 125.63 seconds</p><p>After Load the state_dict to model:</p><p>memory: 745.41 GB</p><p>CPU percent: 11.7</p><p>tensor([[-0.0015, 0.0017, -0.0009, …, -0.0036, 0.0041, 0.0052]],</p><p>&#x20;grad_fn=\<AddmmBackward0>)</p><p>One time forward 6.95 seconds</p><p>memory: 745.42 GB</p><p>CPU percent: 11.4</p></blockquote><h3 id="加速后"><a href="#加速后" class="headerlink" title="加速后"></a>加速后</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">start_time = time.time()</span><br><span class="line">state_dict = torch.load(<span class="string">&#x27;checkpoint.pth&#x27;</span>, mmap=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Loading the state_dict took <span class="subst">&#123;time.time() - start_time:<span class="number">.2</span>f&#125;</span> seconds&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;After Load the state_dict:&#x27;</span>)</span><br><span class="line">print_usage()</span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="keyword">with</span> torch.device(<span class="string">&#x27;meta&#x27;</span>):</span><br><span class="line">  model = BigModel(size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Init the model took <span class="subst">&#123;time.time() - start_time:<span class="number">.2</span>f&#125;</span> seconds&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;After Init the model:&#x27;</span>)</span><br><span class="line">print_usage()</span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line">model.load_state_dict(state_dict, assign=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Loading the state_dict to model took <span class="subst">&#123;time.time() - start_time:<span class="number">.2</span>f&#125;</span> seconds&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;After Load the state_dict to model:&#x27;</span>)</span><br><span class="line">print_usage()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    <span class="built_in">input</span> = torch.randn(<span class="number">1</span>, size)</span><br><span class="line">    output = model(<span class="built_in">input</span>)</span><br><span class="line">    <span class="built_in">print</span>(output)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;One time forward <span class="subst">&#123;time.time() - start_time:<span class="number">.2</span>f&#125;</span> seconds&#x27;</span>)</span><br><span class="line">    print_usage()</span><br></pre></td></tr></table></figure><blockquote><p>Before Load the state_dict:</p><p>memory: 0.34 GB</p><p>CPU percent: 9.1</p><p>Loading the state_dict took 0.11 seconds</p><p>After Load the state_dict:</p><p>memory: 0.34 GB</p><p>CPU percent: 6.1</p><p>Init the model took 0.00 seconds</p><p>After Init the model:</p><p>memory: 0.34 GB</p><p>CPU percent: 4.3</p><p>Loading the state_dict to model took 0.00 seconds</p><p>After Load the state_dict to model:</p><p>memory: 0.34 GB</p><p>CPU percent: 10.0</p><p>tensor([[ 0.0080, -0.0017, -0.0027, …, -0.0011, 0.0097, -0.0048]],</p><p>&#x20;grad_fn=\<AddmmBackward0>)</p><p>One time forward 48.37 seconds</p><p>memory: 372.85 GB</p><p>CPU percent: 5.2</p><p>tensor([[ 0.0038, 0.0014, -0.0076, …, -0.0016, 0.0004, -0.0018]],</p><p>&#x20;grad_fn=\<AddmmBackward0>)</p><p>One time forward 3.28 seconds</p><p>memory: 372.86 GB</p><p>CPU percent: 13.4</p></blockquote><p>通过上面的对比, 加速前100B模型加载时间为</p><p>852.06 + 518.15 + 125.63 = 1495(s) = 25 (min)</p><p>而使用 <code>mmap</code> + <code>meta device</code> 加载几乎没有时间开销, 只有模型真正运行时才会从硬盘拷贝权重到CPU RAM</p><h2 id="相关link"><a href="#相关link" class="headerlink" title="相关link"></a>相关link</h2><ul><li><a href="https://pytorch.org/tutorials/recipes/recipes/module_load_state_dict_tips.html">Tips for Loading an nn.Module from a Checkpoint — PyTorch Tutorials 2.3.0+cu121 documentation</a></li><li><a href="https://huggingface.co/docs/accelerate/usage_guides/big_modeling">Handling big models for inference</a></li><li><a href="https://huggingface.co/docs/accelerate/concept_guides/big_model_inference">Load big models into memory</a></li><li><a href="https://huggingface.co/docs/accelerate/v0.30.0/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch">Working with large models</a></li><li><a href="https://huggingface.co/docs/transformers/big_models">Instantiate a big model</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 《三分算法,三分系统,三分产品,一分销售》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mlsys </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ML system 入坑指南</title>
      <link href="2023/02/21/MLsys/"/>
      <url>2023/02/21/MLsys/</url>
      
        <content type="html"><![CDATA[<h1 id="ML-system-入坑指南"><a href="#ML-system-入坑指南" class="headerlink" title="ML system 入坑指南"></a>ML system 入坑指南</h1><p>最近ChatGpt大火,越来越多开始关注大模型2,但对于大模型落地而言,除了先进的算法,其背后的MLsystem(机器学习系统), 从分布式训练到高效推理的完整链路同样重要, 好的基础设施是应用爆发的基础.</p><p>作为一个入坑MLsys快两年半的练习生, 本文主要围绕自己学习的经历来构筑,会持续更新,希望能给希望入坑的新人一个指引,也给非Mlsys背景但感兴趣的其他领域的同学一些启发.</p><hr><h2 id="Course"><a href="#Course" class="headerlink" title="Course"></a>Course</h2><p>首先是课程,入坑MLsys,基本的计算机背景知识比如数据结构就不多聊了,更多讲讲一些更加专业性的进阶课程,</p><h3 id="Operating-System"><a href="#Operating-System" class="headerlink" title="Operating System"></a>Operating System</h3><h4 id="南京大学JYY-OS"><a href="#南京大学JYY-OS" class="headerlink" title="南京大学JYY OS"></a>南京大学JYY OS</h4><p>南京大学JYY老师开的操作系统课内容非常硬核, workload巨大,课程质量比肩四大</p><h4 id="MIT-6-S081"><a href="#MIT-6-S081" class="headerlink" title="MIT 6.S081"></a>MIT 6.S081</h4><p>MIT经典OS课,资料,lab都非常全</p><ul><li><a href="https://pdos.csail.mit.edu/6.828/2020/schedule.html">课程主页</a></li><li><a href="https://mit-public-courses-cn-translatio.gitbook.io/mit6-s081/">MIT 6.S081 中文 Tutorial Book</a></li></ul><h3 id="Parallel-computing"><a href="#Parallel-computing" class="headerlink" title="Parallel computing"></a>Parallel computing</h3><h4 id="CMU15418-Parallel-computing"><a href="#CMU15418-Parallel-computing" class="headerlink" title="CMU15418 Parallel computing"></a>CMU15418 Parallel computing</h4><p>并行计算非常好的入门课,内容硬核,workload巨大,涉及现代多处理器,CPU加速比如SIMD,分布式通讯协议MPI,GPU加速Cuda编程,异构计算,同步,Cache</p><ul><li><a href="http://15418.courses.cs.cmu.edu/spring2016/">课程主页</a></li></ul><h4 id="UCB-cs267-Applications-of-Parallel-Computers"><a href="#UCB-cs267-Applications-of-Parallel-Computers" class="headerlink" title="UCB cs267 Applications of Parallel Computers"></a>UCB cs267 Applications of Parallel Computers</h4><p>HPC祖师爷 Jim Demmel 22 spring最新版本,</p><ul><li><a href="https://sites.google.com/lbl.gov/cs267-spr2022?pli=1">课程主页</a></li></ul><h3 id="Distributed-system"><a href="#Distributed-system" class="headerlink" title="Distributed system"></a>Distributed system</h3><h4 id="MIT6-824分布式系统"><a href="#MIT6-824分布式系统" class="headerlink" title="MIT6.824分布式系统"></a>MIT6.824分布式系统</h4><p>这门课推荐的人也非常多了,用go实现,了解传统的分布式系统知识和历史对现代的分布式机器学习系统的学习还是有一定的帮助,不过对于做MLsys不是必须.</p><ul><li><a href="https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/">课程主页</a></li></ul><h3 id="MLSystem"><a href="#MLSystem" class="headerlink" title="MLSystem"></a>MLSystem</h3><h4 id="CMU-DL-System"><a href="#CMU-DL-System" class="headerlink" title="CMU DL System"></a>CMU DL System</h4><p>陈天奇老师的课,涉及nn库实现,自动微分,GPU加速,模型部署和部分AI编译内容,内容除了分布式训练涉及的不够,基础的MLsys还是非常全面的.</p><ul><li><a href="https://dlsyscourse.org/">课程主页</a></li></ul><h4 id="Mini-torch"><a href="#Mini-torch" class="headerlink" title="Mini torch"></a>Mini torch</h4><p>完全用python实现的简单torch版本,涉及自动微分,张量,GPU加速.适合新手入门</p><ul><li><a href="https://minitorch.github.io/">课程主页</a></li></ul><h4 id="机器学习系统-设计和实现"><a href="#机器学习系统-设计和实现" class="headerlink" title="机器学习系统:设计和实现"></a>机器学习系统:设计和实现</h4><p>华为Mindspore团队(没错,就是我打过工的Team)和一群大佬AP搞的, 计算图,编译器前后端,分布式训练都有涉及,内容比较全面,比较适合有一定基础的人阅读或者作为工具书.</p><ul><li><a href="https://openmlsys.github.io/#">主页</a></li></ul><h4 id="System-for-AI"><a href="#System-for-AI" class="headerlink" title="System for AI"></a>System for AI</h4><p>微软发起的,目前还在快速迭代更新的工具书,舍和补全基础.</p><ul><li><a href="https://github.com/microsoft/AI-System">主页</a></li></ul><h3 id="AI-Compilation"><a href="#AI-Compilation" class="headerlink" title="AI Compilation"></a>AI Compilation</h3><h4 id="Machine-Learning-Compilation"><a href="#Machine-Learning-Compilation" class="headerlink" title="Machine Learning Compilation"></a>Machine Learning Compilation</h4><p>还是陈天奇老师的课,以TVM为基础, AI编译器这样前沿的领域为数不多的课.</p><ul><li><a href="https://mlc.ai/summer22-zh/">课程主页</a></li></ul><h3 id="Large-model"><a href="#Large-model" class="headerlink" title="Large model"></a>Large model</h3><p>对于做ML system 的同学而言,了解一些最新的算法也是非常必要的,不用过度关系一些fancy的技巧,更多关注模型架构,参数,大的范式上的变化即可.<br>算法的业界进展确实太快了,很难有系统的课,某些顶级大学会用讲座的形式开展,去讲GPT,PLAM这样的大模型, 看论文和官方网站,blog是更好的选择.</p><p>可以看李沐老师论文精讲去follow一些比较新且影响力巨大的工作, Muli is all you need !!!</p><ul><li><a href="https://github.com/mli/paper-reading">paper-reading github 主页</a></li></ul><h3 id="Large-model-training-amp-paper"><a href="#Large-model-training-amp-paper" class="headerlink" title="Large model training &amp; paper"></a>Large model training &amp; paper</h3><p>这块目前还没有比较系统的课,大规模的分布式训练开始应用也就这几年的事情,也是MLsys领域的最大热点,这里简单总结一下需要掌握的知识点和参考论文</p><ul><li>Data Parallel(数据并行)</li><li>Distributed Data Parallel(分布式数据并行)<ul><li><a href="https://arxiv.org/abs/2006.15704">PyTorch Distributed: Experiences on Accelerating Data Parallel Training</a></li></ul></li><li>Mix precise Training(混合精度训练)<ul><li><a href="https://arxiv.org/abs/1710.03740">Mix precise Training</a></li></ul></li><li>Zero Optimizer(零冗余优化器)<ul><li><a href="https://arxiv.org/abs/2104.07857">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</a></li></ul></li><li>Tensor Parallel(张量并行)<ul><li>1D并行:<a href="https://arxiv.org/abs/1909.08053">Megatron-LM</a></li></ul></li><li>Pipeline Parallel(流水并行)<ul><li><a href="https://arxiv.org/abs/1811.06965">GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</a></li></ul></li><li>Auto Parallel(自动并行)<ul><li><a href="https://arxiv.org/abs/2201.12023">Alpa:Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning</a></li></ul></li><li>Sequence Parallel(序列并行)<ul><li><a href="https://arxiv.org/abs/2105.13120">Sequence Parallelism: Long Sequence Training from<br>  System Perspective</a></li></ul></li><li>Large batchsize(超大batch size)<ul><li><a href="https://arxiv.org/abs/1708.03888">LARS:Large Batch Training of Convolutional Networks</a></li></ul></li><li>MOE(混合专家模型)<ul><li><a href="https://arxiv.org/abs/2006.16668">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</a></li></ul></li><li>Kernal Fusion(算子融合)<ul><li><a href="https://arxiv.org/abs/2205.14135">Flash attention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></li></ul></li><li>Optimizer Fusion(优化器融合)<ul><li><a href="https://arxiv.org/abs/2104.00237">OPTIMIZER FUSION: EFFICIENT TRAINING WITH BETTER LOCALITY AND PARALLELISM</a></li></ul></li><li>Activation checkpoint<ul><li><a href="https://arxiv.org/abs/1604.06174">Training Deep Nets with Sublinear Memory Cost</a></li></ul></li><li>Fine-tune accelerate(微调加速)<ul><li><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a></li></ul></li><li>Isomorphic training(异构训练)<ul><li><a href="https://arxiv.org/abs/2108.05818">PatrickStar: Parallel Training of Pre-trained Models via<br>  Chunk-based Dynamic Memory Management</a></li></ul></li><li>Asynchronous Distributed Dataflow(异步数据流)<ul><li><a href="https://arxiv.org/abs/2203.12533">PATHWAYS: ASYNCHRONOUS DISTRIBUTED DATAFLOW FOR ML</a></li></ul></li><li>Distributed Scheduling(分布式调度)<ul><li><a href="https://arxiv.org/abs/1712.09381">RLlib: Abstractions for Distributed Reinforcement Learning</a></li></ul></li><li>Quant(量化)<ul><li><a href="https://arxiv.org/abs/2211.10438">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a></li></ul></li><li>大语言模型推理<ul><li><a href="https://github.com/FMInference/FlexGen/blob/main/docs/paper.pdf">FlexGen: High-throughput Generative Inference of Large Language Models with a Single GPU</a></li></ul></li></ul><blockquote><p>每个细分领域的论文还有很多,不一一列举了,对于入坑来说,抓住主线即可.</p></blockquote><h2 id="Programming-Languages"><a href="#Programming-Languages" class="headerlink" title="Programming Languages"></a>Programming Languages</h2><h3 id="C"><a href="#C" class="headerlink" title="C++"></a>C++</h3><ul><li><a href="https://www.hahack.com/codes/cmake/">Cmake</a></li><li><a href="https://changkun.de/modern-cpp/zh-cn/01-intro/">现代CPP</a></li></ul><h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><p>Python基础课这个太多了,不作推荐了,做MLsys比较需要掌握用python调用C,比如Cpython,pybind,以及一些python高级特性,比如hook,装饰器</p><ul><li><a href="https://github.com/pybind/pybind11">pybind</a></li></ul><h3 id="Cuda"><a href="#Cuda" class="headerlink" title="Cuda"></a>Cuda</h3><p>这个可以参考的也比较多,英伟达的官方手册永远是最好的参考.</p><ul><li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/">Cuda programming guide</a></li></ul><h3 id="OpenCL"><a href="#OpenCL" class="headerlink" title="OpenCL"></a>OpenCL</h3><p>对于非Nvidia芯片的设备,比如手机Soc,移动端推理芯片大多不支持cuda,那么用OpenCL来做异构加速就是一个更通用的方案</p><ul><li><a href="https://www.bookstack.cn/read/Heterogeneous-Computing-with-OpenCL-2.0/README.md">OpenCL异构计算</a></li></ul><h1 id="军火库"><a href="#军火库" class="headerlink" title="军火库"></a>军火库</h1><p>这里会简单总结我接触或使用和直接参与开发的Mlsys的军火库,会持续更新.</p><h2 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h2><p>对于MLsys这样前沿的领域而言,因为很多方面并没有足够的资料,经常被迫去直接学习源码,以实际的case作为学习手段也是非常好的方式.这里简单归类一下我遇到过的MLsys,大多数处于简单了解和使用,有少部分比较深入看过源码.</p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><h4 id="TensorRT"><a href="#TensorRT" class="headerlink" title="TensorRT"></a>TensorRT</h4><p>英伟达的推理方案, 目前整体上在英伟达GPU上做的最好的推理框架,比较是自己的卡.</p><ul><li><a href="https://github.com/NVIDIA/TensorRT">github</a></li></ul><h4 id="AI-Template"><a href="#AI-Template" class="headerlink" title="AI Template"></a>AI Template</h4><p>FaceBook刚搞的一个推理库,在很多硬件上速度性能都超过TensorRT, 还比较新的框架</p><ul><li><a href="https://github.com/facebookincubator/AITemplate">github</a></li></ul><h3 id="Severing"><a href="#Severing" class="headerlink" title="Severing"></a>Severing</h3><h4 id="triton-inference-server"><a href="#triton-inference-server" class="headerlink" title="triton-inference-server"></a>triton-inference-server</h4><p>英伟达的ML serving框架,比较成熟</p><ul><li><a href="https://github.com/triton-inference-server/server">github</a></li></ul><h4 id="clip-as-service"><a href="#clip-as-service" class="headerlink" title="clip-as-service"></a>clip-as-service</h4><p>Jina-AI做的,一家中国 start up, 在mass(model as service)上是一个非常不错的落地产品,特别喜欢</p><ul><li><a href="https://github.com/jina-ai/clip-as-service">github</a></li></ul><h3 id="Mobile-inference"><a href="#Mobile-inference" class="headerlink" title="Mobile inference"></a>Mobile inference</h3><p>移动端推理是我比较深入做过的,对其底层了解的比较多</p><h4 id="Mindsporelite"><a href="#Mindsporelite" class="headerlink" title="Mindsporelite"></a>Mindsporelite</h4><p>我有幸参与写过的推理引擎,对于全流程在mindspore上做的体验还是不错的.</p><ul><li><a href="https://gitee.com/mindspore/mindspore">gitee</a></li></ul><h4 id="MNN"><a href="#MNN" class="headerlink" title="MNN"></a>MNN</h4><p>阿里达摩院做的,我写mindsporelite的遇到问题的时候经常被mentor叫去学习一下友商的代码,CPU的一些kernel用汇编写的,这点映像非常深刻,代码质量非常高.</p><ul><li><a href="https://github.com/alibaba/MNN">github</a></li></ul><h4 id="TensorFlowlite"><a href="#TensorFlowlite" class="headerlink" title="TensorFlowlite"></a>TensorFlowlite</h4><p>集成在Tensorflow的移动端推引擎,国际上应该是最早做的移动端推理.没错,TFlite的大哥就是那个从谷歌跑路重回斯坦福读博的皮特·沃登,他写了TinyML这本书,对整个移动端推理都是有重要意义的.我也从这学习了不少.google的代码质量太高了,看注释都能学很多.</p><ul><li><a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite">github</a></li></ul><h4 id="NCNN"><a href="#NCNN" class="headerlink" title="NCNN"></a>NCNN</h4><p>国内做的最早的端侧推理引擎,腾讯搞的,不得不说,很多东西还是需求驱动, 靠各种移动APP为主要产品的中国互联网公司,移动推理引擎做的都不错.</p><ul><li><a href="https://github.com/Tencent/ncnn">github</a></li></ul><h2 id="DeepLearning-Framework"><a href="#DeepLearning-Framework" class="headerlink" title="DeepLearning Framework"></a>DeepLearning Framework</h2><p>大的深度学习的框架的介绍太多了,不一一介绍了,大同小异,做ML系统多少都会看看各种框架做的一些新特性,Torch2.0的一些东西可以关注.</p><ul><li><a href="https://github.com/pytorch/pytorch">Torch</a></li><li><a href="https://github.com/tensorflow/tensorflow">TensorFlow</a></li><li><a href="https://gitee.com/mindspore/mindspore">Mindspore</a></li><li><a href="https://github.com/google/jax">Jax</a></li><li><a href="https://github.com/Oneflow-Inc/oneflow">oneflow</a></li><li><a href="https://github.com/PaddlePaddle/Paddle">paddle</a></li><li><a href="https://github.com/unifyai/ivy">ivy</a></li></ul><h2 id="AI-compiler"><a href="#AI-compiler" class="headerlink" title="AI compiler"></a>AI compiler</h2><p>编译这块我接触的不多,这两个项目可以参考</p><h3 id="TVM"><a href="#TVM" class="headerlink" title="TVM"></a>TVM</h3><ul><li><a href="https://github.com/apache/tvm">github</a></li></ul><h3 id="BladeDISC"><a href="#BladeDISC" class="headerlink" title="BladeDISC"></a>BladeDISC</h3><ul><li><a href="https://github.com/alibaba/BladeDISC">github</a></li></ul><h2 id="Distributed-training"><a href="#Distributed-training" class="headerlink" title="Distributed training"></a>Distributed training</h2><p>对于大模型而言,分布式训练是比不缺的,除了最基本的分布式数据并行,各种混合并行策略,显存优化策略必不可缺,也因此出现了一系列的大模型训练框架.</p><h3 id="ColossalAI"><a href="#ColossalAI" class="headerlink" title="ColossalAI"></a>ColossalAI</h3><p>All in 参与的项目,刚刚star接近15k了,在torch生态下支持各种并行和显存优化策略,给自己打个广告,欢迎各位朋友star,关注</p><ul><li><a href="https://github.com/hpcaitech/ColossalAI">github</a></li></ul><h3 id="Megatron-LM"><a href="#Megatron-LM" class="headerlink" title="Megatron-LM"></a>Megatron-LM</h3><p>NVIDIA做的模型并行库,也是最早开源的模型并行,但对缺乏分布式训练背景的人使用不太友好.</p><ul><li><a href="https://github.com/NVIDIA/Megatron-LM">Github</a></li></ul><h3 id="Deepspeed"><a href="#Deepspeed" class="headerlink" title="Deepspeed"></a>Deepspeed</h3><p>微软的大模型训练框架,核心技术是Zero infinity相关的一系列paper,使用Megatron-LM作为张量并行的支持</p><ul><li><a href="https://github.com/microsoft/DeepSpeed-MII">github</a></li></ul><h3 id="huggingface-accelerate"><a href="#huggingface-accelerate" class="headerlink" title="huggingface accelerate"></a>huggingface accelerate</h3><p>huggingface的加速器,对各种不同硬件做了兼容,在huggingface生态下非常好用,在分布式上做了比较多的封装,可以直接调用Deepspeed.</p><ul><li><a href="https://github.com/huggingface/accelerate">github</a></li></ul><h3 id="Bagua"><a href="#Bagua" class="headerlink" title="Bagua"></a>Bagua</h3><p>Bagua在多机通讯上做了非常多的工作,对allreduce等分布式通讯做了不少优化.</p><ul><li><a href="https://github.com/BaguaSys/bagua">github</a></li></ul><h3 id="lightning"><a href="#lightning" class="headerlink" title="lightning"></a>lightning</h3><p>lightning集成了各种分布式后端,可以很方便的启动各种分布式策略,lightning本身是一套更好的MLflow,设计理念是算法和工程分开,提供了大量自定义hook,对于大型AI项目而言,是个不错的选择,但是学习门槛不低,对团队人员水平要求比较高.</p><ul><li><a href="https://github.com/Lightning-AI/lightning">github</a></li></ul><h2 id="Distributed-Communication"><a href="#Distributed-Communication" class="headerlink" title="Distributed Communication"></a>Distributed Communication</h2><p>在nccl之前MPI是分布式通讯的主要方式,简单了解一下还是有必要的</p><h3 id="MPI-the-Message-Passing-Interface"><a href="#MPI-the-Message-Passing-Interface" class="headerlink" title="MPI(the Message Passing Interface)"></a>MPI(the Message Passing Interface)</h3><ul><li><a href="https://mpitutorial.com/tutorials/">MPI Tutorials</a></li></ul><h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><p>最后简单给几个Tips,</p><ul><li>尽可能参与到业界的项目去,多注重落地,在实验玩一些很fancy但不落地的东西意义不大,目前MLsys业界比学界跑的快太多了,学界比较有影响力的工作往往也是何大公司合作的,比如前面提到的<a href="https://arxiv.org/abs/2201.12023">Alpa</a>的实验就是在Google的TPU集群和AWS上做的,一般的lab更本没有条件.</li><li>及时关注各种Blog,新的论文,这个领域每天都有新的东西出来,比如我就比较喜欢每天去twitter,一些Discord channel挖宝,总能带来一些惊喜</li><li>Mlsys是一个交叉全面的领域,除了软件和算法,懂点基本的硬件,学会做产品也很重要,更多时候易用性大于性能.</li><li>新的东西肯定是看不完的,抓住主干即可,在自己做的部分保证专注</li><li>拥抱开源社区,不管是github还是huggingface,你永远想象不出开源社区的老哥的创造力</li></ul>]]></content>
      
      
      <categories>
          
          <category> 《三分算法,三分系统,三分产品,一分销售》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mlsys </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>流水线并行论文总结</title>
      <link href="2023/02/21/pipeline-paralism/"/>
      <url>2023/02/21/pipeline-paralism/</url>
      
        <content type="html"><![CDATA[<h3 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h3><p>模型并行 模型并行分为两种：流水线并行和张量并行，也可以称作算子内并行（intra-operator parallelism）和算子间并行（interoperator parallelism）（Alpa 中的叫法）</p><ul><li>流水线并行（pipeline model parallel）</li><li>把模型不同的层放到不同设备之上，比如前面几层放到一个设备之上，中间几层放到另外一个设备上，最后几层放到第三个设备之上。</li><li>张量并行</li><li>层内分割，把某一个层做切分，放置到不同设备之上，也可以理解为把矩阵运算分配到不同的设备之上，比如把某个矩阵乘法切分成为多个矩阵乘法放到不同设备之上。</li></ul><p><img src="https://pica.zhimg.com/80/v2-5849c5b64d649fd4c4956818cba1a03a_1440w.png?source=d16d100b" alt=""></p><p>添加图片注释，不超过 140 字（可选）</p><p>通讯开销对比</p><ul><li>张量并行：</li><li>通信发生在每层的前向传播和后向传播过程之中，通信类型是all-reduce，不但单次通信数据量大，并且通信频繁。</li><li>通常适合节点内使用， 通过 NVLink 来进行加速</li><li>流水线并行</li><li>通信在流水线阶段相邻的切分点之上，通信类型是P2P通信，单次通信数据量较少但是比较频繁，而且因为流水线的特点，会产生GPU空闲时间，这里称为流水线气泡（Bubble）。</li><li>通常更适合节点间使用，一般通过 Infiniband 交换机进行连接。</li></ul><p>流水并行（pipeline parallelism）的核心问题</p><ol><li>流水线并行的一个挑战是确保各个阶段在计算工作量方面得到适当平衡。 如果一个阶段的执行时间比其他阶段长得多，它可能会造成瓶颈，从而减慢整个训练的速度。 为了解决这个问题，可能需要调整每个阶段处理的数据批次的大小或实施动态负载平衡技术。</li><li>流水线并行的另一个挑战是管理不同处理单元之间的通信。 在某些情况下，可能需要在不同阶段之间传输数据或中间结果，这样会引入通信开销和延迟。 为了解决这个问题，可能需要使用高效的数据传输协议或重叠计算和通信以最小化通信延迟的影响。 基础流水并行框架</li></ol><p><img src="https://picx.zhimg.com/80/v2-a3a9d7cd1773f6cfa57385901bcefe1a_1440w.png?source=d16d100b" alt=""></p><p>添加图片注释，不超过 140 字（可选）</p><ul><li>计算资源利用率低：在任何一个时间，只有一个计算设备在执行计算，而其他的计算设备都处于空闲状态，每个计算设备在忽略通信时间的情况下，N个计算设备的情况下， 总的时间利用率只有 1/N, 如果增加更多的计算设备，那么每个设备的利用率会更低。因此需要对正在空闲的计算设备指定任务以此增加计算设备的利用率。这种下游设备需要长期持续处于空闲状态，等待上游设备的计算完成，才可以开始计算的现象被称为Bubble。</li><li>计算和通信顺序执行：当在不同设备之间传输前向计算的中间结果和反向传播时的梯度时，没有任何的计算设备在计算，这也导致了资源的浪费，因此计算和通信需要仔细的设计来增加设备的使用效率。</li><li>如何划分层：由于需要将模型的一系列层划分到不同设备之间，不同的划分策略往往会影响计算的性能，因此如何对模型进行划分也会对性能产生重要的影响。</li></ul><h3 id="GEMS-（SC-20）"><a href="#GEMS-（SC-20）" class="headerlink" title="GEMS （SC 20）"></a><a href="https://ieeexplore.ieee.org/document/9355254">GEMS （SC 20）</a></h3><p><img src="https://picx.zhimg.com/80/v2-57a0fc14a966efb9effab23bf86d3ab2_1440w.png?source=d16d100b" alt=""></p><p>添加图片注释，不超过 140 字（可选）</p><p><a href="https://arxiv.org/pdf/1811.06965.pdf">Gpipe （NeurIPS 19）</a></p><p><img src="https://picx.zhimg.com/80/v2-7dc7d9e2309d861a4f55d5757072b5dc_1440w.png?source=d16d100b" alt=""></p><p>添加图片注释，不超过 140 字（可选）</p><p>&#x20;假设一个网络有L层，每一层L(i)包含对应的前向计算函数和一系列参数。GPipe对每层的计算有个成本计算函数， 给定划分的数量K，GPipe会根据成本计算函数将L层网络会被分成K组，每组中包含一定连续的层数，相邻的两组设备之间的通信操作也会自动设置好。在前向计算的过程中，GPipe会将batch 划分成M份相等的micro-batch，这样M份micro-batch就可以在M个计算设备之间进行流水。在反向传播计算期间，每个micro-batch的梯度是基于用于前向传播计算的模型参数计算的。 在最后对于每个micro-batch，将来自所有 M 个micro-batch的梯度累加起来并应用于更新所有计算设备的模型参数。</p><p><a href="https://arxiv.org/abs/2007.01045">DAPPLE（PPoPP‘21）</a></p><p><img src="https://pic1.zhimg.com/80/v2-62ba8b5b3b3d023db28b39e5040c3396_1440w.png?source=d16d100b" alt=""></p><p>添加图片注释，不超过 140 字（可选）</p><p>&#x20;DAPPLE 是一个同步分布式训练框架，它结合了大型 DNN 模型的数据并行性和流水并行性。主要思想是更早地安排反向传播任务从而释放用于存储相应前向传播任务产生的激活的内存。DAPPLE相较于GPipe改变了排布，没有减小bubble rate，但是减小了activation的占用，对比GPipe的调度机制。 DAPPLE不是一次性执行所有 M 个mini-batch，而是在开始时执行K 个mini-batch (K &lt; M) 从而开始释放内存压力，同时到达内存使用的最高点。其次，严格安排一个mini-batch的前向传播后，紧接着就是一个反向传播，以保证反向传播可以提前执行。 下图 显示了GPipe 和 DAPPLE 中的内存消耗如何随时间变化。 在前K个min-batch, DAPPLE和 GPiped的内存消耗相同，直到第 K 个mini-batch, 然后由于更早的反向传播而DAPPLE达到内存使用的最大值。具体来说，严格控制执行前向传播和反向传播的顺序，由前向传播执行一个mini-batch之后，activation占用的内存由后向传播执行所释放。相比之下，GPipe 的峰值内存在K个mini-batch之后持续增长没有提早释放。 此外，DAPPLE 并不牺牲流水线训练效率。</p><p><img src="https://pic1.zhimg.com/80/v2-af16bce6e07df7817cd5c2ab493e05ed_1440w.png?source=d16d100b" alt=""></p><p>添加图片注释，不超过 140 字（可选）</p><p><a href="https://arxiv.org/abs/1806.03377">PipeDream （SOSP 19）</a></p><p><img src="https://picx.zhimg.com/80/v2-45cd5329a9fd11a2bd71afc17671e309_1440w.png?source=d16d100b" alt=""></p><p>添加图片注释，不超过 140 字（可选）</p><p>&#x20;PipeDream 采用1F1B（One Forward pass followed by One Backward pass）模式，一种前向计算和反向计算交叉进行的方式。在 1F1B 模式下，前向计算和反向计算交叉进行，可以及时释放不必要的中间变量。 PipeDream 和 GPipe 之间的区别很明显：PipeDream 应用异步向后更新，而 GPipe 应用同步向后更新。 PipeDream-Flush and PipeDream-2BW（ICML21） PipeDream 使用权重存储方案来确保相同输入的前向和后向传播中使用相同的权重版本。 在最坏的情况下，隐藏的权重版本总数为 d，其中， d 是流水线深度，这对于大模型来说太高了。 而且使用 PipeDream 默认的权重更新语义，每个阶段（state）的权重更新都有不同的延迟项；同时，流水线内不会执行累积。</p><p><img src="https://picx.zhimg.com/80/v2-81644db22a7d1cbaa40a0183fff877a0_1440w.png?source=d16d100b" alt=""></p><p>添加图片注释，不超过 140 字（可选）</p><p><img src="https://pica.zhimg.com/80/v2-861f07004dab9864a44ad193efde5f7f_1440w.png?source=d16d100b" alt=""></p><p>添加图片注释，不超过 140 字（可选）</p><p>Virtual pipeline Megatron-LM</p><p><img src="https://pica.zhimg.com/80/v2-e9f546935a34e0cbb74b694975ee68f0_1440w.png?source=d16d100b" alt=""></p><p>添加图片注释，不超过 140 字（可选）</p><p>&#x20;非交错式 schedule 可分为三个阶段。第一阶段是热身阶段，处理器进行不同数量的前向计算。在接下来的阶段，处理器进行一次前向计算，然后是一次后向计算。处理器将在最后一个阶段完成后向计算。这种模式比 GPipe 更节省内存。然而，它需要和 GPipe 一样的时间来完成一轮计算。 交错 Schedule要求microbatches的数量是流水线阶段的整数倍。在这个 schedule 中，每个设备可以对多个层的子集（称为模型块）进行计算，而不是一个连续层的集合。具体来看，之前设备1拥有层1-4，设备2拥有层5-8，以此类推；但现在设备1有层1,2,9,10，设备2有层3,4,11,12，以此类推。 在该模式下，流水线上的每个设备都被分配到多个流水线阶段，每个流水线阶段的计算量较少。 Virtual pipeline 降低了bubble，提升了通信次数。 <a href="https://arxiv.org/abs/2107.06925">Chimera （SC’21）</a></p><p><img src="https://picx.zhimg.com/80/v2-3d84b04a8101ec4c1c0c717c85365ba6_1440w.png?source=d16d100b" alt=""></p><p>添加图片注释，不超过 140 字（可选）</p><p><img src="https://picx.zhimg.com/80/v2-29b8d8e0d56dddd4bb7dfa6ad920ba20_1440w.png?source=d16d100b" alt=""></p><p>添加图片注释，不超过 140 字（可选）</p><p>&#x20;Chimera 使用双流水线进行交叉排布，极大地减小了的bubble rate，但是增加了一倍weight的显存占用</p><p><img src="https://pica.zhimg.com/80/v2-3d137e6f51ecec8e32b9a6344dee5030_1440w.png?source=d16d100b" alt=""></p><p>添加图片注释，不超过 140 字（可选）</p><p><img src="https://picx.zhimg.com/80/v2-10fb24cd68e571085eac7e1cdcc5918e_1440w.png?source=d16d100b" alt=""></p><p>添加图片注释，不超过 140 字（可选）</p><p><a href="https://arxiv.org/abs/2201.12023">Alpa （OSDI22）</a></p><p><img src="https://picx.zhimg.com/80/v2-4f1bb0fc42662a22ef1fbe3f4cba8b03_1440w.png?source=d16d100b" alt=""></p><p>添加图片注释，不超过 140 字（可选）</p><p>&#x20;Alpa同时考虑PP+TP+DP，Alpa 将并行性视为两个层次级别：算子内并行（intra-operator parallelism）和算子间并行（interoperator parallelism。Alpa使用整数线性规划（ILP）完成算子间并行规划，动态规划算法来完成算子外并行规划。 常见分布式训练框架Pipeline parallelism方案</p><ul><li>在 PyTorch 中，采用的是GPipe方案。使用的是F-then-B调度策略。</li><li>在 DeepSpeed 中，采用的是PipeDream-Flush，使用的是非交错式1F1B调度策略</li><li>在 Megatron-LM 中，基于PipeDream-Flush进行了改进，提供了一种交错式1F1B方案。</li><li>在 Colossal-AI 中，基于Megatron-LM的交错式1F1B方案，提供了非交错(PipelineSchedule) 和交错(InterleavedPipelineSchedule) 调度策略。</li></ul><p>Reference</p><ol><li>GEMS：<a href="https://ieeexplore.ieee.org/document/9355254">https://ieeexplore.ieee.org/document/9355254</a></li><li>GPipe ：<a href="https://arxiv.org/pdf/1811.06965.pdf">https://arxiv.org/pdf/1811.06965.pdf</a></li><li>PipeDream：<a href="https://arxiv.org/abs/1806.03377">https://arxiv.org/abs/1806.03377</a></li><li>PipeDream-Flush and PipeDream-2BW：<a href="https://arxiv.org/abs/2006.09503">https://arxiv.org/abs/2006.09503</a></li><li>Virtual pipeline Megatron-LM: <a href="https://arxiv.org/abs/2104.04473">https://arxiv.org/abs/2104.04473</a></li><li>DAPPLE ：<a href="https://arxiv.org/abs/2007.01045">https://arxiv.org/abs/2007.01045</a></li><li>Chimera ：<a href="https://arxiv.org/abs/2107.06925">https://arxiv.org/abs/2107.06925</a></li><li>Alpa：<a href="https://arxiv.org/abs/2201.12023">https://arxiv.org/abs/2201.12023</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 《三分算法,三分系统,三分产品,一分销售》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mlsys </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mixed Precision Training</title>
      <link href="2022/03/11/amp/"/>
      <url>2022/03/11/amp/</url>
      
        <content type="html"><![CDATA[<h1 id="Mixed-Precision-Training"><a href="#Mixed-Precision-Training" class="headerlink" title="Mixed Precision Training"></a>Mixed Precision Training</h1><p>2018年ICLR的文章，来自Nvidia，现在流行的混合精度训练的方案基本基于这篇文章，Nvidia针对Pytorch开发了Extension <a href="https://nvidia.github.io/apex/index.html">Apex</a>，可以完美地在Pytorch中进行混合精度训练，在Pytorch 1.6的版本中，这个特性被Pytorch官方merge到Pytorch中，Pytorch原生支持了amp（自动混合精度训练），解决了apex经常和Pytorch版本不兼容的问题</p><ul><li><p><a href="https://nvidia.github.io/apex/index.html">apex文档</a></p></li><li><p><a href="https://pytorch.org/docs/stable/amp.html">Pytorch.amp文档</a></p></li><li><p><a href="https://arxiv.org/pdf/1710.03740.pdf">论文地址</a></p></li></ul><h2 id="Absract"><a href="#Absract" class="headerlink" title="Absract"></a>Absract</h2><p>提升神经网络的规模通常会提高模型预测的准确率，但同时也会有更高的内存和计算要求。这篇文章主要介绍了在不降低模型的准确性并不修改超参数的条件下，使用半精度浮点数训练深度神经网络的方法。半精度训练可以将模型在GPU上的内存需求减半，并且可以加快运算速度。权重、激活和梯度都以IEEE半精度格式存储。由于这种格式的范围比单精度的要窄，所以我们提出了三种技术来防止关键信息的丢失。首先，我们建议维护一个单精度的权重副本，在每个优化器步骤后积累每个优化器步骤后的梯度（这个副本对于正向和反向传播来说被四舍五入为半精度）。第二，我们提出损失缩放，以保留小幅度的梯度值。第三，我们使用半精度计算，累积成单精度输出，在存储到内存之前将其转换为半精度。我们证明了所提出的方法在各种任务和现代大规模（超过1亿个参数）的模型架构中都能发挥作用。</p><h2 id="论文解决什么问题？"><a href="#论文解决什么问题？" class="headerlink" title="论文解决什么问题？"></a>论文解决什么问题？</h2><p>大模型需要更多的计算资源和内存开销，对网络参数进行低精度的表示和计算是解决上述问题对一个重要方法，网络对训练和推理速度取决于以下三个方面：</p><ul><li>计算带宽（arithmetic bandwidth）</li><li>内存带宽 （memory bandwidth</li><li>通信延迟 （communication latency）</li></ul><p>低精度低低参数表示可以很好地降低前两者地开销，现代地机器学习系统使用单精度（FP32）的格式，这篇文章希望在保持模型准确率不变的情况下使用半精度（FP16）进行训练<br> 并提出了三种防止模型精度损失的技术：</p><ul><li>为所有参数保存一个FP32的主副本，</li><li>梯度缩放（loss scaling）。</li><li>使用FP16运算并将结果累加到FP32到内存中。</li></ul><p>文章对CNN和RNN结构，训练了用于分类、回归和生成任务，应用包括图像分类、图像<br>生成、物体检测、语言建模、机器翻译和语音识别，都取得了不错的效果。</p><h2 id="论文解决问题到方法"><a href="#论文解决问题到方法" class="headerlink" title="论文解决问题到方法"></a>论文解决问题到方法</h2><h3 id="accumulating-FP16-products-into-FP32（使用FP16运算并将结果累加到FP32到内存中）"><a href="#accumulating-FP16-products-into-FP32（使用FP16运算并将结果累加到FP32到内存中）" class="headerlink" title="accumulating FP16 products into FP32（使用FP16运算并将结果累加到FP32到内存中）"></a>accumulating FP16 products into FP32（使用FP16运算并将结果累加到FP32到内存中）</h3><p>NVIDIA Volta GPU架构引入了Tensor Core指令，它将半精度矩阵相乘，将结果累加到单精度或半精度输出中。论文指出累积到单一精度对于获得良好的训练结果至关重要。在写入存储器之前，累计值将转换为半精度。cuDNN和CUBLAS库提供了各种依赖Tensor Cores进行算术运算的函数。</p><h3 id="loss-scaling（梯度缩放）"><a href="#loss-scaling（梯度缩放）" class="headerlink" title="loss scaling（梯度缩放）"></a>loss scaling（梯度缩放）</h3><p>训练 DNN 时会遇到四种类型的张量：激活、激活梯度、权重和权重梯度。根据经验，激活、权重和权重梯度落在以半精度表示的值幅度范围内。然而，对于一些网络，小幅度的激活梯度低于半精度范围。激活梯度不使用大多数半精度范围，它们往往是幅度低于 1 的小值。因此，我们可以通过将激活梯度乘以比例因子S将激活梯度“转移”到 FP16 可表示的范围内。确保梯度落在半精度可表示的范围内的一种非常有效的方法是将训练损失乘以比例因子。这仅添加了一个乘法，并且通过链式法则，它确保所有梯度都按比例放大（或向上移动）而无需额外成本。损失缩放确保恢复丢失为零的相关梯度值。 在权重更新之前，权重梯度需要按相同的因子S缩小。</p><h3 id="FP32-Master-Copy-of-Weights-保存一份FP32的权重副本"><a href="#FP32-Master-Copy-of-Weights-保存一份FP32的权重副本" class="headerlink" title="FP32 Master Copy of Weights(保存一份FP32的权重副本)"></a>FP32 Master Copy of Weights(保存一份FP32的权重副本)</h3><p>保存一个单精度浮点的权值备份。在训练过程中舍入到半精度，FP16在硬件实现中更快。（加速训练、减少硬件开销、存储的参数量增加了50%，但是由于减少了过程中的activation，所以总体来说还是减少了memory的消耗）。<br>假如单纯的使用FP16训练，精度降低了80%，所以要使用32位量化训练，但是参数更新过程使用16位。过程如下图所示。</p><p><img src="https://s2.loli.net/2022/03/03/g6ac9T8G4AXNdhE.png" alt="企业微信20220303-152858.png"></p>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> amp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文解读：OPTIMIZER FUSION - EFFICIENT TRAINING WITH BETTER LOCALITY AND PARALLELISM</title>
      <link href="2022/01/23/OPTIMIZER-FUSION/"/>
      <url>2022/01/23/OPTIMIZER-FUSION/</url>
      
        <content type="html"><![CDATA[<h1 id="OPTIMIZER-FUSION-EFFICIENT-TRAINING-WITH-BETTER-LOCALITY-AND-PARALLELISM"><a href="#OPTIMIZER-FUSION-EFFICIENT-TRAINING-WITH-BETTER-LOCALITY-AND-PARALLELISM" class="headerlink" title="OPTIMIZER FUSION: EFFICIENT TRAINING WITH BETTER LOCALITY AND PARALLELISM"></a>OPTIMIZER FUSION: EFFICIENT TRAINING WITH BETTER LOCALITY AND PARALLELISM</h1><p><a href="https://arxiv.org/abs/2104.00237">论文地址</a></p><p>2021 ICLR的文章，来自德克萨斯奥斯汀，提出了一种针对大规模训练的优化器融合训练方法。</p><h2 id="Absract"><a href="#Absract" class="headerlink" title="Absract"></a>Absract</h2><p>机器学习框架采用迭代优化器的方式来训练神经网络。传统的Eager execution的训练方式将可训练参数的更新与前向和后向计算分开。然而，这样的方式由于缺乏对数据本地性(data locality)和计算并行性(computation parallelism)的利用，引入了不可忽视的训练时间开销。在这项工作中，我们通过将优化器与前向或后向计算融合在一起，来更好地利用训练中的数据本地性和计算并行性。通过重新安排前向传播、梯度计算和参数更新的顺序，我们可以在不同的配置上的减少高达20%训练时间。由于我们的方法没有改变优化器的算法。这个方法可以被用作训练过程中的一个一般 “插件 “来应用。</p><h2 id="论文解决什么问题？"><a href="#论文解决什么问题？" class="headerlink" title="论文解决什么问题？"></a>论文解决什么问题？</h2><p>随机梯度下降和其变种是现在深度学习框架中主流的优化算法，Pytorch，TensorFlow，MXnet等框架通过对各个计算操作的自动微分来实现梯度的传播</p><p>Eager Execution(动态图模式)因其灵活性而在这些框架中被广泛采用。它通常将前向传播、梯度计算和参数更新为三个独立的阶段。在每次迭代中，首先执行前向计算。然后计算所有可学习参数的损失函数对应的梯度。最后，由一个指定的优化器来更新可学习的参数。尽管这个实现有一个直观和透明的过程，但可学习参数及其梯度在一次训练迭代中被读写多次，这样这些数据就不能被有效地重复使用。此外，梯度更新依赖于梯度计算，从而导致程序执行中的并行性降低。简而言之，在Eager Execution中有的对数据局部性和计算并行性的很大对提升潜力。</p><p>在本工作中，作者提出了前向融合和后向融合两种方法，即对前向计算、梯度计算和参数更新进行重新排序，以加速Eager Execution的训练过程。作者提出的方法将优化器与正向或向后计算相结合， 以更好地利用局部性和并行性。反向融合方法，由静态计算图编译驱动，其中优化器与梯度计算融合，尽早更新参数。正向融合方法将参数更新与下一个正融合计算融合，从而尽可能晚地更新可学习参数。</p><blockquote><p>什么是Eager Execution？TensorFlow 的 Eager Execution 是一种命令式编程环境，（其实就是TensorFlow的动态图模式，pytorch作为一个动态图框架，本身就是Eager Execution模式）可立即评估操作，无需构建图：操作会返回具体的值，而不是构建以后再运行的计算图。使用静态图模式的TensorFlow通常要先构建计算图，再调用tf.Session对象的run方法。<a href="https://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html">Eager Execution Google 官方博客解释</a></p></blockquote><h2 id="文章作出的贡献"><a href="#文章作出的贡献" class="headerlink" title="文章作出的贡献"></a>文章作出的贡献</h2><p>文章提出的训练方法的恭喜主要有以下三点</p><ul><li>高效性：文章的框架可以提高高达20%的训练速度。 </li><li>通用性：文章的方法在保留Eager Execution所有特征的情况下，可以与其他优化方法一起使用，不影响训练结果。</li><li>易用性：用户只需要很少的工程量就可以采用文章的方法来加速他们的训练。</li></ul><h2 id="问题的背景：计算图和图优化"><a href="#问题的背景：计算图和图优化" class="headerlink" title="问题的背景：计算图和图优化"></a>问题的背景：计算图和图优化</h2><h3 id="计算图分类"><a href="#计算图分类" class="headerlink" title="计算图分类"></a>计算图分类</h3><p>深度学习框架对计算图分为两类动态图和静态图。静态图将一个模型编译为一个静态的（计算图，并使用所有的神经网络信息来执行该图。例如，TensorFlow1.X 默认遵循这个例 程，这在执行之前需要一个预编译的图。动态图在Eager Execution（命令式）模 式下工作，该模式立即执行新遇到的计算节点，并逐步构建一个动态计算图。在每一步中，一个 计算节点被附加到当前的计算图中。PyTorch和 TensorFlow2，在默认情况下以Eager Execution 模式运行，使用户能够更容易和快速地开发机器学习模型。 </p><h3 id="图优化"><a href="#图优化" class="headerlink" title="图优化"></a>图优化</h3><p>机器学习框架中的工程师和从业者已经提出并实现了许多计算图优化器，但现有的图优化器需要整个计算图的信息，这意味着优化器融合只能用于（1）静态计算或（2）用于静态和动态执行的混合。在大多数机器学习框架中，纯Eager Execution(动态图)模式仍 然将前向计算、向后向传递和参数更新划分为三个阶段。</p><h2 id="问题的解决方案：前向融合和后向融合"><a href="#问题的解决方案：前向融合和后向融合" class="headerlink" title="问题的解决方案：前向融合和后向融合"></a>问题的解决方案：前向融合和后向融合</h2><p>下面四张拓扑图分别表示了有数据依赖的训练过程，基础的动态图训练过程，前向融合和后向融合。<script type="math/tex">f_i</script>代表第i层网络，<script type="math/tex">\theta_i</script>代表第i层的参数。</p><p><img src="https://s2.loli.net/2022/01/10/jAgcBmQRkWpoeTY.png" alt="3bac32e57070aba071bc98e217a92bdc_2_Figure_1.png"></p><h3 id="基础训练方案的问题"><a href="#基础训练方案的问题" class="headerlink" title="基础训练方案的问题"></a>基础训练方案的问题</h3><p>上图（b）表示了基础的训练过程，分为这么几步：</p><ul><li>读取模型参数</li><li>正向传播</li><li>计算Loss</li><li>反向传播，累计梯度</li><li>优化器读取梯度和模型参数并进行更新。</li><li>优化器重置梯度</li><li>优化器更新历史参数比如Momentum，Mean。</li><li>重复上述过程</li></ul><h4 id="数据局部性（data-locality）"><a href="#数据局部性（data-locality）" class="headerlink" title="数据局部性（data locality）"></a>数据局部性（data locality）</h4><p>上述过程中所有内存读取的过程在前向和反向传播，优化器更新参数过程这三个过程中是彼此独立的，没有很好地利用数据的局部性，下图表示了这三个过程中读写操作的重叠部分，我们完全可以利用好这些重叠来减少数据读取到内存的时间。</p><p><img src="https://s2.loli.net/2022/01/10/5hHlxBtMY92KvUp.png" alt="3bac32e57070aba071bc98e217a92bdc_2_Figure_2.png"></p><h4 id="计算并行性（computation-parallelism）"><a href="#计算并行性（computation-parallelism）" class="headerlink" title="计算并行性（computation parallelism）"></a>计算并行性（computation parallelism）</h4><p>基础训练方案也没有利用好反向传播和参数更新 之间的并行性。在更新一组参数的同时，我们可以继续反向传播，同时计算其他参数的 梯度。</p><h3 id="前向融合"><a href="#前向融合" class="headerlink" title="前向融合"></a>前向融合</h3><p>前向融合的过程如下图（c）所示，其利用上图紫色框中对内存的重复利用，将一次数据优化器迭代和前向传播相结合</p><h3 id="反向融合"><a href="#反向融合" class="headerlink" title="反向融合"></a>反向融合</h3><p>反向融合将合并来一层网络反向传播和更新参数对内存对读取，如上图（d）所示，利用到来红色框中对数据局部性，同时这一操作实现来计算对并行性</p><p><img src="https://s2.loli.net/2022/01/10/jAgcBmQRkWpoeTY.png" alt="3bac32e57070aba071bc98e217a92bdc_2_Figure_1.png"></p><h3 id="三种方法比较"><a href="#三种方法比较" class="headerlink" title="三种方法比较"></a>三种方法比较</h3><div class="table-container"><table><thead><tr><th>方法</th><th>数据局部性</th><th>计算并行性</th><th>全局信息</th></tr></thead><tbody><tr><td>baseline</td><td>× ️</td><td>×</td><td>✔</td></tr><tr><td>前向融合</td><td>✔</td><td>×</td><td>✔</td></tr><tr><td>反向融合</td><td>✔</td><td>✔</td><td>×</td></tr></tbody></table></div><h2 id="实验结果和结论"><a href="#实验结果和结论" class="headerlink" title="实验结果和结论"></a>实验结果和结论</h2><p>实验将MobileNetV2的一次训练迭代的时间分解，batch size大小为 32。<br>结果如下图所示，可以看出反向融合和前向融合比baseline的时间有显著减少。</p><p><img src="https://s2.loli.net/2022/01/10/rBfW4QHmMvU96dF.png" alt="3bac32e57070aba071bc98e217a92bdc_3_Figure_3.png"></p>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模型训练 </tag>
            
            <tag> 反向融合 </tag>
            
            <tag> 前向融合 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch Distributed Data Parallal</title>
      <link href="2022/01/23/ddp/"/>
      <url>2022/01/23/ddp/</url>
      
        <content type="html"><![CDATA[<p>目前有不少博客都有对pytorch Distirbuted data parallel的介绍和使用，但大多是针对实际代码的使用，本篇文章更侧重Pytorch DDP论文中提到的低层机制的实现以及目前存在的一些问题。</p><h1 id="数据并行基本概念（Data-Parallel）"><a href="#数据并行基本概念（Data-Parallel）" class="headerlink" title="数据并行基本概念（Data Parallel）"></a>数据并行基本概念（Data Parallel）</h1><p>如果工作节点没有共享的公共内存，只有容量受限的本地内存，而训练数据的规模很大，无法存储于本地内存，我们就需要对数据集进行划分，分配到各个工作节点上，然后工作节点依据各自分配的局部数据对模型进行训练 我们称此种并行模式为“数据并行模式。</p><h2 id="数据并行的两种基本方式"><a href="#数据并行的两种基本方式" class="headerlink" title="数据并行的两种基本方式"></a>数据并行的两种基本方式</h2><h3 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h3><p>把原始训练数据集作为采样的数据集，通过有放回的方式进行随机采样，然后根据每个工作节点的内存容量为其分配相应数目的训练样本。随机采样方法可以保证每台机器上的局部训练数据与原始训练数据是独立同分布的。</p><h3 id="置乱切分"><a href="#置乱切分" class="headerlink" title="置乱切分"></a>置乱切分</h3><p>将训练数据进行随机置乱，然后按照工作节点的个数将打乱后的数据顺序划分成相应的小份，随后将这些小份数据分配到各个工作节点上。每个工作节点在进行模型训练的过程中，只利用分配给向己的局部数据，并且会定期地（如每完成一个训练周期之后）将局部数据再打乱次 。 到一定阶段（如l完成多个训练周期之后），还可能再重新进行全局的数据打乱和重新分配 。</p><h2 id="数据并行要求"><a href="#数据并行要求" class="headerlink" title="数据并行要求"></a>数据并行要求</h2><ul><li>数学上等价：必须在数学上可以证明本地训练和数据并行训练对模型结果的影响是一致的</li><li>非侵入和截断的 API：一般都是从本地开发然后扩展到分布式。API 需要允许内部实现可以即时拦截信号来做通信和系统优化</li><li>高性能：数据并行训练受到计算和通信之间依赖关系的限制，所以设计和实现必须平衡两者之间的关系。</li></ul><h1 id="Pytorch-分布式数据并行（Distributed-Data-Parallel）"><a href="#Pytorch-分布式数据并行（Distributed-Data-Parallel）" class="headerlink" title="Pytorch 分布式数据并行（Distributed Data Parallel）"></a>Pytorch 分布式数据并行（Distributed Data Parallel）</h1><h2 id="DDP的原理"><a href="#DDP的原理" class="headerlink" title="DDP的原理"></a>DDP的原理</h2><p>DP模式是很早就出现的、单机多卡的、参数服务器架构的多卡训练模式，在PyTorch，即是：</p><pre><code>model = torch.nn.DataParallel(model)</code></pre><p>在DP模式中，总共只有一个进程。master节点相当于参数服务器，其会向其他卡广播其参数；在梯度反向传播后，各卡将梯度集中到master节点，master节点对搜集来的参数进行平均后更新参数，再将参数统一发送到其他卡上。这种参数更新方式，会导致master节点的计算任务、通讯量很重，从而导致网络阻塞，降低训练速度。</p><p>在分类上，DDP 属于Data Parallel。简单来讲，DDP就是在每个计算节点上复制模型，并独立地生成梯度，然后在每次迭代中互相传递这些梯度并同步，以保持各节点模型的一致性</p><p>在Pytorch中DP和DDP的区别总结如下</p><div class="table-container"><table><thead><tr><th>方法</th><th>进程和线程</th><th>设备</th><th>模型副本数量</th></tr></thead><tbody><tr><td>Data Parallel</td><td>单进程多线程</td><td>使用同一个机器上的多块卡</td><td>一个模型</td></tr><tr><td>Distributed Data Parallel</td><td>多进程</td><td>使用多个设备和其上的多个GPU</td><td>每台设备上一个模型副本</td></tr></tbody></table></div><p><strong>对于分布式数据并行，其核心问题在于如何保持每个设备上模型的一致性以及计算和通信效率的平衡</strong></p><h2 id="保持数据一致性两种方式"><a href="#保持数据一致性两种方式" class="headerlink" title="保持数据一致性两种方式"></a>保持数据一致性两种方式</h2><h3 id="参数平均"><a href="#参数平均" class="headerlink" title="参数平均"></a>参数平均</h3><p>参数平均则直接计算所有模型参数的平均值。参数平均的操作在优化器执行梯度下降后，这意味着其可以作为一个辅助步骤，非常的灵活，但是参数存在一定问题：</p><ul><li>数学上不等价</li><li>各批次数据的梯度下降方向不同，不利于收敛</li><li>计算和通信低效，两个阶段无法重叠</li></ul><p>所以Pytorch的DDP设计中放弃了参数平均的方式。</p><h3 id="梯度平均"><a href="#梯度平均" class="headerlink" title="梯度平均"></a>梯度平均</h3><p>梯度平均是将各个设备的梯度求平均，然后将这个平均梯度在各个节点的模型上作更新，这样的方式一方面在数学上和本地训练完全等价，而且可以实现异步，比参数平均更加高效。</p><h2 id="ALL-Reduce"><a href="#ALL-Reduce" class="headerlink" title="ALL Reduce"></a>ALL Reduce</h2><p>AllReduce是实现梯度平均最简单的方法。AllReduce是分布式数据并行用于计算所有进程的梯度总和的原始通信API。它由多个通信库支持，包括NCCL[2]、Gloo[1]和MPI[4]。AllReduce操作期望每个进程提供一个相同大小的张量，共同将给定的算术操作（例如，求和、平均、最小、最大）。最简单的实现方法是让每个进程将其输入张量广播给其他进程，然后每个进程独立地进行算术运算后进行梯度下降来更新模型参数。</p><h3 id="ALLReduce的改进算法"><a href="#ALLReduce的改进算法" class="headerlink" title="ALLReduce的改进算法"></a>ALLReduce的改进算法</h3><ul><li>RingALLReduce</li><li>TreeAllReduce</li></ul><p>pytorch中实现DDP实际使用RingALLReduce实现AllReduce算法，Ring AllReduce的通信成本是恒定的，与 GPU 数量无关，其流程如下，具体细节本文不做过多介绍，可以参考</p><p><a href="https://www.zhihu.com/question/57799212/answer/612786337">ring allreduce和tree allreduce的具体区别是什么？</a>。</p><p><img src="https://s2.loli.net/2022/01/11/cNQewRgA57HusBn.png" alt="v2-d725a864c17515b0677e37877693d84b_1440w.png"></p><h3 id="ALLReduce-存在问题"><a href="#ALLReduce-存在问题" class="headerlink" title="ALLReduce 存在问题"></a>ALLReduce 存在问题</h3><ul><li>集合通信在小张量上表现较差，这在具有大量小参数的大模型上尤为突出。</li><li>梯度计算和同步分离开，就没有机会把计算和通信过程重叠到一起</li></ul><h2 id="梯度分桶-Gradient-Bucketing"><a href="#梯度分桶-Gradient-Bucketing" class="headerlink" title="梯度分桶(Gradient Bucketing)"></a>梯度分桶(Gradient Bucketing)</h2><p>Pytorch DDP设计最核心的机制就是<strong>Gradient Bucketing</strong>，其解决了直接ALLreduce存在的问题</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20211230104223.png" alt=""></p><p>上图是ALLreduce执行时间和Tensor大小之间关系，从图中可以看出传递Tensor的参数越多，效率越高，这是梯度分桶的思想来源，既然每次传递小Tensor效率低，为什么不将多个参数的梯度聚合后传递？所以梯度分桶的做法将模型的Model的参数逆序插入每个Bucket中，当一个Bucket的参数的梯度都已经更新时，开启Allreduce，向另一个节点的对应Bucket传递梯度，这样也同时实现了异步AllReduce。</p><p><img src="https://s2.loli.net/2022/01/11/B2eOUNzFTHvdWMY.png" alt="图片1.png"></p><h3 id="梯度分桶的两个要点"><a href="#梯度分桶的两个要点" class="headerlink" title="梯度分桶的两个要点"></a>梯度分桶的两个要点</h3><ul><li>DDP的Bucket中的参数是按model.parameters()倒序插入的，在实际执行中$bucket_{i+1}$ 必须在$bucket_i$ 完成后才能进行，在文章中提到这是工程上的近似，因为模型梯度参数梯度更新的顺序不一定完全和模型参数的顺序一致，但在大多数情况这样方法是适用且高效的。所以，在创建model的时候，请务必把先进行计算的parameter注册在前面，后计算的在后面。不然，reducer会卡在某一个bucket等待，使训练时间延长，出现图（a)中的情况，进程2的$g_2$参数的梯度在$g_3,g_4$后更新导致进程2的第二个Bucket迟迟无法进行ALLReduce操作。</li><li>因为pytorch是完全的动态图机制，每次训练中不是所有子图都会参与训练，在模型中部分子图被冻结，不参与梯度更新时，在创建ddp Model时需要把<code>find_unused_parameter</code>设置为true，否则一个Bucket中有一个参数的梯度没有ready就不会进行Allreduce,如图（b）。<code>find_unused_parameter</code>为true时在每次前向传播后会搜索所有没有参与梯度更新的参数并把它们的状态设置为ready。</li></ul><p><img src="https://s2.loli.net/2022/01/11/QdlHorfgLtU3sZY.png" alt=""></p><h2 id="hook机制"><a href="#hook机制" class="headerlink" title="hook机制"></a>hook机制</h2><p>前文提到pytorch DDP设计者在设计API时希望DDP的功能不会对原有训练流程有影响，即非侵入式的API，这利用了实现Pytorch中的hook实现。<br>用形象的话讲，hook提供了这么一种机制：程序提供hook接口，用户可以写一个hook函数，然后钩在hook接口，即程序的主体上从而可以插入到中间执行。DDP使用hook技术把自己的逻辑插入到module的训练过程中去。<br>parameter在反向梯度计算结束后提供了一个hook接口。DDP把Ring-Reduce的代码写成一个hook函数，插入到这里。每次parameter的反向梯度计算结束后，程序就会调用这个hook函数，从而开启Ring-Reduce流程。</p><h2 id="DDP代码和执行流程"><a href="#DDP代码和执行流程" class="headerlink" title="DDP代码和执行流程"></a>DDP代码和执行流程</h2><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>下面是实现DDP的一段最简单的代码Demo,对原有训练流程没有任何改动，只需额外的两行代码<code>dist.init_process_group(&quot;gloo&quot;, rank=rank, world_size=world_size)</code>和<br><code>ddp_model = DDP(model, device_ids=[rank])</code></p><pre><code>import torchimport torch.distributed as distimport torch.multiprocessing as mpimport torch.nn as nnimport torch.optim as optimfrom torch.nn.parallel import DistributedDataParallel as DDPdef example(rank, world_size):    # 创建进程组    dist.init_process_group(&quot;gloo&quot;, rank=rank, world_size=world_size)    # 创建模型    model = nn.Linear(10, 10).to(rank)    # 创建DDP模型    ddp_model = DDP(model, device_ids=[rank])    # 定义损失函数和优化器    loss_fn = nn.MSELoss()    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)    # 前向传播    outputs = ddp_model(torch.randn(20, 10).to(rank))    labels = torch.randn(20, 10).to(rank)    # 反向传播    loss_fn(outputs, labels).backward()    # 更新参数    optimizer.step()</code></pre><h3 id="DDP执行流程"><a href="#DDP执行流程" class="headerlink" title="DDP执行流程"></a>DDP执行流程</h3><ul><li>准备阶段<ul><li>环境初始化<ul><li>在各张卡上初始化进程并建立进程之间的通信</li><li>对应代码：init_process_group</li></ul></li><li>模型广播<ul><li>将模型parameter，buffer广播到各节点</li><li>对应代码：model = DDP(model)</li></ul></li><li>初始化Bucket<ul><li>将模型的参数倒序分配到各个桶中（为什么必须倒序后面会讲）</li><li>对应代码：model = DDP(model)</li></ul></li><li>创建管理器reducer，给每个parameter注册梯度平均的hook。</li></ul></li><li>训练阶段<ul><li>采样数据（for data, label in dataloader）<br>  +从dataloader得到一个batch的数据，用于当前计算。</li><li>前向传播（output = model(data)）<ul><li>同步各进程状态（parameter和buffer）</li><li>前向计传播</li><li>当DDP参数find_unused_parameter为true时，其会在forward结束时，启动一个回溯，标记出所有没被用到的parameter，提前把这些设定为ready。</li></ul></li><li>计算梯度（loss.backward()）<ul><li>reducer外面：各个进程各自开始反向地计算梯度。</li><li>reducer外面：当某个parameter的梯度计算好了的时候，其之前注册的grad hook就会被触发，在reducer里把这个parameter的状态标记为ready。</li><li>reducer里面：当某个bucket的所有parameter都是ready状态时，reducer会开始对这个bucket的所有parameter都开始一个异步的all-reduce梯度平均操作。</li><li>reducer里面：当所有bucket的梯度平均都结束后，reducer才会把得到的平均grad结果正式写入到parameter.grad里面。</li></ul></li><li>优化器optimizer应用gradient，更新参数（optimizer.step()）。</li></ul></li></ul><h2 id="梯度累计"><a href="#梯度累计" class="headerlink" title="梯度累计"></a>梯度累计</h2><p>对于DDP的ALLreduce,文章中提到进一步的改进方法，使用梯度累计来进一步减少通信次数，DDP的梯度同步在<code>loss.backward()</code>阶段进行，Pytorch提供了<code>model.no_sync()</code>接口，可以使反向传播取消梯度同步，这样我们可以选择K次迭代进行一次梯度同步，当然K不建议取的过大，代码如下。</p><pre><code>model = DDP(model)for 每次梯度累加循环    optimizer.zero_grad()    # 前accumulation_step-1个step，不进行梯度同步，累积梯度。    for _ in range(K-1)::        with model.no_sync():            prediction = model(data)            loss = loss_fn(prediction, label) / K            loss.backward()  # 积累梯度，不应用梯度改变    # 第K个step，进行梯度同步    prediction = model(data)    loss = loss_fn(prediction, label) / K    loss.backward()  # 积累梯度，不应用梯度改变    optimizer.step()</code></pre><h2 id="DDP未来可能的改进方式"><a href="#DDP未来可能的改进方式" class="headerlink" title="DDP未来可能的改进方式"></a>DDP未来可能的改进方式</h2><p>对于当前基于Bucket的DDP实现，文章提到了下面三种可能改进的方式</p><h3 id="梯度顺序预测"><a href="#梯度顺序预测" class="headerlink" title="梯度顺序预测"></a>梯度顺序预测</h3><p>目前DDP的模型parameter和Bucket的参数的map映射关系是直接将模型的参数按model.parameter() reverse后将Bucket一一填满，每个Bucket的容量默认为25MB个参数，，所以实际执行时可能出现阻塞现象，所以可以考虑更加智能的parameter到Bucket的映射算法</p><h3 id="layer-dropping"><a href="#layer-dropping" class="headerlink" title="layer dropping"></a>layer dropping</h3><p>在网络采用dropout时每次前向传播都会构建一个新的计算图，DDP同样也支持Dropout，但是由于parameter到bucket的映射是固定的，虽然没次前向传播后会把这些dropout的参数设定为ready，但是这也带来了额外的内存开销。解决这样的方法一种是修改parameter到bucket的映射，另一种是将一个layer映射到bucket，每次更新直接跳过一个bucket。</p><h3 id="梯度压缩"><a href="#梯度压缩" class="headerlink" title="梯度压缩"></a>梯度压缩</h3><p>DDP的另一种改进是压缩梯度，将FP64，FP32，压缩为更低精度的IN8，所以可以考虑使用自适应的梯度压缩策略。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul><li><a href="https://arxiv.org/abs/2006.15704">论文原文：PyTorch Distributed: Experiences on Accelerating<br>Data Parallel Training</a></li><li><a href="https://pytorch.org/docs/stable/notes/ddp.html">官方设计文档</a></li><li><a href="https://zhuanlan.zhihu.com/p/343951042">OpenMMlab:PyTorch 源码解读之 DP &amp; DDP：模型并行和分布式训练解析</a></li><li><a href="https://zhuanlan.zhihu.com/p/178402798">PyTorch DDP系列解读</a></li><li><a href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">Pytoech文档：torch.nn.parallel.DistributedDataParallel</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模型训练 </tag>
            
            <tag> 分布式机器学习 </tag>
            
            <tag> 分布式数据并行 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文解读：Lite Transformer</title>
      <link href="2021/12/13/Lite-Transformer/"/>
      <url>2021/12/13/Lite-Transformer/</url>
      
        <content type="html"><![CDATA[<h2 id="LITE-TRANSFORMER-WITH-LONG-SHORT-RANGE-ATTENTION"><a href="#LITE-TRANSFORMER-WITH-LONG-SHORT-RANGE-ATTENTION" class="headerlink" title="LITE TRANSFORMER WITH LONG-SHORT RANGE ATTENTION"></a>LITE TRANSFORMER WITH LONG-SHORT RANGE ATTENTION</h2><p><a href="https://arxiv.org/abs/2004.11886v1">论文地址</a></p><p><a href="https://github.com/mit-han-lab/lite-transformer">代码地址</a></p><p>本篇文章来袭MIT 韩松的团队，我们先看摘要</p><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>Transformer网络结构在自然语言处理中已经被广泛应用（如机器翻译，回答等）；然而，它需要大量的计算资源来实现高性能，硬件资源和电池容量的限制使得它很难在端侧设备部署。在本文中，我们提出了一个高效的移动NLP架构—Lite Transformer，以便于在边缘设备上部署基于Transformer的NLP模型。其关键点是 Long Short range attention（长短程注意力，LSRA），它由两部分组成，一部分走传统的self-attention，这部分可以得到长距离的关系；另一部分使用一个精简版本的卷积神经网络，这部分来获得短距离的关系长距离关系建模（通过注意）。在三个通用NLP任务：机器翻译、抽象概括和语言建模上，Lite Transformer相比普通的transformer都有明细的性能提升。在资源受限的情况下（500M/100M MACs），Lite Transformer在数据集WMT’14的英语-法语翻译任务上的表现超过了Transformer，其BLEU分别为1.7和1.2 BLEU。Lite Transformer将transformer模型的计算量减少了2.5倍，而BLEU得分只下降了0.3。通过剪枝和量化，我们进一步将Lite Transformer的模型大小压缩了18.2倍。同时，Lite Transformer和基于AutoML的Evolved Transformer相比，BLEU得分高了0.5，却不需要花费大量的GPU资源进行参数搜索。</p><h3 id="论文试图解决什么问题？"><a href="#论文试图解决什么问题？" class="headerlink" title="论文试图解决什么问题？"></a>论文试图解决什么问题？</h3><p>本篇文章的Lite Transformer模型对比了传统的Transformer模型和基于AutoML 的 Evolved Transformer，试图解决原有模型计算量大的的缺点。</p><ul><li>Transformer的优点</li></ul><p>Transformer模型因其高训练效率和捕获长距离依赖关系的优越能力，在自然语言处理中得到广泛应用。在此基础上，现在NLP各任务上的SOTA模型，如BERT，能够从未标记的文本中学习语言表征，在具有挑战性的问题回答任务上甚至超过人类的表现。</p><ul><li>Transformer的缺点</li></ul><p>Transformer良好的性能表现需要很高的计算代价。例如，一个单个的Transformer模型需要超过 10G 的 Mult-Adds 才 能翻译一个只有30个单词的句子。同样，利用AutoML搜索参数的Evolved Transformer也需要大量的搜索成本，消耗大量的GPU资源。</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20211213154214.png" alt=""><br>（模型参数和训练开销对比）</p><p>所以对于计算能力受限的边缘设备的能力，如智能手机 和 lot 设计高效快速的Transformer结构具有重要意义，本文提出的模型Lite Transformer也正是为了解决这个问题。</p><h3 id="这是否是一个新的问题？"><a href="#这是否是一个新的问题？" class="headerlink" title="这是否是一个新的问题？"></a>这是否是一个新的问题？</h3><p>是，Transformer模型出现后，各种大规模预训练模型如GPT，Bert不断出现，预训练成为NLP任务的主流方式，对于算力受限的边缘设备，设计一个轻量化的Transformer模型尤为关键。</p><h3 id="这篇文章要验证一个什么科学假设？"><a href="#这篇文章要验证一个什么科学假设？" class="headerlink" title="这篇文章要验证一个什么科学假设？"></a>这篇文章要验证一个什么科学假设？</h3><p>注意力机制引入了大量的计算量。假设元素的数量（例如，语言处理中的标记符的长度、图像中的像素数等）。输入注意层为N，特征（即通道）的维数为d，点积所需的计算量为$N^2d$.对于图像和视频，N通常非常大。例如，视频网络中的中间特征图有 16帧，每帧的分辨率为 $112 \times 112$，导致 $N = 2\times 10^5$。卷积层和全连通层的计算呈$w\cdot r\cdot t\cdot N$级别的线性增长,而注意层的计算呈$w\cdot r\cdot t\cdot N$的二次增长。</p><p>为了解决这一困境，通常的做法是首先在应用注意力层之前使用线性投影层减少通道数d，然后增加维数 （如图所示）。在Transformer的原始设计中，注意力模块的通道尺寸比FFN 层的小 4倍。但是它也降低了具有较小特征维度的注意层的上下文捕获能力。语言处理的情况可能更糟，因为注意力是上下文捕获的主要模块</p><p>作者通过研究Transformer的计算分布，发现计算量(Mult-Adds)由前馈网络(FFN)主导。普遍存在的BottleNeck层使Transformer块效率低下。如果能把FFN层用其他计算效率更高的层替换，那么计算效率一定可以提升，更少参数的网络也可以达到同样的效果。</p><p>简单来说，就是作者认为原有Transformer为了减少计算量的FFN层本身也消耗了大量计算时间，所以直接将其去掉，用一个不做维度变化的层代替。<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20211213154531.png" alt=""></p><h3 id="论文中提到的解决方案之关键是什么？"><a href="#论文中提到的解决方案之关键是什么？" class="headerlink" title="论文中提到的解决方案之关键是什么？"></a>论文中提到的解决方案之关键是什么？</h3><p>针对上述假设，作者提出了一种新的Long-Short Range Attention（长-短距离注意力，LSRA)的基本模块。LSRA 把 FFN 中的计算换成更广泛的注意力层。它延长了瓶颈，为注意层引入了更多的捕获依赖的能力，然后作者在保证性能不降低的情况下，缩小了Embedding层的大小，以减少了总的计算量。Lite Transformer基本的模块如下</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20211213161953.png" alt=""></p><p>可以看到整个Lite Transformer分为两个模块，一个Attention层和一个卷积层，这个设计也就是本篇文章的核心思想所在，作者认为对于翻译任务，模型需要同时捕捉全局信息和本地局部信息，原Transformer模型用了8个注意力头，也就是Multi-Head Attention去捕捉全局和局部信息，作者认为这样的设计是冗余的，所以作者设计了两个模块分别捕捉全局和局部信息。</p><h4 id="卷积注意力"><a href="#卷积注意力" class="headerlink" title="卷积注意力"></a>卷积注意力</h4><p>不少文章中认为卷积神经网络用于NLP任务效果不如Transformer的原因是CNN不擅长捕捉长远的信息，因为CNN本身是通过滑窗实现的，这样的计算方式本身就更适合提取本地信息。所以作者直接将其用于提取本地信息，将注意力值可视化后结果如下，可以看出每个词对邻近词的关注度更大。</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20211213163627.png" alt=""></p><h4 id="Attention层"><a href="#Attention层" class="headerlink" title="Attention层"></a>Attention层</h4><p>Lite Transformer的Attention层只有一个头，可视化后结果如下，可以看出Attention层的注意力分布不集中在邻近，更关注长远的注意力。</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20211213163613.png" alt=""></p><p>综上所述，作者用一个卷积层和注意力层分别捕捉全局和局部信息，这样专业化的设置可以用更浅的网络得到同样的信息。</p><h3 id="论文中的实验是如何设计的？"><a href="#论文中的实验是如何设计的？" class="headerlink" title="论文中的实验是如何设计的？"></a>论文中的实验是如何设计的？</h3><p>作者在机器翻译、文本摘要和语言建模三个任务上进行了实验和评估。具体而言，机器翻译任务使用了三个基准数据集：IWSLT’14 德语 - 英语 (De-En)、WMT 英语 - 德语 (En-De)、WMT 英语 - 法语(En-Fr)。文本摘要任务使用的是 CNN-DailyMail 数据集。语言建模任务则在 WIKITEXT-103 数据集上进行。</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20211213194825.png" alt=""><br>可以看出在参数量接近的情况下，LiteTransformer和效果个训练开销都更加优秀。</p>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep learning </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于tfidf和xgboost文本分类</title>
      <link href="2021/12/05/xgboost-tfidf-nlp/"/>
      <url>2021/12/05/xgboost-tfidf-nlp/</url>
      
        <content type="html"><![CDATA[<h1 id="boost-amp-bagging-amp-stacking-区别"><a href="#boost-amp-bagging-amp-stacking-区别" class="headerlink" title="boost &amp; bagging &amp; stacking 区别"></a>boost &amp; bagging &amp; stacking 区别</h1><div class="table-container"><table><thead><tr><th></th><th>算法</th><th>模型</th><th>数据</th><th>作用</th><th>实现</th></tr></thead><tbody><tr><td>Bagging</td><td>多个相同不稳定模型</td><td>对数据随机采样，每个模型不同</td><td>减少 variance</td><td>用多个模型平均或投票，并行</td></tr><tr><td>Boosting</td><td>多个相同弱模型</td><td>每次从样本分布不同的数据集采样，上个模型预测错误的数据有更大可能被选择</td><td>减少 bias</td><td>用多个相同弱模型不断拟合残差，减少方差。串行</td></tr><tr><td>Stacking</td><td>多个不同模型</td><td>全部数据</td><td>增强预测效果</td><td>用多个不同模型输出concat后生成一个新的数据后再通过一个分类器输出</td></tr></tbody></table></div><h2 id="baggingBagging"><a href="#baggingBagging" class="headerlink" title="baggingBagging"></a>baggingBagging</h2><p>是 bootstrap aggregation的缩写。bagging对于数据集进行取样，每个数据点有同等几率被采样，然后创建n个模型，每个模型进行m个数据采样，最后进行投票（voting）得出最后结果。</p><h1 id="xgboost实现文本分类"><a href="#xgboost实现文本分类" class="headerlink" title="xgboost实现文本分类"></a>xgboost实现文本分类</h1><ul><li>导入相关文件</li></ul><pre><code>  import xgboost as xgb  import pandas as pd  from sklearn.feature_extraction.text import TfidfVectorizer  from sklearn.metrics import f1_score</code></pre><ul><li>数据读取并做TF-IDF处理<br>xgboost的输入数据要求为DMatrix类，所以要通过<code>xgb.DMatrix</code>方法进行转化</li></ul><pre><code>  model_path = &quot;./xgboost/&quot;  train_df = pd.read_csv(&#39;./train_set.csv/train_set.csv&#39;, sep=&#39;\t&#39;, nrows=50000)  tfidf = TfidfVectorizer(ngram_range=(1, 3), max_features=5000, smooth_idf=True)  label = train_df[&#39;label&#39;]train_test = tfidf.fit_transform(train_df[&#39;text&#39;])  train_data = train_test[:45000]  train_label = label[:45000]  dtrain = xgb.DMatrix(train_data, label=train_label, nthread=-1)  test_data = train_test[45000:50000]  test_label = label[45000:50000]  dtest = xgb.DMatrix(test_data, label=test_label, nthread=-1)</code></pre><ul><li>设置训练参数</li></ul><pre><code>  params = &#123;        &#39;booster&#39;: &#39;gbtree&#39;,           # gbliner 或 gbtree树模型或者线性模型        &#39;objective&#39;: &#39;multi:softmax&#39;,  # 多分类的问题        &#39;num_class&#39;: 14,               # 类别数，与 multisoftmax 并用        &#39;gamma&#39;: 0.1,                  # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。        &#39;max_depth&#39;: 12,               # 构建树的深度，越大越容易过拟合        &#39;lambda&#39;: 2,                   # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。        &#39;subsample&#39;: 0.7,              # 随机采样训练样本        &#39;colsample_bytree&#39;: 0.7,       # 生成树时进行的列采样        &#39;min_child_weight&#39;: 3,         # 最小叶子节点样本权重和        &#39;silent&#39;: 1,                   # 设置成1则没有运行信息输出，最好是设置为0.        &#39;eta&#39;: 0.1,                  # 学习率        &#39;seed&#39;: 1000,        &#39;nthread&#39;: 4,                  # cpu 线程数&#125;</code></pre><ul><li>训练模型并保存</li></ul><pre><code>print(&quot;start train xgboost classifier&quot;)xgbc = xgb.train(params, dtrain)  # 初始化xgboost分类器，原生接口默认启用全部线程xgbc.save_model(model_path+&#39;xgboost.model&#39;)  # 保存模型</code></pre><ul><li>预测结果并打印和_score</li></ul><pre><code>print(&quot;xgboost train over&quot;)pre_train = xgbc.predict(xgb.DMatrix(train_data, nthread=-1))   # 训练数据的预测概率矩阵，启用全部线程pre_test = xgbc.predict(xgb.DMatrix(test_data, nthread=-1))     # 测试数据的预测概率矩阵，启用全部线程print(&quot;train data f1score:&quot;)print(f1_score(train_label, pre_train, average=&#39;macro&#39;))print(&quot;test data f1score:&quot;)print(f1_score(test_label, pre_test, average=&#39;macro&#39;))</code></pre><ul><li><p>输出结果</p><p>  train data f1score:<br>  0.9321652074415424<br>  test data f1score:<br>  0.8822079711934986</p></li></ul><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">xgboost 调参指南</a></p>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> ML </tag>
            
            <tag> xgboost </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于TF-IDF和线性回归的简单文本分类</title>
      <link href="2021/11/29/TFIDF/"/>
      <url>2021/11/29/TFIDF/</url>
      
        <content type="html"><![CDATA[<p>在机器学习算法的训练过程中，假设给定<br>N个样本，每个样本有 M个特征，这样组成了 <code>N×M</code>的样本矩阵，然后完成算法的训练和预测。同样的在计算机视觉中可以将图片的像素看作特征，每张图片看作<code>hight×width×3</code>的特征图，一个三维的矩阵来进入计算机进行计算。</p><p>但是在自然语言领域，上述方法却不可行：文本是不定长度的。文本表示成计算机能够运算的数字或向量的方法一般称为词嵌入（Word Embedding）方法。词嵌入将不定长的文本转换到定长的空间内，是文本分类的第一步。</p><h2 id="Word-Emdedding方法"><a href="#Word-Emdedding方法" class="headerlink" title="Word Emdedding方法"></a>Word Emdedding方法</h2><h3 id="One-hot"><a href="#One-hot" class="headerlink" title="One-hot"></a>One-hot</h3><p>One-hot是最简单的词嵌入方法，这里的One-hot与数据挖掘任务中的操作是一致的，即将每一个单词使用一个离散的向量表示。具体将每个字/词编码一个索引，然后根据索引进行赋值。</p><p>One-hot表示方法的例子如下：</p><p>句子1：我 爱 北 京 天 安 门<br>句子2：我 喜 欢 上 海<br>首先对所有句子的字进行索引，即将每个字确定一个编号：</p><p>{<br>    ‘我’: 1, ‘爱’: 2, ‘北’: 3, ‘京’: 4, ‘天’: 5,<br>  ‘安’: 6, ‘门’: 7, ‘喜’: 8, ‘欢’: 9, ‘上’: 10, ‘海’: 11<br>}<br>在这里共包括11个字，因此每个字可以转换为一个11维度稀疏向量：</p><p>我：[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]<br>爱：[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]<br>…<br>海：[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</p><h3 id="Bag-of-Words-词袋模型"><a href="#Bag-of-Words-词袋模型" class="headerlink" title="Bag of Words(词袋模型)"></a>Bag of Words(词袋模型)</h3><p>Bag of Words（词袋表示），也称为Count Vectors，每个文档的字/词可以使用其出现次数来进行表示。比如</p><pre><code>句子1：我 爱 北 京 天 安 门句子2：我 喜 欢 上 海</code></pre><p>直接统计每个字出现的次数，并进行赋值：</p><pre><code>句子1：我 爱 北 京 天 安 门[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]句子2：我 喜 欢 上 海[2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]</code></pre><p>其中‘我’出现两次，所以被赋值为2</p><p>在sklearn中可以直接CountVectorizer来实现这一步骤：</p><pre><code>from sklearn.feature_extraction.text import CountVectorizercorpus = [    &#39;This is the first document.&#39;,    &#39;This document is the second document.&#39;,    &#39;And this is the third one.&#39;,    &#39;Is this the first document?&#39;,]vectorizer = CountVectorizer()vectorizer.fit_transform(corpus).toarray()</code></pre><p>词袋模型存在的问题和One-Hot相同，编码方式并没有体现相邻词的相关性，只显示单个词的统计频率。</p><h3 id="N-gram"><a href="#N-gram" class="headerlink" title="N-gram"></a>N-gram</h3><p>N-gram与词袋模型类似，不过加入了相邻单词组合成为新的单词，并进行计数，这样做的好处是增加了词之间的关联信息。</p><p>如果N取值为2，则句子1和句子2就变为：</p><pre><code>句子1：我爱 爱北 北京 京天 天安 安门句子2：我喜 喜欢 欢上 上海</code></pre><p>但词袋模型和N-gram都存在同一问题，对于一个长文本，类似于‘的’这样的介词会出现多次，而这些词所含文本的信息量极少，赋予一个较高的权值并不合理。</p><h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>TF-IDF解决了上述问题。<br>TF-IDF 分数由两部分组成：第一部分是词语频率（Term Frequency），第二部分是逆文档频率（Inverse Document Frequency）。其中计算语料库中文档总数除以含有该词语的文档数量，然后再取对数就是逆文档频率。</p><script type="math/tex; mode=display">TF(t)= \frac{该词语在当前文档出现的次数}{ 当前文档中词语的总数}IDF(t)= log_e（\frac{文档总数}{ 出现该词语的文档总数}）TF-IDF = IDF(t) * TF(t)</script><p>对于在所有文档中出现的词，取对数后为0，计算结果为0；</p><h2 id="使用Bag-of-Words-词袋模型-进行预测"><a href="#使用Bag-of-Words-词袋模型-进行预测" class="headerlink" title="使用Bag of Words(词袋模型)进行预测"></a>使用Bag of Words(词袋模型)进行预测</h2><p>我们使用Bag of Words(词袋模型)进行编码后，采用最简单的逻辑回归进行预测，代码如下：</p><pre><code>import pandas as pdfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.linear_model import RidgeClassifierfrom sklearn.metrics import f1_scoretrain_df = pd.read_csv(&#39;./train_set.csv/train_set.csv&#39;, sep=&#39;\t&#39;, nrows=15000)vectorizer = CountVectorizer(max_features=3000)train_test = vectorizer.fit_transform(train_df[&#39;text&#39;])clf = RidgeClassifier()clf.fit(train_test[:10000], train_df[&#39;label&#39;].values[:10000])val_pred = clf.predict(train_test[10000:])print(f1_score(train_df[&#39;label&#39;].values[10000:], val_pred, average=&#39;macro&#39;))</code></pre><p>输出结果用f1_score作为评分标准</p><pre><code>0.7416952793751392</code></pre><h2 id="使用TF-IDF进行预测"><a href="#使用TF-IDF进行预测" class="headerlink" title="使用TF-IDF进行预测"></a>使用TF-IDF进行预测</h2><pre><code>from sklearn.feature_extraction.text import TfidfVectorizertfidf = TfidfVectorizer(ngram_range=(1, 3), max_features=3000)train_test = tfidf.fit_transform(train_df[&#39;text&#39;])clf.fit(train_test[:10000], train_df[&#39;label&#39;].values[:10000])val_pred = clf.predict(train_test[10000:])print(f1_score(train_df[&#39;label&#39;].values[10000:], val_pred, average=&#39;macro&#39;))</code></pre><p>输出结果</p><pre><code>0.8721598830546126</code></pre><p>对比词袋模型提升明显</p><p>我们将最大特征提高到5000</p><pre><code>tfidf = TfidfVectorizer(ngram_range=(1, 3), max_features=5000)</code></pre><p>输出</p><pre><code>0.8850817067811825</code></pre><p>因为训练样本较少，加入平滑措施，此时idf公式变为</p><script type="math/tex; mode=display">idf(t)=log( \frac{(1+训练集文本总数) } {(1+包含词t的文本数) )}+1</script><pre><code>tfidf = TfidfVectorizer(ngram_range=(1, 3), max_features=5000, smooth_idf=True)</code></pre><p>输出结果</p><pre><code>0.8891785268087526</code></pre>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cmake入门和MindsporeLite Cmake文件分析</title>
      <link href="2021/11/29/Cmake/"/>
      <url>2021/11/29/Cmake/</url>
      
        <content type="html"><![CDATA[<p>对于没有接触Cmake的同学而言，刚接触Cmake难免一脸懵逼,我刚开始实习时也是第一次接触到Cmake，原来CPP开发也一直是在Visual Studio环境下进行，对Liunx环境下的CPP开发并不熟悉，对于Liunx环境的CPP开发而言，CMake和MakeFile是必须掌握的，本文将简单介绍Cmake的原理和使用方法，并以<a href="https://gitee.com/mindspore/mindspore">mindsporelite</a>作为案例讲解一个大型项目的Cmake写法。</p><hr><h1 id="MakeFile和Cmake简介"><a href="#MakeFile和Cmake简介" class="headerlink" title="MakeFile和Cmake简介"></a>MakeFile和Cmake简介</h1><p>对于一个C++或者C程序的编译分为预处理、编译、汇编、链接四个过程，比如对于单文件<code>&quot;hello.cpp&quot;,</code><br>生成hello（hello.exe）所需要执行的bash命令：</p><pre><code>gcc -v -o hello hello.c</code></pre><p>但当涉及多文件编译时，我们的命令会十分复杂，比如一个<code>main.cpp</code>文件依赖于<code>Foo1.cpp</code>,<code>Foo2.cpp</code>,…<code>Foo10</code>,我们需要先将这10个文件编译成<code>.o</code>文件，再将各<code>.o</code>文件和<code>main.o</code>进行链接，代码如下</p><pre><code>g++ -std=c++17 -O2 -o Foo1.o -c Foo1.cppg++ -std=c++17 -O2 -o Foo2.o -c Foo2.cppg++ -std=c++17 -O2 -o Foo3.o -c Foo3.cppg++ -std=c++17 -O2 -o Foo4.o -c Foo4.cppg++ -std=c++17 -O2 -o Foo5.o -c Foo5.cppg++ -std=c++17 -O2 -o Foo6.o -c Foo6.cppg++ -std=c++17 -O2 -o Foo7.o -c Foo7.cppg++ -std=c++17 -O2 -o Foo8.o -c Foo8.cppg++ -std=c++17 -O2 -o Foo9.o -c Foo9.cppg++ -std=c++17 -O2 -o Foo10.o -c Foo10.cppg++ -std=c++17 -O2 -o main.o -c main.cppg++ -std=c++17 -O2 main.o Foo1.o Foo2.o Foo3.o Foo4.o Foo5.o Foo6.o Foo7.o Foo8.o Foo9.o Foo10.o -o main</code></pre><p>上述编译过程还算简单，因为各子文件还不相互依赖，但如果子文件有相互依赖关系，比如<code>foo1.cpp</code>依赖于<code>foo10.cpp</code>我们编译的顺序有必须更改，在文件数目更大的工程中，一行一行输入命令进行编译是不现实的，所以我们引入了Makefile,对于上述编译任务，我们的MakeFile如下，我们只需输入<code>make</code>一句命令即可完成编译</p><pre><code>COMPILER = g++ -std=c++17 -O2 -I./include  OBJECTS = Foo1.o Foo2.o Foo3.o Foo4.o Foo5.o Foo6.o Foo7.o Foo8.o Foo9.o Foo10.o main.oTARGET = main$(TARGET): $(OBJECTS)    $(COMPILER) -o $@ $^ -lprotobuf$(OBJECTS): %.o: %.cpp    $(COMPILER) -o $@ -c $&lt;$(DEPENDENCIES): %.d: %.cpp    $(COMPILER) -o $@ -MM $&lt;include $(DEPENDENCIES).PHONY: cleanclean:    rm $(OBJECTS) $(DEPENDENCIES) $(TARGET)</code></pre><p>Makefile 规定了一套编译规则，使用什么编译器，编译器使用什么样的编译选项，每个文件都有什么依赖关系。在编译的时候，能够直接根据 makefile 来确定哪些文件依赖于其他的哪些文件，从而把编译顺序、需不需要重新编译以及链接都自动检测出来,在Linux环境下，MakeFile本身可以看做一个shell脚本。</p><p>既然我们有了MakeFile，又为什么需要Cmake工具？对于不同环境下的编译，有着多种Make工具，比如 GNU Make ，QT 的 qmake ，微软的 MS nmake，BSD Make（pmake），Makepp，等等。这些 Make 工具遵循着不同的规范和标准，所执行的 Makefile 格式也千差万别。这样就带来了一个严峻的问题：如果软件想跨平台，必须要保证能够在不同平台编译。而如果使用上面的 Make 工具，就得为每一种标准写一次 Makefile。</p><p>CMake 就是针对上面问题所设计的工具：它首先允许开发者编写一种平台无关的 CMakeList.txt 文件来定制整个编译流程，然后再根据目标用户的平台进一步生成所需的本地化 Makefile 和工程文件，如 Unix 的 Makefile 或 Windows 的 Visual Studio 工程。从而做到“Write once, run everywhere”。显然，CMake 是一个比上述几种 make 更高级的编译配置工具。对于深度学习框架而言，跨平台是一件非常重要的事，因为深度学习模型可能在不同的环境下运行，可能是x86的Linux或者Windows，也可能是ARM，涉及到跨平台交叉编译。</p><h1 id="MindSporeLite-Cmake详解"><a href="#MindSporeLite-Cmake详解" class="headerlink" title="MindSporeLite Cmake详解"></a>MindSporeLite Cmake详解</h1><h2 id="生成可执行文件或库"><a href="#生成可执行文件或库" class="headerlink" title="生成可执行文件或库"></a>生成可执行文件或库</h2><p>在 linux 平台下使用 CMake 生成 Makefile 并编译的流程如下：</p><ul><li>编写 CMake 配置文件 CMakeLists.txt 。</li><li>执行命令 cmake PATH 或者 ccmake PATH 生成 Makefile（ccmake 和 cmake 的区别在于前者提供了一个交互式的界面）。其中， PATH 是 CMakeLists.txt 所在的目录。</li><li>使用 make 命令进行编译。</li></ul><p>下面是一个最简单的单文件CmakeLists,执行命令后会编译一个名为Demo的 <code>exe</code>文件</p><pre><code># CMake 最低版本号要求cmake_minimum_required (VERSION 2.8)# 项目信息project (mindsporelite)# 指定生成目标add_executable(mindsporelite main.cc)</code></pre><p>如果把<code>add_executable</code>改成<a href="https://cmake.org/cmake/help/v3.20/command/add_library.html">add_library</a>,那么会生成一个的静态或者动态库</p><pre><code># 生成动态库add_library(mindsporelite SHARED main.cc)# 生成静态库add_library(mindsporelite STATIC main.cc)</code></pre><h2 id="多文件编译"><a href="#多文件编译" class="headerlink" title="多文件编译"></a>多文件编译</h2><p>对于多文件的情况，我们可以通过在<code>add_library</code>或<code>add_executable</code>的目标后意义列出所有文件，如下</p><pre><code># 生成动态库add_library(mindsporelite SHARED main.cc a.cc b.cc)# 生成静态库add_library(mindsporelite STATIC main.cc a.cc b.cc)</code></pre><p>但是对于一个较大的工程，一一列举会显得Cmake代码十分冗长，我们可以用<code>set</code>设置一个变量，包含所有需要的<code>.cc</code>或<code>.cpp</code>文件</p><pre><code>set(LITE_SRC        $&#123;API_SRC&#125;        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/common/context_util.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/common/file_utils.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/common/config_file.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/common/utils.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/common/graph_util.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/common/log.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/common/lite_utils.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/common/prim_util.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/common/tensor_util.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/runtime/inner_allocator.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/runtime/runtime_allocator.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/runtime/infer_manager.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/schema_tensor_wrapper.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/tensor.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/ms_tensor.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/executor.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/inner_context.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/lite_model.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/kernel_registry.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/inner_kernel.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/lite_kernel.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/lite_kernel_util.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/sub_graph_kernel.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/scheduler.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/lite_session.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/errorcode.cc        $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/cpu_info.cc        )# 生成动态库add_library(mindsporelite SHARED $&#123;LITE_SRC&#125;)# 生成静态库add_library(mindsporelite STATIC $&#123;LITE_SRC&#125;)</code></pre><p>当然除了动态库和静态库，我们还可以选择将代码编译成未链接的<code>.o</code>中间文件，在<code>add_library</code>使用<code>OBJECT</code>参数，使用方法如下</p><pre><code>add_library(&lt;name&gt; OBJECT [&lt;source&gt;...])add_library(... $&lt;TARGET_OBJECTS:objlib&gt; ...)add_executable(... $&lt;TARGET_OBJECTS:objlib&gt; ...)</code></pre><p>对于生成的中间产物，这些文件并未被链接，所以并不能作为库或执行，我们对这些中间产物还可以进行如<code>add_dependencies</code>的操作，<code>add_dependencies()</code>会为顶层目标添加一个依赖关系，可以保证某个目标在其他的目标之前被构建。比如<code>mindsporelite</code>依赖于<a href="https://github.com/google/flatbuffers">flatbuffers</a>（一个谷歌开源的序列化库）生成的<code>fbs</code>文件</p><pre><code>ms_build_flatbuffers_lite(FBS_FILES $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/schema/ fbs_src $&#123;CMAKE_BINARY_DIR&#125;/schema &quot;&quot;)#生成中间文件add_library(lite_src_mid OBJECT $&#123;LITE_SRC&#125;)#添加依赖add_dependencies(lite_src_mid fbs_src)# 生成动态库add_library(mindsporelite SHARED $&lt;TARGET_OBJECTS:lite_src_mid&gt;)# 生成静态库add_library(mindsporelite STATIC $&lt;TARGET_OBJECTS:lite_src_mid&gt;)</code></pre><h2 id="生成多个库或者可执行文件"><a href="#生成多个库或者可执行文件" class="headerlink" title="生成多个库或者可执行文件"></a>生成多个库或者可执行文件</h2><p>有时我们希望在一个项目中能编译多个独立的库或者可执行文件，比如在<code>mindsporelite</code>中,我们总共会生成以下可执行文件和动静态库</p><ul><li>Mindsporelite<ul><li>runtime<ul><li>libminddata-lite.a (数据加载静态库）     </li><li>libmindspore-lite-train.a（端侧训练静态库）</li><li>libmindspore-lite.a （端侧推理静态库）   </li><li>libminddata-lite.so (数据加载动态库）     </li><li>libmindspore-lite-train.so（端侧训练动态库）</li><li>libmindspore-lite.so （端侧推理动态库）  </li></ul></li><li>tools<ul><li>benchmark（基准测试·工具）</li><li>cropper（裁剪工具）</li><li>converter（模型转化工具）</li><li>benchmark_train（训练基准测试工具）</li><li>codegen（代码生成工具）</li></ul></li></ul></li></ul><p>而对于每个库或者可执行文件，一般在其相关的<code>.cc</code>文件目录下有一个<code>CmakeLists</code>文件，在最外侧目录的<code>CmakeLists</code>中通过<code>add_subdirectory</code>命令，指明本项目包含一个子目录 ，这样子目录下的 CMakeLists.txt 文件和源代码也会被处理，通过多层的<code>CmakeLists</code>我们一一构建各层的库和依赖。以下代码表示添加一个<br><code>src</code>的子目录</p><pre><code>add_subdirectory($&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/src)</code></pre><h2 id="链接库"><a href="#链接库" class="headerlink" title="链接库"></a>链接库</h2><p><code>add_subdirectory</code>往往和<code>target_link_libraries</code>一起使用，在我们编译好一个可执行文件或者库时，如果它依赖其他库,我们可以使用<code>target_link_libraries</code>将其链接其他库，方法如下</p><pre><code># 生成动态库add_library(mindsporelite SHARED $&lt;TARGET_OBJECTS:lite_src_mid&gt;)# 生成静态库add_library(mindsporelite-static STATIC $&lt;TARGET_OBJECTS:lite_src_mid&gt;)# 生成可执行文件add_executable(benchmark main.cc)# 动态链接target_link_libraries(benchmark mindsporelite）#静态链接target_link_libraries(benchmark mindsporelite-static)</code></pre><p>mindsporelite中的基准测试工具依赖于mindsporelite runtime库，链接代码如上，链接又分为动态链接和静态链接，静态链接会将库中所有的代码一起编译到可执行文件中，运行时速度更快，但包的大小更大，动态链接不会将库的代码编译到可执行文件中，文件更小，但在运行时会搜索动态库，运行速度慢。如果在系统目录和环境变量中找不到动态库，那么在运行时会报错，在Linux环境中，可以通过设置环境变量<code>LD_LIBRARY_PATH</code>指定动态库目录</p><pre><code>export LD_LIBRARY_PATH=/path/to/lib:$&#123;LD_LIBRARY_PATH&#125;</code></pre><p>这里有在链接库时，Cmake是如何找到对应库的位置的？在Cmake中，我们一般在文件开始添加 <a href="https://cmake.org/cmake/help/v3.20/command/add_compile_definitions.html">include_directories</a>（包含指定目录）或aux_source_directory（包含所有子目录）命令，Cmake会在这些目录下进行搜索。</p><pre><code>add_library (MathFunctions $&#123;DIR_LIB_SRCS&#125;)</code></pre><h2 id="编译选项和编译宏"><a href="#编译选项和编译宏" class="headerlink" title="编译选项和编译宏"></a>编译选项和编译宏</h2><p>对于跨平台的项目，我们往往要进行交叉编译，针对不同环境的代码和功能可能有所不同，<br>我们往往通过<code>option</code>确定编译选项，<a href="https://cmake.org/cmake/help/v3.20/command/add_compile_definitions.html">add_compile_definitions</a>来增加编译宏定义，不同平台编译的代码用宏进行隔离，同时编译宏也可以决定某部分代码是否编译，是否包含某个功能，对一些还不稳定的特性进行隔离，</p><p>比如,在mindsporelilte中，通过设置<code>MSLITE_ENABLE_SHARING_MEM_WITH_OPENGL</code>来确定是否编译OpenGL相关代码</p><pre><code>option(MSLITE_ENABLE_SHARING_MEM_WITH_OPENGL &quot;enable sharing memory with OpenGL&quot; on)if(DEFINED ENV&#123;MSLITE_ENABLE_SHARING_MEM_WITH_OPENGL&#125;)    set(MSLITE_ENABLE_SHARING_MEM_WITH_OPENGL $ENV&#123;MSLITE_ENABLE_SHARING_MEM_WITH_OPENGL&#125;)endif()if(MSLITE_ENABLE_SHARING_MEM_WITH_OPENGL)    add_definitions(-DENABLE_OPENGL_TEXTURE)endif()</code></pre><p>在相关cpp文件中用宏<code>ENABLE_OPENGL_TEXTURE</code>隔离相关代码</p><pre><code>#ifdef ENABLE_OPENGL_TEXTURE  if (!lite::opencl::LoadOpenGLLibrary(&amp;gl_handle_)) &#123;    MS_LOG(ERROR) &lt;&lt; &quot;Load OpenGL symbols failed!&quot;;    return RET_ERROR;  &#125;#endif</code></pre><p>对于任何跨平台的Cpp项目，其Cmake的架构基本大同小异，按层级结构一一编译各个动静态库，最后链接成一个可执行文件或者库。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul><li><a href="https://www.hahack.com/codes/cmake/">CMake 入门实战</a></li><li><a href="https://gist.github.com/mbinna/c61dbb39bca0e4fb7d1f73b0d66a4fd1">Effective Modern CMake</a></li><li><a href="https://cmake.org/cmake/help/v3.20/">Cmake官方文档</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 《栈溢出工程师》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CMake </tag>
            
            <tag> CPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DQN实现机器人自动走迷宫</title>
      <link href="2021/06/15/%E6%9C%BA%E5%99%A8%E4%BA%BA%E8%87%AA%E5%8A%A8%E8%B5%B0%E8%BF%B7%E5%AE%AB/"/>
      <url>2021/06/15/%E6%9C%BA%E5%99%A8%E4%BA%BA%E8%87%AA%E5%8A%A8%E8%B5%B0%E8%BF%B7%E5%AE%AB/</url>
      
        <content type="html"><![CDATA[<h2 id="问题简介"><a href="#问题简介" class="headerlink" title="问题简介"></a>问题简介</h2><p><img src="https://imgbed.momodel.cn/20200914145238.png" alt="image"></p><p>如上图所示，左上角的红色椭圆既是起点也是机器人的初始位置，右下角的绿色方块是出口。<br>游戏规则为：从起点开始，通过错综复杂的迷宫，到达目标点(出口)。</p><ul><li>在任一位置可执行动作包括：向上走 <code>&#39;u&#39;</code>、向右走 <code>&#39;r&#39;</code>、向下走 <code>&#39;d&#39;</code>、向左走 <code>&#39;l&#39;</code>。</li><li>执行不同的动作后，根据不同的情况会获得不同的奖励，具体而言，有以下几种情况。<ul><li>撞墙</li><li>走到出口</li><li>其余情况</li></ul></li></ul><p>我们分别实现了<strong>基于基础搜索算法</strong>和 <strong>Deep QLearning 算法</strong>的机器人，使机器人自动走到迷宫的出口。</p><h2 id="设计思想"><a href="#设计思想" class="headerlink" title="设计思想"></a>设计思想</h2><h3 id="Maze-类介绍"><a href="#Maze-类介绍" class="headerlink" title="Maze 类介绍"></a>Maze 类介绍</h3><h4 id="创建迷宫"><a href="#创建迷宫" class="headerlink" title="创建迷宫"></a>创建迷宫</h4><p>通过迷宫类 Maze 可以随机创建一个迷宫。</p><ol><li>使用  Maze(maze_size=size)  来随机生成一个 size * size 大小的迷宫。</li><li>使用 print() 函数可以输出迷宫的 size 以及画出迷宫图</li><li>红色的圆是机器人初始位置</li><li>绿色的方块是迷宫的出口位置</li></ol><pre><code>&quot;&quot;&quot; 创建迷宫并展示 &quot;&quot;&quot;maze = Maze(maze_size=10) # 随机生成迷宫print(maze)</code></pre><h4 id="重要的成员方法"><a href="#重要的成员方法" class="headerlink" title="重要的成员方法"></a>重要的成员方法</h4><p>在迷宫中已经初始化一个机器人，你要编写的算法实现在给定条件下控制机器人移动至目标点。</p><p>Maze 类中重要的成员方法如下：</p><ol><li>sense_robot() ：获取机器人在迷宫中目前的位置。</li></ol><blockquote><p>return：机器人在迷宫中目前的位置。</p></blockquote><ol><li>move_robot(direction) ：根据输入方向移动默认机器人，若方向不合法则返回错误信息。</li></ol><blockquote><p>direction：移动方向, 如:”u”, 合法值为： [‘u’, ‘r’, ‘d’, ‘l’]</p><p>return：执行动作的奖励值</p></blockquote><ol><li>can_move_actions(position)：获取当前机器人可以移动的方向</li></ol><blockquote><p>position：迷宫中任一处的坐标点 </p><p>return：该点可执行的动作，如：[‘u’,’r’,’d’]</p></blockquote><ol><li>is_hit_wall(self, location, direction)：判断该移动方向是否撞墙</li></ol><blockquote><p>location, direction：当前位置和要移动的方向，如(0,0) , “u”</p><p>return：True(撞墙) / False(不撞墙)</p></blockquote><ol><li>draw_maze()：画出当前的迷宫</li></ol><p><strong>随机移动机器人，并记录下获得的奖励，展示出机器人最后的位置。</strong></p><pre><code>import randomrewards = [] # 记录每走一步的奖励值actions = [] # 记录每走一步的移动方向# 循环、随机移动机器人10次，记录下奖励for i in range(10):    valid_actions = maze.can_move_actions(maze.sense_robot())    action = random.choice(valid_actions)    rewards.append(maze.move_robot(action))    actions.append(action)print(&quot;the history of rewards:&quot;, rewards)print(&quot;the actions&quot;, actions)# 输出机器人最后的位置print(&quot;the end position of robot:&quot;, maze.sense_robot())# 打印迷宫，观察机器人位置print(maze)</code></pre><h3 id="基础搜索算法介绍（广度优先搜索算法）"><a href="#基础搜索算法介绍（广度优先搜索算法）" class="headerlink" title="基础搜索算法介绍（广度优先搜索算法）"></a>基础搜索算法介绍（广度优先搜索算法）</h3><p>对于迷宫游戏，常见的三种的搜索算法有广度优先搜索、深度优先搜索和最佳优先搜索（A*)。</p><p>在下面的代码示例中，将实现广度优先搜索算法；主要通过建立一颗搜索树并进行层次遍历实现。</p><ul><li>每个节点表示为以 <code>Class SearchTree</code> 实例化的对象，类属性有：<strong>当前节点位置、到达当前节点的动作、当前节点的父节点、当前节点的子节点</strong>；</li><li><code>valid_actions():</code> 用以获取机器人可以行走的位置（即不能穿墙）；</li><li><code>expand():</code> 对于未拓展的子节点进行拓展；</li><li><code>backpropagation():</code> 回溯搜索路径。</li></ul><h4 id="算法具体步骤"><a href="#算法具体步骤" class="headerlink" title="算法具体步骤"></a>算法具体步骤</h4><p>首先以机器人起始位置建立根节点，并入队；接下来不断重复以下步骤直到判定条件:</p><ol><li>将队首节点的位置标记已访问；判断队首是否为目标位置(出口)， <strong>是</strong> 则终止循环并记录回溯路径</li><li>判断队首节点是否为叶子节点，<strong>是</strong> 则拓展该叶子节点</li><li>如果队首节点有子节点，则将每个子节点插到队尾</li><li>将队首节点出队</li></ol><h4 id="实现广度优先搜索算法"><a href="#实现广度优先搜索算法" class="headerlink" title="实现广度优先搜索算法"></a>实现广度优先搜索算法</h4><pre><code>import numpy as np# 机器人移动方向move_map = &#123;    &#39;u&#39;: (-1, 0), # up    &#39;r&#39;: (0, +1), # right    &#39;d&#39;: (+1, 0), # down    &#39;l&#39;: (0, -1), # left&#125;# 迷宫路径搜索树class SearchTree(object):    def __init__(self, loc=(), action=&#39;&#39;, parent=None):        &quot;&quot;&quot;        初始化搜索树节点对象        :param loc: 新节点的机器人所处位置        :param action: 新节点的对应的移动方向        :param parent: 新节点的父辈节点        &quot;&quot;&quot;        self.loc = loc  # 当前节点位置        self.to_this_action = action  # 到达当前节点的动作        self.parent = parent  # 当前节点的父节点        self.children = []  # 当前节点的子节点    def add_child(self, child):        &quot;&quot;&quot;        添加子节点        :param child:待添加的子节点        &quot;&quot;&quot;        self.children.append(child)    def is_leaf(self):        &quot;&quot;&quot;        判断当前节点是否是叶子节点        &quot;&quot;&quot;        return len(self.children) == 0def expand(maze, is_visit_m, node):    &quot;&quot;&quot;    拓展叶子节点，即为当前的叶子节点添加执行合法动作后到达的子节点    :param maze: 迷宫对象    :param is_visit_m: 记录迷宫每个位置是否访问的矩阵    :param node: 待拓展的叶子节点    &quot;&quot;&quot;    can_move = maze.can_move_actions(node.loc)    for a in can_move:        new_loc = tuple(node.loc[i] + move_map[a][i] for i in range(2))        if not is_visit_m[new_loc]:            child = SearchTree(loc=new_loc, action=a, parent=node)            node.add_child(child)def back_propagation(node):    &quot;&quot;&quot;    回溯并记录节点路径    :param node: 待回溯节点    :return: 回溯路径    &quot;&quot;&quot;    path = []    while node.parent is not None:        path.insert(0, node.to_this_action)        node = node.parent    return pathdef breadth_first_search(maze):    &quot;&quot;&quot;    对迷宫进行广度优先搜索    :param maze: 待搜索的maze对象    &quot;&quot;&quot;    start = maze.sense_robot()    root = SearchTree(loc=start)    queue = [root]  # 节点队列，用于层次遍历    h, w, _ = maze.maze_data.shape    is_visit_m = np.zeros((h, w), dtype=np.int)  # 标记迷宫的各个位置是否被访问过    path = []  # 记录路径    while True:        current_node = queue[0]        is_visit_m[current_node.loc] = 1  # 标记当前节点位置已访问        if current_node.loc == maze.destination:  # 到达目标点            path = back_propagation(current_node)            break        if current_node.is_leaf():            expand(maze, is_visit_m, current_node)        # 入队        for child in current_node.children:            queue.append(child)        # 出队        queue.pop(0)    return path</code></pre><p><strong>测试广度优先搜索算法</strong></p><pre><code>maze = Maze(maze_size=10)height, width, _ = maze.maze_data.shapepath_1 = breadth_first_search(maze)print(&quot;搜索出的路径：&quot;, path_1)for action in path_1:    maze.move_robot(action)if maze.sense_robot() == maze.destination:    print(&quot;恭喜你，到达了目标点&quot;)print(maze)</code></pre><h4 id="深度优先搜索算法"><a href="#深度优先搜索算法" class="headerlink" title="深度优先搜索算法"></a>深度优先搜索算法</h4><ul><li><p>输入：迷宫</p></li><li><p>输出：到达目标点的路径</p></li></ul><pre><code>def my_search(maze):    &quot;&quot;&quot;    对迷宫实现深度优先搜索算法    :param maze: 迷宫对象    :return :到达目标点的路径 如：[&quot;u&quot;,&quot;u&quot;,&quot;r&quot;,...]    &quot;&quot;&quot;    start = maze.sense_robot()    root = SearchTree(loc=start)    h, w, _ = maze.maze_data.shape    is_visit_m = np.zeros((h, w), dtype=np.int)  # 标记迷宫的各个位置是否被访问过    path = []  # 记录路径    path = dfs(maze, is_visit_m, root)    return pathdef dfs(maze, is_visit_m, current_node):    if current_node.loc == maze.destination:  # 到达目标点        path = back_propagation(current_node)        return path    elif current_node.is_leaf():        expand(maze, is_visit_m, current_node)        for node in current_node.children:            is_visit_m[node.loc] = 1            path = dfs(maze, is_visit_m, node)            if not path == None:                return path    else:        return None</code></pre><h5 id="测试深度优先算法"><a href="#测试深度优先算法" class="headerlink" title="测试深度优先算法"></a>测试深度优先算法</h5><pre><code>maze = Maze(maze_size=10) # 从文件生成迷宫path_2 = my_search(maze)print(&quot;搜索出的路径：&quot;, path_2)for action in path_2:    maze.move_robot(action)if maze.sense_robot() == maze.destination:    print(&quot;恭喜你，到达了目标点&quot;)</code></pre><h3 id="强化学习算法介绍"><a href="#强化学习算法介绍" class="headerlink" title="强化学习算法介绍"></a>强化学习算法介绍</h3><p>强化学习作为机器学习算法的一种，其模式也是让智能体在“训练”中学到“经验”，以实现给定的任务。但不同于监督学习与非监督学习，在强化学习的框架中，我们更侧重通过智能体与环境的<strong>交互</strong>来学习。通常在监督学习和非监督学习任务中，智能体往往需要通过给定的训练集，辅之以既定的训练目标（如最小化损失函数），通过给定的学习算法来实现这一目标。然而在强化学习中，智能体则是通过其与环境交互得到的奖励进行学习。这个环境可以是虚拟的（如虚拟的迷宫），也可以是真实的（自动驾驶汽车在真实道路上收集数据）。</p><p>在强化学习中有五个核心组成部分，它们分别是：<strong>环境（Environment）</strong>、<strong>智能体（Agent）</strong>、<strong>状态（State）</strong>、<strong>动作（Action）</strong>和<strong>奖励（Reward）</strong>。</p><p>在某一时间节点 $t$：</p><ul><li>智能体在从环境中感知其所处的状态 $s_t$</li><li>智能体根据某些准则选择动作 $a_t$</li><li>环境根据智能体选择的动作，向智能体反馈奖励 $r_{t+1}$</li></ul><p>通过合理的学习算法，智能体将在这样的问题设置下，成功学到一个在状态 $s_t$ 选择动作 $a_t$ 的策略 $\pi (s_t) = a_t$。</p><p><img src="https://imgbed.momodel.cn/20200914153419.png" alt=""></p><h3 id="QLearning-算法"><a href="#QLearning-算法" class="headerlink" title="QLearning 算法"></a>QLearning 算法</h3><p>Q-Learning 是一个值迭代（Value Iteration）算法。<br>与策略迭代（Policy Iteration）算法不同，值迭代算法会计算每个”状态“或是”状态-动作“的值（Value）或是效用（Utility），然后在执行动作的时候，会设法最大化这个值。<br>因此，对每个状态值的准确估计，是值迭代算法的核心。<br>通常会考虑<strong>最大化动作的长期奖励</strong>，即不仅考虑当前动作带来的奖励，还会考虑动作长远的奖励。</p><h4 id="Q-值的计算与迭代"><a href="#Q-值的计算与迭代" class="headerlink" title="Q 值的计算与迭代"></a>Q 值的计算与迭代</h4><p>Q-learning 算法将状态（state）和动作（action）构建成一张 Q_table 表来存储 Q 值，Q 表的行代表状态（state），列代表动作（action）：</p><p><img src="https://imgbed.momodel.cn/20200914161241.png" alt=""></p><p>在 Q-Learning 算法中，将这个长期奖励记为 Q 值，其中会考虑每个 ”状态-动作“ 的 Q 值，具体而言，它的计算公式为：</p><script type="math/tex; mode=display">Q(s_{t},a) = R_{t+1} + \gamma \times\max_a Q(a,s_{t+1})</script><p>也就是对于当前的“状态-动作” $(s<em>{t},a)$，考虑执行动作 $a$ 后环境奖励 $R</em>{t+1}$，以及执行动作 $a$ 到达 $s<em>{t+1}$后，执行任意动作能够获得的最大的Q值 $\max_a Q(a,s</em>{t+1})$，$\gamma$ 为折扣因子。</p><p>计算得到新的 Q 值之后，一般会使用更为保守地更新 Q 表的方法，即引入松弛变量 $alpha$ ，按如下的公式进行更新，使得 Q 表的迭代变化更为平缓。</p><script type="math/tex; mode=display">Q(s_{t},a) = (1-\alpha) \times Q(s_{t},a) + \alpha \times(R_{t+1} + \gamma \times\max_a Q(a,s_{t+1}))</script><h4 id="机器人动作的选择"><a href="#机器人动作的选择" class="headerlink" title="机器人动作的选择"></a>机器人动作的选择</h4><p>在强化学习中，<strong>探索-利用</strong> 问题是非常重要的问题。<br>具体来说，根据上面的定义，会尽可能地让机器人在每次选择最优的决策，来最大化长期奖励。<br>但是这样做有如下的弊端：    </p><ol><li>在初步的学习中，Q 值是不准确的，如果在这个时候都按照 Q 值来选择，那么会造成错误。</li><li>学习一段时间后，机器人的路线会相对固定，则机器人无法对环境进行有效的探索。</li></ol><p>因此需要一种办法，来解决如上的问题，增加机器人的探索。<br>通常会使用 <strong>epsilon-greedy</strong> 算法：</p><ol><li>在机器人选择动作的时候，以一部分的概率随机选择动作，以一部分的概率按照最优的 Q 值选择动作。</li><li>同时，这个选择随机动作的概率应当随着训练的过程逐步减小。</li></ol><p><img src="http://imgbed.momodel.cn/20200602153554.png" width=400><br><img src="http://imgbed.momodel.cn/20200601144827.png" width=400></p><h4 id="Q-Learning-算法的学习过程"><a href="#Q-Learning-算法的学习过程" class="headerlink" title="Q-Learning 算法的学习过程"></a>Q-Learning 算法的学习过程</h4><p><img src="http://imgbed.momodel.cn/20200601170657.png" width=900></p><h3 id="QRobot-类"><a href="#QRobot-类" class="headerlink" title="QRobot 类"></a>QRobot 类</h3><p> QRobot 类，其中实现了 Q 表迭代和机器人动作的选择策略，可通过 <code>from QRobot import QRobot</code> 导入使用。</p><p><strong>QRobot 类的核心成员方法</strong></p><ol><li>sense_state()：获取当前机器人所处位置</li></ol><blockquote><p>return：机器人所处的位置坐标，如： (0, 0)</p></blockquote><ol><li>current_state_valid_actions()：获取当前机器人可以合法移动的动作</li></ol><blockquote><p>return：由当前合法动作组成的列表，如： [‘u’,’r’]</p></blockquote><ol><li>train_update()：以<strong>训练状态</strong>，根据 QLearning 算法策略执行动作</li></ol><blockquote><p>return：当前选择的动作，以及执行当前动作获得的回报, 如： ‘u’, -1</p></blockquote><ol><li>test_update()：以<strong>测试状态</strong>，根据 QLearning 算法策略执行动作</li></ol><blockquote><p>return：当前选择的动作，以及执行当前动作获得的回报, 如：’u’, -1</p></blockquote><ol><li>reset()</li></ol><blockquote><p>return：重置机器人在迷宫中的位置</p></blockquote><pre><code>from QRobot import QRobotfrom Maze import Mazemaze = Maze(maze_size=5) # 随机生成迷宫robot = QRobot(maze) # 记得将 maze 变量修改为你创建迷宫的变量名action, reward = robot.train_update() # QLearning 算法一次Q值迭代和动作选择print(&quot;the choosed action: &quot;, action)print(&quot;the returned reward: &quot;, action)</code></pre><h3 id="Runner-类"><a href="#Runner-类" class="headerlink" title="Runner 类"></a>Runner 类</h3><p>QRobot 类实现了 QLearning 算法的 Q 值迭代和动作选择策略。在机器人自动走迷宫的训练过程中，需要不断的使用 QLearning 算法来迭代更新 Q 值表，以达到一个“最优”的状态，因此封装好了一个类 Runner 用于机器人的训练和可视化。可通过 <code>from Runner import Runner</code> 导入使用。</p><p><strong>Runner 类的核心成员方法：</strong></p><ol><li>run_training(training_epoch, training_per_epoch=150): 训练机器人，不断更新 Q 表，并讲训练结果保存在成员变量 train_robot_record 中</li></ol><blockquote><p>training_epoch, training_per_epoch: 总共的训练次数、每次训练机器人最多移动的步数</p></blockquote><ol><li><p>run_testing()：测试机器人能否走出迷宫</p></li><li><p>generate_gif(filename)：将训练结果输出到指定的 gif 图片中</p></li></ol><blockquote><p>filename：合法的文件路径,文件名需以 <code>.gif</code> 为后缀</p></blockquote><ol><li>plot_results()：以图表展示训练过程中的指标：Success Times、Accumulated Rewards、Runing Times per Epoch</li></ol><p><strong>设定训练参数、训练、查看结果</strong></p><pre><code>from QRobot import QRobotfrom Maze import Mazefrom Runner import Runner&quot;&quot;&quot;  Qlearning 算法相关参数： &quot;&quot;&quot;epoch = 10  # 训练轮数epsilon0 = 0.5  # 初始探索概率alpha = 0.5  # 公式中的 ⍺gamma = 0.9  # 公式中的 γmaze_size = 5  # 迷宫size&quot;&quot;&quot; 使用 QLearning 算法训练过程 &quot;&quot;&quot;g = Maze(maze_size=maze_size)r = QRobot(g, alpha=alpha, epsilon0=epsilon0, gamma=gamma)runner = Runner(r)runner.run_training(epoch, training_per_epoch=int(maze_size * maze_size * 1.5))# 生成训练过程的gif图, 建议下载到本地查看；也可以注释该行代码，加快运行速度。runner.generate_gif(filename=&quot;results/size5.gif&quot;)runner.plot_results() # 输出训练结果，可根据该结果对您的机器人进行分析。</code></pre><h3 id="实现-Deep-QLearning-算法"><a href="#实现-Deep-QLearning-算法" class="headerlink" title="实现 Deep QLearning 算法"></a>实现 Deep QLearning 算法</h3><h4 id="DQN-算法介绍"><a href="#DQN-算法介绍" class="headerlink" title="DQN 算法介绍"></a>DQN 算法介绍</h4><p>强化学习是一个反复迭代的过程，每一次迭代要解决两个问题：给定一个策略求值函数，和根据值函数来更新策略。而 DQN 算法使用神经网络来近似值函数。(<a href="https://files.momodel.cn/Playing%20Atari%20with%20Deep%20Reinforcement%20Learning.pdf">DQN 论文地址</a>)</p><ul><li><strong>DQN 算法流程</strong></li></ul><p><img src="https://imgbed.momodel.cn/20200918101051.png" alt=""></p><ul><li><strong>DQN 算法框架图</strong></li></ul><p><img src="https://imgbed.momodel.cn/20200918101137.png" alt=""></p><h4 id="完成-DQN-算法"><a href="#完成-DQN-算法" class="headerlink" title="完成 DQN 算法"></a>完成 DQN 算法</h4><p><strong>ReplayDataSet 类的核心成员方法</strong></p><ul><li>add(self, state, action_index, reward, next_state, is_terminal) 添加一条训练数据</li></ul><blockquote><p>state: 当前机器人位置</p><p>action_index: 选择执行动作的索引</p><p>reward： 执行动作获得的回报</p><p>next_state：执行动作后机器人的位置</p><p>is_terminal：机器人是否到达了终止节点（到达终点或者撞墙）</p></blockquote><ul><li>random_sample(self, batch_size)：从数据集中随机抽取固定batch_size的数据</li></ul><blockquote><p>batch_size: 整数，不允许超过数据集中数据的个数</p></blockquote><ul><li><strong>build_full_view(self, maze)：开启金手指，获取全图视野</strong></li></ul><blockquote><p>maze: 以 Maze 类实例化的对象</p></blockquote><pre><code>&quot;&quot;&quot;ReplayDataSet 类的使用&quot;&quot;&quot;from ReplayDataSet import ReplayDataSettest_memory = ReplayDataSet(max_size=1e3) # 初始化并设定最大容量actions = [&#39;u&#39;, &#39;r&#39;, &#39;d&#39;, &#39;l&#39;]  test_memory.add((0,1), actions.index(&quot;r&quot;), -10, (0,1), 1)  # 添加一条数据（state, action_index, reward, next_state）print(test_memory.random_sample(1)) # 从中随机抽取一条（因为只有一条数据）</code></pre><h4 id="实现DQNRobot"><a href="#实现DQNRobot" class="headerlink" title="实现DQNRobot"></a>实现DQNRobot</h4><ul><li><strong>输入:</strong> 由 Maze 类实例化的对象 maze</li></ul><pre><code>from QRobot import QRobotclass Robot(QRobot):valid_action = [&#39;u&#39;, &#39;r&#39;, &#39;d&#39;, &#39;l&#39;]&#39;&#39;&#39; QLearning parameters&#39;&#39;&#39;epsilon0 = 0.5  # 初始贪心算法探索概率gamma = 0.94  # 公式中的 γEveryUpdate = 1  # the interval of target model&#39;s updating&quot;&quot;&quot;some parameters of neural network&quot;&quot;&quot;target_model = Noneeval_model = Nonebatch_size = 32learning_rate = 1e-2TAU = 1e-3step = 1  # 记录训练的步数&quot;&quot;&quot;setting the device to train network&quot;&quot;&quot;device = torch.device(&quot;cuda:0&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)def __init__(self, maze):    &quot;&quot;&quot;    初始化 Robot 类    :param maze:迷宫对象    &quot;&quot;&quot;    super(Robot, self).__init__(maze)    maze.set_reward(reward=&#123;        &quot;hit_wall&quot;: 10.,        &quot;destination&quot;: -50.,        &quot;default&quot;: 1.,    &#125;)    self.maze = maze    self.maze_size = maze.maze_size    &quot;&quot;&quot;build network&quot;&quot;&quot;    self.target_model = None    self.eval_model = None    self._build_network()    &quot;&quot;&quot;create the memory to store data&quot;&quot;&quot;    max_size = max(self.maze_size ** 2 * 3, 1e4)    self.memory = ReplayDataSet(max_size=max_size)def _build_network(self):    seed = 0    random.seed(seed)    &quot;&quot;&quot;build target model&quot;&quot;&quot;    self.target_model = QNetwork(state_size=2, action_size=4, seed=seed).to(self.device)    &quot;&quot;&quot;build eval model&quot;&quot;&quot;    self.eval_model = QNetwork(state_size=2, action_size=4, seed=seed).to(self.device)    &quot;&quot;&quot;build the optimizer&quot;&quot;&quot;    self.optimizer = optim.Adam(self.eval_model.parameters(), lr=self.learning_rate)def target_replace_op(self):    &quot;&quot;&quot;        Soft update the target model parameters.        θ_target = τ*θ_local + (1 - τ)*θ_target    &quot;&quot;&quot;    # for target_param, eval_param in zip(self.target_model.parameters(), self.eval_model.parameters()):    #     target_param.data.copy_(self.TAU * eval_param.data + (1.0 - self.TAU) * target_param.data)    &quot;&quot;&quot; replace the whole parameters&quot;&quot;&quot;    self.target_model.load_state_dict(self.eval_model.state_dict())def _choose_action(self, state):    state = np.array(state)    state = torch.from_numpy(state).float().to(self.device)    if random.random() &lt; self.epsilon:        action = random.choice(self.valid_action)    else:        self.eval_model.eval()        with torch.no_grad():            q_next = self.eval_model(state).cpu().data.numpy()  # use target model choose action        self.eval_model.train()        action = self.valid_action[np.argmin(q_next).item()]    return actiondef _learn(self, batch: int = 16):    if len(self.memory) &lt; batch:        print(&quot;the memory data is not enough&quot;)        return    state, action_index, reward, next_state, is_terminal = self.memory.random_sample(batch)    &quot;&quot;&quot; convert the data to tensor type&quot;&quot;&quot;    state = torch.from_numpy(state).float().to(self.device)    action_index = torch.from_numpy(action_index).long().to(self.device)    reward = torch.from_numpy(reward).float().to(self.device)    next_state = torch.from_numpy(next_state).float().to(self.device)    is_terminal = torch.from_numpy(is_terminal).int().to(self.device)    self.eval_model.train()    self.target_model.eval()    &quot;&quot;&quot;Get max predicted Q values (for next states) from target model&quot;&quot;&quot;    Q_targets_next = self.target_model(next_state).detach().min(1)[0].unsqueeze(1)    &quot;&quot;&quot;Compute Q targets for current states&quot;&quot;&quot;    Q_targets = reward + self.gamma * Q_targets_next * (torch.ones_like(is_terminal) - is_terminal)    &quot;&quot;&quot;Get expected Q values from local model&quot;&quot;&quot;    self.optimizer.zero_grad()    Q_expected = self.eval_model(state).gather(dim=1, index=action_index)    &quot;&quot;&quot;Compute loss&quot;&quot;&quot;    loss = F.mse_loss(Q_expected, Q_targets)    loss_item = loss.item()    &quot;&quot;&quot; Minimize the loss&quot;&quot;&quot;    loss.backward()    self.optimizer.step()    &quot;&quot;&quot;copy the weights of eval_model to the target_model&quot;&quot;&quot;    self.target_replace_op()    return loss_itemdef train_update(self):    &quot;&quot;&quot;    以训练状态选择动作并更新Deep Q network的相关参数    :return :action, reward 如：&quot;u&quot;, -1    &quot;&quot;&quot;    state = self.sense_state()    action = self._choose_action(state)    reward = self.maze.move_robot(action)    next_state = self.sense_state()    is_terminal = 1 if next_state == self.maze.destination or next_state == state else 0    self.memory.add(state, self.valid_action.index(action), reward, next_state, is_terminal)    &quot;&quot;&quot;--间隔一段时间更新target network权重--&quot;&quot;&quot;    if self.step % self.EveryUpdate == 0:        self._learn(batch=32)    &quot;&quot;&quot;---update the step and epsilon---&quot;&quot;&quot;    self.step += 1    self.epsilon = max(0.01, self.epsilon * 0.995)    return action, rewarddef test_update(self):    &quot;&quot;&quot;    以测试状态选择动作并更新Deep Q network的相关参数    :return : action, reward 如：&quot;u&quot;, -1    &quot;&quot;&quot;    state = np.array(self.sense_state(), dtype=np.int16)    state = torch.from_numpy(state).float().to(self.device)    self.eval_model.eval()    with torch.no_grad():        q_value = self.eval_model(state).cpu().data.numpy()    action = self.valid_action[np.argmin(q_value).item()]    reward = self.maze.move_robot(action)    return action, reward</code></pre><h4 id="测试-DQN-算法"><a href="#测试-DQN-算法" class="headerlink" title="测试 DQN 算法"></a>测试 DQN 算法</h4><pre><code>from QRobot import QRobotfrom Maze import Mazefrom Runner import Runner&quot;&quot;&quot;  Deep Qlearning 算法相关参数： &quot;&quot;&quot;epoch = 10  # 训练轮数maze_size = 5  # 迷宫sizetraining_per_epoch=int(maze_size * maze_size * 1.5)&quot;&quot;&quot; 使用 DQN 算法训练 &quot;&quot;&quot;g = Maze(maze_size=maze_size)r = Robot(g)runner = Runner(r)runner.run_training(epoch, training_per_epoch)# 生成训练过程的gif图, 建议下载到本地查看；也可以注释该行代码，加快运行速度。runner.generate_gif(filename=&quot;results/dqn_size10.gif&quot;)</code></pre><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="基础搜索算法"><a href="#基础搜索算法" class="headerlink" title="基础搜索算法"></a>基础搜索算法</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210628212105.png" alt=""></p><h3 id="深度强化学习算法"><a href="#深度强化学习算法" class="headerlink" title="深度强化学习算法"></a>深度强化学习算法</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210628212103.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210628212106.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DQN </tag>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文献笔记：Playing Atari with Deep Reinforcement Learning</title>
      <link href="2021/06/14/DQN/"/>
      <url>2021/06/14/DQN/</url>
      
        <content type="html"><![CDATA[<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>这篇提出了第一个深度学习模型，成功地直接从高维输入学习控制策略的强化学习。该模型是一个卷积神经网络，使用Q-learing的变体训练，其输入是原始像素，输出是估计未来奖励的Value函数。该方法在七款街机游戏中取得了不错的效果</p><h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>学习高维度信息比如视觉和语音是强化学习的一大挑战，这样的强化学习非常依赖于数据（特征工程）的质量，而深度学习方法可以很好地提取高维度特征简单来说深度神经网络可以看做一个可以拟合任何复杂函数的工具，只要网络的深度足够大，但是深度学习需要大量人工标注的数据<br>，而且深度学习假设所有数据是独立同分布的，而强化学习的数据来自于环境，且随环境进行变化。在这篇文章中，作者用卷积神经网络，从复杂的RL环境中的原始视频数据中学习成功的控制策略。该网络用q-学习算法的一个变体进行训练，通过随机梯度下降来更新权值。</p><h1 id="environment"><a href="#environment" class="headerlink" title="environment"></a>environment</h1><p>在Atari game中我们考虑一个agent与环境$\epsilon$交互的过程，用一个向量$x<em>t$表示当前屏幕显示的图像的像素值，动作（action）定义为当前玩家可以采取的操作，奖励（reward）定义为游戏得分的改变。因为游戏的当前状态不能只用当前的屏幕图像表示，所以作者采用了图像和动作的时间序列 $s_t = x_1, a_1, x_2, …, a</em>{t−1},x_t$来表示状态State<br>，总结一下，强化学习中各个核心要素在本任务中的表示如下</p><ul><li>State：图像和动作的时间序列</li><li>Action：玩家采取的操作</li><li>Reward：游戏得分的改变</li></ul><p>在传统的强化学习（Q-Learning）中我们通常用Bellman 方程来更新Q值，公式如下</p><script type="math/tex; mode=display">Q^{*}(s, a)=\mathbb{E}_{s^{\prime} \sim \mathcal{E}}\left[r+\gamma \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right) \mid s, a\right]</script><p>但在实践中，这种基本方法是完全不切实际的，因为Q函数是为每个序列单独估计的，而没有通用性，我们可以考虑用一个函数来近似Q函数，$Q(s,a;\theta)\approx Q^*(s,a)$,但在强化学习中采用的近似器通常是非线性的，所以我们用一个非线性的近似器-神经网络来近似，我们称之为Q-network，在这个网络训练过程中，损失函数如下定义</p><script type="math/tex; mode=display">L_{i}\left(\theta_{i}\right)=\mathbb{E}_{s, a \sim \rho(\cdot)}\left[\left(y_{i}-Q\left(s, a ; \theta_{i}\right)\right)^{2}\right]</script><p>其中target为$y<em>i=Q^{*}(s,a)=\mathbb{E}</em>{s^{\prime}\sim\mathcal{E}}\left[r+\gamma \max_{a^{\prime}}Q^{*}\left(s^{\prime}, a^{\prime}\right) \mid s,a\right]$</p><p>梯度如下定义</p><script type="math/tex; mode=display">\nabla_{\theta_{i}} L_{i}\left(\theta_{i}\right)=\mathbb{E}_{s, a \sim \rho(\cdot) ; s^{\prime} \sim \mathcal{E}}\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i-1}\right)-Q\left(s, a ; \theta_{i}\right)\right) \nabla_{\theta_{i}} Q\left(s, a ; \theta_{i}\right)\right]</script><p>这篇文章采用了off-policy的策略，model-free的方式进行</p><h1 id="Deep-Reinforcement-Learning"><a href="#Deep-Reinforcement-Learning" class="headerlink" title="Deep Reinforcement Learning"></a>Deep Reinforcement Learning</h1><p>不同于一些在线学习的方式，这篇文章中采用了experience replay技术，简单来说就是将每个时间步的状态和动作反馈（$e<em>t=s_t,a_t,r_t,s</em>{t+1}$)存储在一个数据集D中,在每次算法内循环中，会从数据集D中随机抽取数据先进行训练，更新Q值，然后执行$\epsilon-greedy$的策略来探索新状态，因为用完整的的时间序列来表示比较困难，所以在算法中用定长序列来表示状态，状态由$\phi$函数表示，具体的算法过程如下<br><img src="https://imgbed.momodel.cn/20200918101051.png" alt=""></p><p><img src="https://imgbed.momodel.cn/20200918101137.png" alt=""></p><h2 id="DQN的优点"><a href="#DQN的优点" class="headerlink" title="DQN的优点"></a>DQN的优点</h2><ul><li>相比Q-learning，DQN可以在每一步进行多个权重的更新</li><li>由于样本之间的强相关性，直接从连续样本中学习效率低效；随机化样本会打破这些相关性，从而减少更新的方差。</li><li>当学习策略时，当前参数确定参数训练的下一个数据样本。例如，如果最大化动作是向左移动，那么训练样本将由左侧的样本主导；如果最大化动作然后切换到右边，那么训练分布也会切换。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DQN </tag>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Eigenface</title>
      <link href="2021/05/27/Eigenface/"/>
      <url>2021/05/27/Eigenface/</url>
      
        <content type="html"><![CDATA[<h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>特征脸（eigenface）是第一种有效的人脸识别方法，通过在一大组描述不同人脸的图像上进行主成分分析（PCA），其本质是用一种称为“特征人脸(eigenface)”的特征向量按照线性组合形式来表达每一张原始人脸图像，进而实现人脸识别。为了提高准确率，我们尽可能取出其中含有信息较多（即方差较大）的坐标轴（基），构成（span）一个新的空间，舍弃其他维度的信息。由于新空间的维度小于原来的空间，所以把数据投影到新的空间后，可以大大降低数据的复杂度（虽然会损失少量信息）。</p><h2 id="设计思想"><a href="#设计思想" class="headerlink" title="设计思想"></a>设计思想</h2><p>首先我们需要把所有二维的人脸squeeze为一个一维的向量，$row\times col \to 1\times (row\times col)$，然后进行如下算法。</p><hr><ul><li>输入： n个1024维人脸样本数据所构成的矩阵X，降维后的维数l</li><li>输出：映射矩阵$W = {w_1,w_2,…,w_l}$ (其中每个$w_j(1≤ j ≤ l)$是一个特征人脸</li><li>算法步骤</li></ul><hr><ol><li>对于每个人脸样本数据$x<em>i$进行中心化处理： $x_i = x_i -\mu$,$\mu=\frac{1}{n}\sum</em>{j=1}^nx_j$</li><li>计算原始人脸样本数据的协方差矩阵:$\sum = \frac{1}{n-1}X^TX$</li><li>对协方差矩阵?进行特征值分解，对所得特征根从到小排序$\lambda_1 ≥ \lambda_2, ≥ ⋯ ≥ \lambda_d$</li><li>取前l个最大特征根所对应特征向量$w_1, w_2, … ,w_l$组成映射矩阵W</li><li>将每个人脸图像$x<em>i$按照如下方法降维:$(x_i)</em>{1\times d}(W)_{d\times l} = 1\times l$</li></ol><blockquote><p>每个人脸特征向量$w_i$与原始人脸数据$x_i$的维数是一样的，均为1024。<br>同时我们也可以将可将每个特征向量还原为$32×32$的人脸图像，称之为特征人脸，因此可得到l个特征人脸。</p></blockquote><p>将每幅人脸分别与每个特征人脸做矩阵乘法，得到一个相关系数向量，包含l个系数，这个向量表示我们可以用l个特征人脸的线性组合表示任意一个人脸数据。<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210527112508.png" alt=""></p><p>对于任意一个未知人脸，我们都可以得到其降维后的向量，然后将这个向量和数据库中的所有人脸向量进行对比，找到最接近的向量，从而进行分类</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210527112436.png" alt=""></p><h2 id="代码内容"><a href="#代码内容" class="headerlink" title="代码内容"></a>代码内容</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>本次实验采用的数据集来及剑桥AT&amp;T Lab的人脸数据库<a href="http://cam-orl.co.uk/facedatabase.html">ORL</a><br>，整个数据集包含来自 40 名志愿者的 280 张照片，每名志愿者的照片都包含 7 个不同的表情和角度。</p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>数据预处理主要包括划分训练集和测试集，并将所有图片squeeze为一维</p><pre><code>def spilt_data(nPerson, nPicture, data, label):    &quot;&quot;&quot;    分割数据集    :param nPerson : 志愿者数量    :param nPicture: 各志愿者选入训练集的照片数量    :param data : 等待分割的数据集    :param label: 对应数据集的标签    :return: 训练集, 训练集标签, 测试集, 测试集标签    &quot;&quot;&quot;    # 数据集大小和意义    allPerson, allPicture, rows, cols = data.shape    # 划分训练集和测试集    train = data[:nPerson, :nPicture, :, :].reshape(nPerson * nPicture, rows * cols)    train_label = label[:nPerson, :nPicture].reshape(nPerson * nPicture)    test = data[:nPerson, nPicture:, :, :].reshape(nPerson * (allPicture - nPicture), rows * cols)    test_label = label[:nPerson, nPicture:].reshape(nPerson * (allPicture - nPicture))    # 返回: 训练集, 训练集标签, 测试集, 测试集标签    return train, train_label, test, test_labeldatapath = &#39;./ORL.npz&#39;ORL = np.load(datapath)data = ORL[&#39;data&#39;]label = ORL[&#39;label&#39;]num_eigenface = 200train_vectors, train_labels, test_vectors, test_labels = spilt_data(40, 5, data,label)train_vectors = train_vectors / 255test_vectors = test_vectors / 255print(&quot;训练数据集:&quot;, train_vectors.shape)print(&quot;测试数据集:&quot;, test_vectors.shape)</code></pre><h3 id="训练特征脸（eigenface）算法的实现"><a href="#训练特征脸（eigenface）算法的实现" class="headerlink" title="训练特征脸（eigenface）算法的实现"></a>训练特征脸（eigenface）算法的实现</h3><p>eigenface算法的输入是人脸数据训练集和希望提取的主特征数，算法先根据测试数据求出平均脸，然后计算训练数据里每张脸与平均脸的差异，求差异矩阵的特征值和特征向量，取前 K 个特征向量，计算出 K 张特征脸，然后就可以利用这 K 个特征脸对测试人脸进行识别了。</p><pre><code>def eigen_train(trainset, k=20):    &quot;&quot;&quot;    训练特征脸（eigenface）算法的实现    :param trainset: 使用 get_images 函数得到的处理好的人脸数据训练集    :param K: 希望提取的主特征数    :return: 训练数据的平均脸, 特征脸向量, 中心化训练数据    &quot;&quot;&quot;                                                                 #    trainset = np.array(trainset)    # 计算平均脸    avg_img = trainset.mean(axis=0)    # 计算中心化人脸    norm_img = trainset - avg_img    # 求协方差矩阵    cov_img = np.cov(norm_img.T)    # 求特征值和特征向量    eig_val, eig_vec = np.linalg.eig(cov_img)    # 选取最大的K个特征向量    feature = eig_vec[:, :-k-1:-1]    # 返回：平均人脸、特征人脸、中心化人脸    return avg_img, feature, norm_img# 返回平均人脸、特征人脸、中心化人脸avg_img, eigenface_vects, trainset_vects = eigen_train(train_vectors, num_eigenface)</code></pre><p>求特征值和特征向量可以直接调用numpy的<code>np.linalg.eig</code>函数，可以得到特征值和特征向量，其中得到的特征值在实数域按从小到大排列，所以在取k个最大特征值时可以直接用<code>eig_vec[:, :-k-1:-1]</code>取向量后k个向量</p><h3 id="人脸识别模型"><a href="#人脸识别模型" class="headerlink" title="人脸识别模型"></a>人脸识别模型</h3><p>接下来，我们使用上面得到的模型在测试集上进行测试。<br>然后使用特征脸（Eigenface）算法对测试集中的人脸照片进行预测，我们在这里定义了 <code>rep_face</code> 函数，其输入是测试数据, 训练集的平均人脸数据，特征脸向量, 选用的特征脸数量。</p><pre><code>def rep_face(image, avg_img, eigenface_vects, numComponents=0):    &quot;&quot;&quot;    用特征脸（eigenface）算法对输入数据进行投影映射，得到使用特征脸向量表示的数据    :param image: 输入数据    :param avg_img: 训练集的平均人脸数据    :param eigenface_vects: 特征脸向量    :param numComponents: 选用的特征脸数量    :return: 输入数据的特征向量表示, 最终使用的特征脸数量    &quot;&quot;&quot;    representation = np.dot(image, eigenface_vects)    numEigenFaces = np.count_nonzero(representation)    # 返回：输入数据的特征向量表示, 特征脸使用数量    return representation, numEigenFacestrain_reps = []for img in train_vectors:    train_rep, _ = rep_face(img, avg_img, eigenface_vects, num_eigenface)    train_reps.append(train_rep)num = 0for idx, image in enumerate(test_vectors):    label = test_labels[idx]    test_rep, _ = rep_face(image, avg_img, eigenface_vects, num_eigenface)    results = []    for train_rep in train_reps:        similarity = np.sum(np.square(train_rep - test_rep))        results.append(similarity)    results = np.array(results)    if label == np.argmin(results) // 5 + 1:        num = num + 1print(&quot;人脸识别准确率: &#123;&#125;%&quot;.format(num / 80 * 100))</code></pre><h3 id="人脸重建"><a href="#人脸重建" class="headerlink" title="人脸重建"></a>人脸重建</h3><pre><code>def recFace(representations, avg_img, eigenVectors, numComponents, sz=(112, 92)):    &quot;&quot;&quot;    利用特征人脸重建原始人脸    :param representations: 表征数据    :param avg_img: 训练集的平均人脸数据    :param eigenface_vects: 特征脸向量    :param numComponents: 选用的特征脸数量    :param sz: 原始图片大小    :return: 重建人脸, str 使用的特征人脸数量    &quot;&quot;&quot;    d = eigenVectors.shape[0]    l = eigenVectors.shape[1]    face = np.zeros(d)    for i in range(l):        face = face + representations[i] * eigenVectors[:, i]    face.reshape(sz[0], sz[1])    # 返回: 重建人脸, str 使用的特征人脸数量    return face, &#39;numEigenFaces_&#123;&#125;&#39;.format(numComponents)print(&quot;重建训练集人脸&quot;)# 读取train数据image = train_vectors[100]faces = []names = []# 选用不同数量的特征人脸重建人脸for i in range(20, 200, 20):    representations, numEigenFaces = rep_face(image, avg_img, eigenface_vects, i)    face, name = recFace(representations, avg_img, eigenface_vects, numEigenFaces)    faces.append(face)    names.append(name)plot_gallery(faces, names, n_row=3, n_col=3)print(&quot;-&quot;*55)print(&quot;重建测试集人脸&quot;)# 读取test数据image = test_vectors[54]faces = []names = []# 选用不同数量的特征人脸重建人脸for i in range(20, 200, 20):    representations, numEigenFaces = rep_face(image, avg_img, eigenface_vects, i)    face, name = recFace(representations, avg_img, eigenface_vects, numEigenFaces)    faces.append(face)    names.append(name)plot_gallery(faces, names, n_row=3, n_col=3)</code></pre><h3 id="其他函数"><a href="#其他函数" class="headerlink" title="其他函数"></a>其他函数</h3><pre><code>def show_img(img, h=112, w=92):    &quot;&quot;&quot;    展示单张图片    :param img: numpy array 格式的图片    :return:    &quot;&quot;&quot;    # 展示图片    plt.imshow(img.reshape(h, w), &#39;gray&#39;)    plt.axis(&#39;off&#39;)    plt.show()def plot_gallery(images, titles, n_row=3, n_col=5, h=112, w=92):  # 3行4列    &quot;&quot;&quot;    展示多张图片    :param images: numpy array 格式的图片    :param titles: 图片标题    :param h: 图像reshape的高    :param w: 图像reshape的宽    :param n_row: 展示行数    :param n_col: 展示列数    :return:    &quot;&quot;&quot;    # 展示图片    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)    for i in range(n_row * n_col):        plt.subplot(n_row, n_col, i + 1)        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)        plt.title(titles[i], size=12)        plt.xticks(())        plt.yticks(())    plt.show()def letterbox_image(image, size):    &quot;&quot;&quot;    调整图片尺寸    :param image: 用于训练的图片    :param size: 需要调整到网络输入的图片尺寸    :return: 返回经过调整的图片    &quot;&quot;&quot;    new_image = cv2.resize(image, size, interpolation=cv2.INTER_AREA)    return new_imagedef read_one_img(path):    &quot;&quot;&quot;    根据路径读取一张人脸图片    :param path: 图片的路径    :return:    &quot;&quot;&quot;    # 图片路径    # 以灰度模式读取图片    img_sample = Image.open(path).convert(&#39;L&#39;)    # 把图片格式转为 numpy array 格式    img_sample = np.array(img_sample, &#39;uint8&#39;)    return img_sampledef get_images(path):    &quot;&quot;&quot;    读取输入的文件夹路径下的所有照片，读取输入的文件夹路径下的所有照片，将其转为 1 维，    统一保存到一个矩阵中，然依据图片名提取标签，最终该函数将输出这个照片矩阵及其中每    张照片的标签。    照片的命名格式请参照&quot;person41_01.png&quot;, 其含义为第41位志愿者的第01张人脸图像。    :param path: 照片存放的文件夹路径    :return: numpy matrix 格式的处理好的图片，及 list 格式的各个图片的标签    &quot;&quot;&quot;    # 首先获取所有人脸图片的路径    image_paths = [os.path.join(path, f) for f in os.listdir(path) if                   f.endswith(&#39;png&#39;)]    # 所有的人脸区域图片都将保存在 images 这个矩阵中    images = np.mat(np.zeros((len(image_paths), 112 * 92)))    trainset_labels = []    # 对于每一张图片    for index, image_path in enumerate(image_paths):        # 读取图片并将其转为灰度图        image_pil = Image.open(image_path).convert(&#39;L&#39;)        # 把图片转为 numpy array 格式        image = np.array(image_pil, &#39;uint8&#39;)        image = letterbox_image(image=image, size=(112, 92))        # 把 2 维的平面图像转为 1 维        img_1D = image.flatten()        # 把处理后的图片保存到 images 中        images[index, :] = img_1D        # 提取图片名作为图片的标签        trainset_labels.append(int(image_path.split(&#39;.&#39;)[-2][-2:]))    # 得到最终处理好的人脸图片和各个图片的标签    trainset_labels = np.array(trainset_labels)    return images, trainset_labels</code></pre><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><li><a href="https://watermark.silverchair.com/jocn.1991.3.1.71.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAokwggKFBgkqhkiG9w0BBwagggJ2MIICcgIBADCCAmsGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMyuYvhfoK0obOZuFSAgEQgIICPIsXUpJB7tf0FEiQ_w94NO5u_rznkTcV2-Gmpjlp-YyTygD_RcOQVFxtq3gWOZm1nPJL_6b3I4BF25ZHxTCOKAZGK9CR1rk-EzK-pT2-q5lj1nkvMX76qACpDxEq-ajQUPbO9vYeso1gRJx43JejZth0xifFUpaj5ss1-lH1-pKJNPrtTHZ6qkaaB63BPI4or8jf8RoCTL2clv0IKJEkHAZ-L1MJJ2bi1EkdHWSrpnYcK7b_D6N2ZP70yGKkFERFIEjAkY4jtOClVkjzfQL0n5Aa3-Y84tjFO9LwLbuLFCPu4khN4k8SDpBSpvjqi-Il6cGkhi_AA20PgE5g5t9HwA6v_NtSJcNtP9bOpNF6ebLjSeZKjbcplWevJe8dnOznCtk-f4YPmug39qrGbW6CQWaGE0s8rlhfxQFaxaVkq0VE3boWWbZIbAoB1BGAZe1peQSSsaY_hxUwDa6QR8KhHmKyUlG_LVqIoGzVoFArOc3xQHaWybcS-ENcRmTal6xOePTrxXvYVjowEqVgCbTPxYtnu-ZCu-5IHgfvvczlZ65y5UWnRNPVQUoY0c_69FFsjqJeZAPhA60r5ODE6ZlN1kUZSedS8mjoQn82AvXACsfpCOgNnZjt32_BMlc7QTngrdTfaWG827d5uBM1ru0B5vA-LviFZYZvJfVR3C9wwx3dtPwyOl4ltiPvbauUleiV1cQbSmQ7ODGolTCne--J2w-iOhFgqNbNUyBtbwU0yp-3crO8eMDngRcsIVqk/">Eigenfaces for Recognition</a></li><li><a href="https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E8%84%B8">特征脸-wiki</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PCA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>蒙特卡洛搜索实现黑白棋</title>
      <link href="2021/05/10/black-white-cheese/"/>
      <url>2021/05/10/black-white-cheese/</url>
      
        <content type="html"><![CDATA[<h1 id="问题重述"><a href="#问题重述" class="headerlink" title="问题重述"></a>问题重述</h1><h2 id="黑白棋简介"><a href="#黑白棋简介" class="headerlink" title="黑白棋简介"></a>黑白棋简介</h2><p>黑白棋(Reversi),也叫翻转棋，是一款经典的策略游戏。</p><p>一般棋子双面为黑白两色，故称“黑白棋”。因为行棋之时将对方棋子翻转，则变为己方棋子，故又称“翻转棋” (Reversi) 。</p><p>它使用 8x8 的棋盘，由两人执黑子和白子轮流下棋，最后子多方为胜方。</p><h3 id="游戏规则"><a href="#游戏规则" class="headerlink" title="游戏规则"></a>游戏规则</h3><p>开始时，黑棋位于D5和E4,白棋位于D4和E5.<br>一个合法的落子：</p><ul><li>在空处落子、并翻转对手一个或多个棋子</li><li>新落子位置必须在可以夹住对方的位置上、对方被夹住的棋子翻转。可以是横着夹、竖着夹、对角线夹</li><li>任何被夹住的棋子必须被反过来<br>如果一方没有合法棋步，也就是无论他下在哪里，都无法翻转对方的棋子了，这一轮只能弃权<br>棋局持续知道棋盘填满或双方都没有合法棋步可下<br>如果一方落子时间超过1min，或者连续三次落子不合法，则判断该方失败</li></ul><h2 id="棋盘类介绍"><a href="#棋盘类介绍" class="headerlink" title="棋盘类介绍"></a>棋盘类介绍</h2><h3 id="初始化棋盘"><a href="#初始化棋盘" class="headerlink" title="初始化棋盘"></a>初始化棋盘</h3><p>棋盘规格是 8x8，’X’ 代表黑棋，’O’ 代表白棋，’.’ 代表未落子状态。</p><p>棋盘初始化 - 利用 Board 类（board.py）中的 <code>display()</code> 方法展示棋盘：</p><h3 id="导入棋盘文件"><a href="#导入棋盘文件" class="headerlink" title="导入棋盘文件"></a>导入棋盘文件</h3><pre><code>from board import Board</code></pre><h4 id="初始化棋盘-1"><a href="#初始化棋盘-1" class="headerlink" title="初始化棋盘"></a>初始化棋盘</h4><p>board = Board()</p><h1 id="打印初始化棋盘"><a href="#打印初始化棋盘" class="headerlink" title="打印初始化棋盘"></a>打印初始化棋盘</h1><pre><code>board.display()</code></pre><h3 id="棋盘与坐标之间的关系"><a href="#棋盘与坐标之间的关系" class="headerlink" title="棋盘与坐标之间的关系"></a>棋盘与坐标之间的关系</h3><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">A</th><th style="text-align:center">B</th><th style="text-align:center">C</th><th style="text-align:center">D</th><th style="text-align:center">E</th><th style="text-align:center">F</th><th style="text-align:center">G</th><th style="text-align:center">H</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">(0,0)</td><td style="text-align:center">(0,1)</td><td style="text-align:center">(0,2)</td><td style="text-align:center">(0,3)</td><td style="text-align:center">(0,4)</td><td style="text-align:center">(0,5)</td><td style="text-align:center">(0,6)</td><td style="text-align:center">(0,7)</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">(1,0)</td><td style="text-align:center">(1,1)</td><td style="text-align:center">(1,2)</td><td style="text-align:center">(1,3)</td><td style="text-align:center">(1,4)</td><td style="text-align:center">(1,5)</td><td style="text-align:center">(1,6)</td><td style="text-align:center">(1,7)</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">(2,0)</td><td style="text-align:center">(2,1)</td><td style="text-align:center">(2,2)</td><td style="text-align:center">(2,3)</td><td style="text-align:center">(2,4)</td><td style="text-align:center">(2,5)</td><td style="text-align:center">(2,6)</td><td style="text-align:center">(2,7)</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">(3,0)</td><td style="text-align:center">(3,1)</td><td style="text-align:center">(3,2)</td><td style="text-align:center">(3,3)</td><td style="text-align:center">(3,4)</td><td style="text-align:center">(3,5)</td><td style="text-align:center">(3,6)</td><td style="text-align:center">(3,7)</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">(4,0)</td><td style="text-align:center">(4,1)</td><td style="text-align:center">(4,2)</td><td style="text-align:center">(4,3)</td><td style="text-align:center">(4,4)</td><td style="text-align:center">(4,5)</td><td style="text-align:center">(4,6)</td><td style="text-align:center">(4,7)</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">(5,0)</td><td style="text-align:center">(5,1)</td><td style="text-align:center">(5,2)</td><td style="text-align:center">(5,3)</td><td style="text-align:center">(5,4)</td><td style="text-align:center">(5,5)</td><td style="text-align:center">(5,6)</td><td style="text-align:center">(5,7)</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">(6,0)</td><td style="text-align:center">(6,1)</td><td style="text-align:center">(6,2)</td><td style="text-align:center">(6,3)</td><td style="text-align:center">(6,4)</td><td style="text-align:center">(6,5)</td><td style="text-align:center">(6,6)</td><td style="text-align:center">(6,7)</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">(7,0)</td><td style="text-align:center">(7,1)</td><td style="text-align:center">(7,2)</td><td style="text-align:center">(7,3)</td><td style="text-align:center">(7,4)</td><td style="text-align:center">(7,5)</td><td style="text-align:center">(7,6)</td><td style="text-align:center">(7,7)</td></tr></tbody></table></div><p>棋盘坐标 E4, 转化为坐标形式就是 (3, 4), 坐标数值大小是从 0 开始，到 7 结束。  </p><p>Board 类中，提供以上两种坐标的转化方法：</p><ul><li><code>board_num(action)</code>: 棋盘坐标转化为数字坐标。<ul><li>action: 棋盘坐标，e.g. ‘G6’</li><li>返回值: 数字坐标，e.g. (5, 6)</li></ul></li><li><code>num_board(action)</code>: 数字坐标转化为棋盘坐标。<ul><li>action: 数字坐标，e.g. (2, 7)</li><li>返回值: 棋盘坐标，e.g. ‘H3’</li></ul></li></ul><h3 id="查看坐标-4-3-在棋盘上的位置"><a href="#查看坐标-4-3-在棋盘上的位置" class="headerlink" title="查看坐标 (4,3) 在棋盘上的位置"></a>查看坐标 (4,3) 在棋盘上的位置</h3><pre><code>position = (4, 3)print(board.num_board(position))</code></pre><h3 id="查看棋盘位置-‘G2’-的坐标"><a href="#查看棋盘位置-‘G2’-的坐标" class="headerlink" title="查看棋盘位置 ‘G2’ 的坐标"></a>查看棋盘位置 ‘G2’ 的坐标</h3><pre><code>position = &#39;G2&#39;print(board.board_num(position))</code></pre><h3 id="Board-类中比较重要的方法"><a href="#Board-类中比较重要的方法" class="headerlink" title="Board 类中比较重要的方法"></a>Board 类中比较重要的方法</h3><ul><li><code>get_legal_actions(color)</code>： 根据黑白棋的规则获取 color 方棋子的合法落子坐标，用 <code>list()</code> 方法可以获取所有的合法坐标。<ul><li>color: 下棋方，’X’ - 黑棋，’O’ - 白棋</li><li>返回值: 合法的落子坐标列表  </li></ul></li></ul><h3 id="棋盘初始化后，黑方可以落子的位置"><a href="#棋盘初始化后，黑方可以落子的位置" class="headerlink" title="棋盘初始化后，黑方可以落子的位置"></a>棋盘初始化后，黑方可以落子的位置</h3><pre><code>print(list(board.get_legal_actions(&#39;X&#39;)))</code></pre><ul><li><code>_move(action, color)</code>：  根据 color 落子坐标 action 获取翻转棋子的坐标。  <ul><li>action: 落子的坐标，e.g. ‘C4’</li><li>color: 下棋方，’X’ - 黑棋，’O’ - 白棋</li><li>返回值: 反转棋子棋盘坐标列表</li></ul></li></ul><h3 id="打印初始化后的棋盘"><a href="#打印初始化后的棋盘" class="headerlink" title="打印初始化后的棋盘"></a>打印初始化后的棋盘</h3><pre><code>board.display()</code></pre><h4 id="假设现在黑棋下棋，可以落子的位置有：-‘D3’-‘C4’-‘F5’-‘E6’-，"><a href="#假设现在黑棋下棋，可以落子的位置有：-‘D3’-‘C4’-‘F5’-‘E6’-，" class="headerlink" title="假设现在黑棋下棋，可以落子的位置有：[‘D3’, ‘C4’, ‘F5’, ‘E6’]，"></a>假设现在黑棋下棋，可以落子的位置有：[‘D3’, ‘C4’, ‘F5’, ‘E6’]，</h4><h4 id="黑棋落子-D3-则白棋被翻转的棋子是-D4。"><a href="#黑棋落子-D3-则白棋被翻转的棋子是-D4。" class="headerlink" title="黑棋落子 D3 , 则白棋被翻转的棋子是 D4。"></a>黑棋落子 D3 , 则白棋被翻转的棋子是 D4。</h4><h4 id="表示黑棋"><a href="#表示黑棋" class="headerlink" title="表示黑棋"></a>表示黑棋</h4><pre><code>color = &#39;X&#39; </code></pre><h4 id="落子坐标"><a href="#落子坐标" class="headerlink" title="落子坐标"></a>落子坐标</h4><pre><code>action = &#39;D3&#39; </code></pre><h3 id="打印白方被翻转的棋子位置"><a href="#打印白方被翻转的棋子位置" class="headerlink" title="打印白方被翻转的棋子位置"></a>打印白方被翻转的棋子位置</h3><pre><code>print(board._move(action,color))</code></pre><h3 id="打印棋盘"><a href="#打印棋盘" class="headerlink" title="打印棋盘"></a>打印棋盘</h3><pre><code>board.display()</code></pre><h2 id="Game-类"><a href="#Game-类" class="headerlink" title="Game 类"></a>Game 类</h2><p>该类主要实现黑白棋的对弈，已经实现随机玩家和人类玩家，现在可以来对弈一下。<br>Game 类（game.py）的主要方法和属性:  </p><ul><li>属性：<ul><li><code>self.board</code>：棋盘</li><li><code>self.current_player</code>：定义当前的下棋一方，考虑游戏还未开始我们定义为 None</li><li><code>self.black_player</code>：定义黑棋玩家 black_player</li><li><code>self.white_player</code>：定义白棋玩家 white_player</li></ul></li></ul><ul><li>方法：   <ul><li><code>switch_player()</code>：下棋时切换玩家  </li><li><code>run()</code>：黑白棋游戏的主程序  </li></ul></li></ul><h1 id="设计思想"><a href="#设计思想" class="headerlink" title="设计思想"></a>设计思想</h1><h2 id="蒙特卡洛方法-Monte-Carlo-method"><a href="#蒙特卡洛方法-Monte-Carlo-method" class="headerlink" title="蒙特卡洛方法(Monte Carlo method)"></a>蒙特卡洛方法(Monte Carlo method)</h2><p>MCTS是一种“统计模拟方法”。20世纪40年代，为建造核武器，冯.诺伊曼等人发明了该算法。因赌城蒙特卡洛而得名，暗示其以概率作为算法的基础。<br>假设我们要计算一个不规则形状的面积，我们只需在包含这个不规则形状的矩形内，随机的掷出一个点，每掷出一个点，则N+1，如果这个点在不规则图形内则W+1。落入不规则图形的概率即为 W/N。当掷出足够多的点之后，我们可以认为：不规则图形面积＝矩形面积＊W/N。</p><h2 id="UCT（信心上限树算法）"><a href="#UCT（信心上限树算法）" class="headerlink" title="UCT（信心上限树算法）"></a>UCT（信心上限树算法）</h2><p>UCT 算法是蒙特卡洛方法的一种改进，比蒙特卡洛方法更容易得 到最优解，其基本结构和蒙特卡洛方法相同，主要分为四个步骤：选 择 (Selection) ， 拓 展 (Expansion) ， 模 拟 (Simulation) 和 反 向 传 播 (Backpropagation)</p><p><strong>其价值函数定义为</strong></p><script type="math/tex; mode=display">value = child.reward / child.visits + c\sqrt{\frac{2In(node.visits)}{child.visits}}</script><p><img src="https://pic4.zhimg.com/80/v2-16cafcfda07f07733d2a2326500b6bd7_720w.jpg" alt="image"></p><h3 id="选择-Selection"><a href="#选择-Selection" class="headerlink" title="选择(Selection)"></a>选择(Selection)</h3><p>在选择阶段，需要从根节点，也就是要做决策的局面R出发向下选择出一个最急迫需要被拓展的节点N，局面R是是每一次迭代中第一个被检查的节点；对于被检查的局面而言，他可能有三种可能：</p><ul><li>该节点所有可行动作都已经被拓展过</li><li>该节点有可行动作还未被拓展过</li><li>这个节点游戏已经结束了(例如已经连成五子的五子棋局面)</li></ul><p>对于这三种可能：</p><ul><li>如果所有可行动作都已经被拓展过了，那么我们将使用UCB公式计算该节点所有子节点的UCB值，并找到值最大的一个子节点继续检查。反复向下迭代。</li><li>如果被检查的局面依然存在没有被拓展的子节点(例如说某节点有20个可行动作，但是在搜索树中才创建了19个子节点)，那么我们认为这个节点就是本次迭代的的目标节点N，并找出N还未被拓展的动作A。执行步骤[2]</li><li>如果被检查到的节点是一个游戏已经结束的节点。那么从该节点直接执行步骤{4]。<br>每一个被检查的节点的被访问次数在这个阶段都会自增。在反复的迭代之后，我们将在搜索树的底端找到一个节点，来继续后面的步骤。</li></ul><h3 id="拓展-Expansion"><a href="#拓展-Expansion" class="headerlink" title="拓展(Expansion)"></a>拓展(Expansion)</h3><p>在选择阶段结束时候，我们查找到了一个最迫切被拓展的节点N，以及他一个尚未拓展的动作A。在搜索树中创建一个新的节点Nn作为N的一个新子节点。Nn的局面就是节点N在执行了动作A之后的局面。</p><h3 id="模拟-Simulation"><a href="#模拟-Simulation" class="headerlink" title="模拟(Simulation)"></a>模拟(Simulation)</h3><p>为了让Nn得到一个初始的评分。我们从Nn开始，让游戏随机进行，直到得到一个游戏结局，这个结局将作为Nn的初始评分。一般使用胜利/失败来作为评分，只有1或者0。</p><h3 id="反向传播-Back-Propagation"><a href="#反向传播-Back-Propagation" class="headerlink" title="反向传播(Back Propagation)"></a>反向传播(Back Propagation)</h3><p>在Nn的模拟结束之后，它的父节点N以及从根节点到N的路径上的所有节点都会根据本次模拟的结果来添加自己的累计评分。如果在[1]的选择中直接发现了一个游戏结局的话，根据该结局来更新评分。每一次迭代都会拓展搜索树，随着迭代次数的增加，搜索树的规模也不断增加。当到了一定的迭代次数或者时间之后结束，选择根节点下最好的子节点作为本次决策的结果。</p><h2 id="MCT伪代码"><a href="#MCT伪代码" class="headerlink" title="MCT伪代码"></a>MCT伪代码</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210510192520.png" alt=""></p><h1 id="代码内容"><a href="#代码内容" class="headerlink" title="代码内容"></a>代码内容</h1><h2 id="节点类的定义"><a href="#节点类的定义" class="headerlink" title="节点类的定义"></a>节点类的定义</h2><pre><code>class Node:    def __init__(self, state, parent=None, action=None, color=&quot;X&quot;):        self.visits = 0  #访问次数        self.reward = 0.0 #期望值        self.state = state #棋盘状态，Broad类        self.children = [] #子节点        self.parent = parent #父节点        self.action = action #从父节点转移到本节点采取的动作        self.color = color #该节点玩家颜色    # 增加子节点    def add_child(self, child_state, action, color):        child_node = Node(child_state, parent=self, action=action, color=color)        self.children.append(child_node)    # 判断是否完全展开    def fully_expanded(self):        action = list(self.state.get_legal_actions(self.color))        if len(self.children) == len(action):            return True        return False</code></pre><h2 id="AI玩家类的定义"><a href="#AI玩家类的定义" class="headerlink" title="AI玩家类的定义"></a>AI玩家类的定义</h2><pre><code>class AIPlayer:    &quot;&quot;&quot;    AI 玩家    &quot;&quot;&quot;    step = 0    def __init__(self, color):        &quot;&quot;&quot;        玩家初始化        :param color: 下棋方，&#39;X&#39; - 黑棋，&#39;O&#39; - 白棋        &quot;&quot;&quot;        # 最大迭代次数        self.max_times = 100        # 玩家颜色        self.color = color        # UCB超参数        self.SCALAR = 1</code></pre><h3 id="移动棋子的方法"><a href="#移动棋子的方法" class="headerlink" title="移动棋子的方法"></a>移动棋子的方法</h3><pre><code>def get_move(self, board):    &quot;&quot;&quot;    根据当前棋盘状态获取最佳落子位置    :param board: 棋盘    :return: action 最佳落子位置, e.g. &#39;A1&#39;    &quot;&quot;&quot;    if self.color == &#39;X&#39;:        player_name = &#39;黑棋&#39;    else:        player_name = &#39;白棋&#39;    print(&quot;请等一会，对方 &#123;&#125;-&#123;&#125; 正在思考中...&quot;.format(player_name, self.color))    board_state = copy.deepcopy(board)    root = Node(state=board_state, color=self.color)    action = self.uct_search(self.max_times, root)    return action</code></pre><h3 id="UCT-search的核心框架"><a href="#UCT-search的核心框架" class="headerlink" title="UCT search的核心框架"></a>UCT search的核心框架</h3><pre><code>def uct_search(self, max_times, root):    &quot;&quot;&quot;    根据当前棋盘状态获取最佳落子位置    :param max_times: 最大搜索次数，默认100    :param root: 根节点    :return: action 最佳落子位置, e.g. &#39;A1&#39;    &quot;&quot;&quot;    for t in range(max_times):        # print(t)        leave = self.select_policy(root)        reward = self.stimulate_policy(leave)        self.backup(leave, reward)        best_child = self.ucb(root, 0)    return best_child.action</code></pre><h3 id="选择扩展的节点"><a href="#选择扩展的节点" class="headerlink" title="选择扩展的节点"></a>选择扩展的节点</h3><p>在这里对UCT的算法进行了改进，鼓励优先考虑目前期望值较大的节点，有0.5的概率在当前节点存在可扩展节点时选择不扩展</p><pre><code>def select_policy(self, node):    &quot;&quot;&quot;    选择扩展的节点    :param node: 根节点，Node 类    :return: leave，Node 类    &quot;&quot;&quot;    while not self.terminal(node.state):        #node.state.display()        #print(list(node.state.get_legal_actions(node.color)))        if len(node.children) == 0:            new_node = self.expand(node)            #print(new_node.action)            return new_node        elif random.uniform(0, 1) &lt; .5:            node = self.ucb(node, self.SCALAR)        else:            node = self.ucb(node, self.SCALAR)            if not node.fully_expanded():                return self.expand(node)            else:                node = self.ucb(node, self.SCALAR)    return node</code></pre><h3 id="扩展函数"><a href="#扩展函数" class="headerlink" title="扩展函数"></a>扩展函数</h3><pre><code>def expand(self, node):    &quot;&quot;&quot;    选择扩展的节点    :param node: 根节点，Node 类    :return: leave，Node 类    &quot;&quot;&quot;    # 随机选择动作    action_list = list(node.state.get_legal_actions(node.color))    # 防止尾盘时出现卡死，没有动作可以选择    if len(action_list) == 0:        return node.parent    action = random.choice(action_list)    tried_action = [c.action for c in node.children]    while action in tried_action:        action = random.choice(action_list)    # 复制状态并根据动作更新到新状态    new_state = copy.deepcopy(node.state)    new_state._move(action, node.color)    # 确定子节点颜色    if node.color == &#39;X&#39;:        new_color = &#39;O&#39;    else:        new_color = &#39;X&#39;    # 新建节点    node.add_child(new_state, action=action, color=new_color)    return node.children[-1]</code></pre><h3 id="ucb选择函数"><a href="#ucb选择函数" class="headerlink" title="ucb选择函数"></a>ucb选择函数</h3><pre><code>def ucb(self, node, scalar):    &quot;&quot;&quot;    选择最佳子节点    :param node: 节点，Node 类    :param scalar: UCT公式超参数    :return: best_child:最佳子节点，Node 类    &quot;&quot;&quot;    best_score = -float(&#39;inf&#39;)    best_children = []    for c in node.children:        exploit = c.reward / c.visits        if c.visits == 0:            best_children = [c]            break        explore = math.sqrt(2.0 * math.log(node.visits) / float(c.visits))        score = exploit + scalar * explore        if score == best_score:            best_children.append(c)        if score &gt; best_score:            best_children = [c]            best_score = score    if len(best_children) == 0:        return node.parent    return random.choice(best_children)</code></pre><h3 id="随机模拟对弈"><a href="#随机模拟对弈" class="headerlink" title="随机模拟对弈"></a>随机模拟对弈</h3><p>在定义期望值时同时考虑了胜负关系和获胜的子数，board.get_winner()会返回胜负关系和获胜子数<br>在这里我们定义获胜积100分，每多赢一个棋子多1分</p><script type="math/tex; mode=display">reward = 100 + difference</script><pre><code>def stimulate_policy(self, node):    &quot;&quot;&quot;    随机模拟对弈    :param node: 节点，Node 类    :return: reward:期望值    在定义期望值时同时考虑了胜负关系和获胜的子数，board.get_winner()会返回胜负关系和获胜子数    在这里我们定义获胜积100分，每多赢一个棋子多1分    reward = 100 + difference    &quot;&quot;&quot;    board = copy.deepcopy(node.state)    color = node.color    count = 0    while not self.terminal(board):        action_list = list(node.state.get_legal_actions(color))        if not len(action_list) == 0:            action = random.choice(action_list)            board._move(action, color)            if color == &#39;X&#39;:                color = &#39;O&#39;            else:                color = &#39;X&#39;        else:            if color == &#39;X&#39;:                color = &#39;O&#39;            else:                color = &#39;X&#39;            action_list = list(node.state.get_legal_actions(color))            action = random.choice(action_list)            board._move(action, color)            if color == &#39;X&#39;:                color = &#39;O&#39;            else:                color = &#39;X&#39;        count = count + 1        if count &gt;= 10:            break    # 价值函数定义    winner, difference = board.get_winner()    if winner == 2:        reward = 0    elif winner == 1:        reward = 100 + difference    else:        reward = -(100 + difference)    if self.color == &#39;X&#39;:        reward = - reward    return reward</code></pre><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><pre><code>def backup(self, node, reward):    while node is not None:        node.visits += 1        if node.parent.color == self.color:            node.reward += reward        else:            node.reward -= reward        node = node.parent    return 0</code></pre><h3 id="判断游戏是否结束"><a href="#判断游戏是否结束" class="headerlink" title="判断游戏是否结束"></a>判断游戏是否结束</h3><pre><code>def terminal(self, state):    &quot;&quot;&quot;    判断游戏是否结束    :return: True/False 游戏结束/游戏没有结束    &quot;&quot;&quot;    # 根据当前棋盘，判断棋局是否终止    # 如果当前选手没有合法下棋的位子，则切换选手；如果另外一个选手也没有合法的下棋位置，则比赛停止。    b_list = list(state.get_legal_actions(&#39;X&#39;))    w_list = list(state.get_legal_actions(&#39;O&#39;))    is_over = len(b_list) == 0 and len(w_list) == 0  # 返回值 True/False    return is_over</code></pre><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><h2 id="初级"><a href="#初级" class="headerlink" title="初级"></a>初级</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210510215115.png" alt=""></p><h2 id="中级"><a href="#中级" class="headerlink" title="中级"></a>中级</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210510215143.png" alt=""></p><h2 id="高级"><a href="#高级" class="headerlink" title="高级"></a>高级</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210510215134.png" alt=""></p><p>在实际实验过程中基本可以碾压对手</p><p><a href="https://github.com/Fazziekey/AI_project/tree/main/black-white-cheese">项目地址</a></p>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Monte Carlo search </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法岗白给面经整理</title>
      <link href="2021/05/01/experience-of-meeting/"/>
      <url>2021/05/01/experience-of-meeting/</url>
      
        <content type="html"><![CDATA[<p>寒假时开始考虑要不要去找暑期实习，因为今年去找暑研面临着比较大的不确定性，而且对于找暑研，寒假显然已经不算早了，几番思想斗争后就明确放弃找暑研，选择去找份实习，不过寒假实在是太摸鱼，leetcode也没写几道，科研也动的不多，倒是文明6可以全文明神标200T内飞天了。</p><hr><p>三月初开学其实已经有不少同学拿到日常实习的offer了，暑期的提前批差不多刚开始，但此时我对整个过程还了解甚少，在投岗位时就面临了很大的困境，如果选择开发岗，整体要求会低一点，本科生容易一点，但自己其实相关实战经验很少，OS和计网两门课都欠缺，而且java相关的岗位会多一点，如果选择算法，其实暑期实习是和研二的学生直接竞争，少部分公司会把算法的门槛直接拉到研究生，所以很可能过不了简历关，而且按知乎的论调基本是车毁人亡。</p><hr><p>最后基本是算法，后端，大数据混投了，以算法为主，其实不太建议这样，实习显然是 </p><blockquote><p>专精选手&gt;自由人</p></blockquote><p>不过在被社会暴打之前我基本是在乱来，也没啥经验，然后就开始了一路白给的过程</p><h1 id="第一次面试-字节AML"><a href="#第一次面试-字节AML" class="headerlink" title="第一次面试-字节AML"></a>第一次面试-字节AML</h1><p>字节AML是第一个投的日常，也是比较想去的部门，部门主要以CPP和Python为主，技术栈也比较match，属于开发和算法都需要一点，不过那个时候自己也菜的离谱，写了5道leetcode就敢去给字节HR刷KPI，啥也不会，啥也不懂，面试前看了一个晚上的machine learning，结果面试问了进程线程和网络相关的一些八股，直接白给，你们可能不知道leetcode只写了五道就去面试是什么概念，我们一般只会用两个字来形容这种人—白给。</p><h1 id="中期的准备和思路转化"><a href="#中期的准备和思路转化" class="headerlink" title="中期的准备和思路转化"></a>中期的准备和思路转化</h1><p>三月初第一次白给之后，才开始逐渐去了解互联网面试的一些具体要求，然后整个三月基本处于同时准备托福和笔试面试的过程中，因为中间在准备托福的原因，不敢多做笔试，一直拖到考试结束才开始做笔试，这其实错过了很多机会，因为HC到后面只会变少。这中间笔试的字节和腾讯两家，但题目写的太少基本没有ac全部的，来道hard就直接选择放弃，最后字节强化学习中台挂在笔试。</p><h1 id="持续白给的过程"><a href="#持续白给的过程" class="headerlink" title="持续白给的过程"></a>持续白给的过程</h1><p>托福考完后开始了一路的笔试过程，但其实写的题还是不多，网易有道的NLP和美团的data都挂在笔试了，特别是美团直接上来三道hard，一直到阿里的笔试，算是第一次拿到一个还可以的笔试成绩，阿里笔试就两道，笔试前刚剑指完了《剑指offer》，再零零散散的写了一些，大概有80+左右的体量在，最后两道一道100Ac，一道也写出了大半，其实两道题整体难度也估计在hard到mid之间，并不简单，阿里之后的笔试就开始顺利起来，其实只要有一定题量了之后，应试的东西是最容易解决的，百度的算法也过了笔试是挺吃惊的，因为其实难度不小，华为笔试第一次三道全部AC（第一次全部AC的心情还是有点小激动的，题目不算特别难，但全部ac还是不容易的）。中间一直笔试失利其实心态挺不稳定，基本每次挂了都会去看看还有什么公司能投的，中间也投过一些独角兽（图森，旷世，依图等）不过独角兽的门槛似乎更高，基本投完就没消息的。</p><p>整个4月初就一直开始笔试，笔试后开始面试，最多的一天笔试了三场，确实非常消耗精力。</p><h1 id="网易"><a href="#网易" class="headerlink" title="网易"></a>网易</h1><p>网易最后拿到了雷火面试资格，然后告诉我投的岗位现在没了，转到另一个部门，当然技术方向差别也比较大，自然是没有结果了，网易互联网，互娱和雷火三个部门分开面试笔试，所以得重复笔试，最后互娱有发我笔试通知，但真的没力气再去做，干脆就放弃了。</p><h1 id="腾讯"><a href="#腾讯" class="headerlink" title="腾讯"></a>腾讯</h1><p>腾讯其实挺尴尬，因为腾讯其实算法岗只有研究员性质的岗位，其余全是开发，然后本网瘾少年直接投了个游戏引擎开发，(寒假瞎玩过Stanford的UE4课程)，然后一个天美的老哥直接来面，结果自然也是给HR刷KPI了。</p><h1 id="阿里"><a href="#阿里" class="headerlink" title="阿里"></a>阿里</h1><p>阿里其实投的最早，但我笔试拖到很晚才去做，主要是之前也没有信心，因为听说阿里笔试爆炸难（最后主要是白给多了也就不怕了，大不了再送一次）最后结果居然还可以，但是阿里部门特别多，一定要靠谱的内推，不然会被一个部门卡住，投之前一定要对部门有了解，我就被一个部门卡了三个月的简历，不给面试也不给挂。最后直接让HR给我挂了（挂人倒是巨快，5分钟就发邮件了），这之后多亏了学长的帮忙，转简历到了蚂蚁，蚂蚁的整体效率非常高了，转完第二天就面试了，但比较可惜的是蚂蚁基本以java为主，自己不是非常match，而且java方向的东西基本不会。但确实这过程还是非常得感谢蚂蚁学长的帮忙，让面试官按C++的岗位去面，过了一面，但最后可能也是HC已经不多了同时自己能力确实有所欠缺，因为阿里基本四月初就结束一轮了。阿里的整个面试还是比较友好，基本不问八股，更加考察全面的能力，对某个方向最好有比较深入的理解。</p><h1 id="百度"><a href="#百度" class="headerlink" title="百度"></a>百度</h1><p>百度投的算法岗，面试难度确实自闭，我都感觉面试官在怀疑我的智商了，一个点会问的非常深入，问到你不会为止，我基本最多答出第三层，但面试官在大气层，面完就感觉白给了。</p><h1 id="字节大数据"><a href="#字节大数据" class="headerlink" title="字节大数据"></a>字节大数据</h1><p>字节真的是算法后端大数据全挂一次，挂人的效率也是非常之高，感觉字节现在基本leetcode得200+刷透，得背熟八股文。</p><h1 id="华为"><a href="#华为" class="headerlink" title="华为"></a>华为</h1><p>华为面试前两天收到字节挂的邮件，前一天阿里二面挂的通知，自己的心态也发生了如下转变，只想随便找个班上，实在太拉胯了，<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210504143919.jpg" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210504143904.jpg" alt=""></p><p>华为算是背水一战了，本来也是比较想去的，面试前一天除了复习完机器学习，顺便下了mindspore，跑了一下demo，这其实对最后面试结果帮助非常大，华为整个面试流程算是比较顺利，问了一些machine learning相关的基础知识，主要聊了一下项目经历，之后面试官在我写算法题的时候介绍了mindspore的一些工作。再后面就开始了比较随意的聊天，然后我凭借丰富的画饼吹逼经验和这几天对mindspore的了解，从前几天华为发布的通用模型盘古聊到具体的业务聊了好久，面试官问我，对汇编有没有了解，我说写过RISC-V的汇编器，ARM有简单了解，然后他居然说“你这全栈工程师啊”（前天还被挂，说基础知识不扎实，怎么到这就全栈了）我直接一脸懵逼（内心狂喜）。</p><p>最后的结果自然是过了，看到结果时我直接戴上了我的战术目镜和耳机，靠着一首Future Bounce在阳台一个人蹦了30分钟</p><div class="aplayer no-destroy" data-id="001dghhk1BxxxH" data-server="tencent" data-type="song" data-fixed="false" data-mini="false" data-listFolded="false" data-order="random" data-preload="none" data-autoplay="false" muted></div><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210504231933.jpg" alt=""></p><h1 id="复习总结"><a href="#复习总结" class="headerlink" title="复习总结"></a>复习总结</h1><p>下面主要是自己学过和复习用的一些觉得非常好的资料和课程，</p><ul><li>吴恩达深度学习：<a href="https://www.bilibili.com/video/BV164411S78V">https://www.bilibili.com/video/BV164411S78V</a></li><li>斯坦福李飞飞计算机视觉：<a href="https://www.bilibili.com/video/BV1TJ411d7b7">https://www.bilibili.com/video/BV1TJ411d7b7</a></li><li>周博磊强化学习：<a href="https://www.bilibili.com/video/BV1LE411G7Xj">https://www.bilibili.com/video/BV1LE411G7Xj</a></li><li>斯坦福凸优化：<a href="https://www.bilibili.com/video/BV1Pg4y187Ed">https://www.bilibili.com/video/BV1Pg4y187Ed</a></li><li>动手实现深度学习：<a href="http://zh.gluon.ai/">http://zh.gluon.ai/</a></li><li>操作系统：<a href="https://classroom.udacity.com/courses/ud189">https://classroom.udacity.com/courses/ud189</a></li><li>SQL：<a href="https://classroom.udacity.com/courses/ud198">https://classroom.udacity.com/courses/ud198</a></li><li>data science：<a href="https://www.coursera.org/specializations/jhu-data-science">https://www.coursera.org/specializations/jhu-data-science</a></li><li>适合面试前复习的知识手册：<a href="https://github.com/NLP-LOVE/ML-NLP">https://github.com/NLP-LOVE/ML-NLP</a></li></ul><p>Udacity和coursera都是非常好的自学网站，特别是Udacity的课程和工业界实际应用非常的接近，知识更加实用。</p><h1 id="面试总结"><a href="#面试总结" class="headerlink" title="面试总结"></a>面试总结</h1><ul><li>传统ML SVM，KNN，LR问到最多，boost面经讲到很多，但我没怎么碰到，可能是推荐广告的岗位问的多一点？</li><li>尽可能让面试官多问项目经历，要主动诱导面试官去问项目经历，当然前提是对项目非常了解。</li><li>深度这块除了有个整体认识，最好对某一个网络有深入的理解。</li><li>本科去面算法就要做好准备，不会因为你本科就降低要求。</li></ul><p>我总共笔试了八场，面试了7场，这整个过程确实非常折磨人，但从中也确实意识到了自己的知识体系的局限性和技术能力上的不足，同时自己各方面也有所提升，从刚开始笔试0AC到后面可以全AC。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>整个找实习过程确实挺不容易，毕竟直接和研二学生对线，但最后结果还算不错，去了自己最想去的，对于AI整个行业来说，个人认为在现阶段显然是能实际落地的工程大于算法本身，数据质量大于算法。目前我将自己的方向主要聚焦于AI模型在端侧的部署和应用这样一个细分赛道，避开了CV，NLP这样比较卷的领域，对于整个</p><p>我花了近一年的时间说服自己不适合做偏理论的研究，更适合做能落地的工程或产品，做不了天花板，做个吊灯还是有机会的。所以对我来说与其再做些CNN换transformer的东西，不如做点能实际落地的工程，而整个工程能力却是学校教学所欠缺的，甚至个人认为学校的工程教学，哪怕是ZJU也是和工业界应用脱节严重的，不可否认现在zju对通识教育以及微积分这种以应试为核心的课程的教学的质量还是值得肯定的，但到工程教育时，很难想象，一些课程除了把2000年创建的PPT给你念一遍，然后来一场靠记忆驱动的考试，消磨你对这个学科的兴趣，并不会提升你的核心能力，确实让人失望，个人认为工科生最终主要还是去工业界去做一些伟大的产品或系统，而不是去纠结一些理论性的东西（至少个人定位是这样），在这样的情况下，不如自己去多上上CS四大的课程，去工业界提前积累实战经验而不是沉溺于简单的猫分类。</p><p>找实习期间有幸看到filestorm大神（现在已经是图森的CTO了）的上海交大生存手册，真心佩服这样可以聚焦自己热爱的方向，放下一些GPA这样一些表层的东西的大神， 大神会因为觉得考试浪费时间，而选择直接参加补考，只是因为补考简单，不需要花太多时间，我直到大三才有勇气去放弃一些原来认为很重要但实际没有意义的东西，去更加聚焦于一些更值得去做的事情（<a href="https://survivesjtu.gitbook.io/survivesjtumanual/li-zhi-pian/zong-you-geng-zhi-de-zuo-de-shi-qing">https://survivesjtu.gitbook.io/survivesjtumanual/li-zhi-pian/zong-you-geng-zhi-de-zuo-de-shi-qing</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 面经 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习激活函数和优化</title>
      <link href="2021/03/14/deep-learning-sum/"/>
      <url>2021/03/14/deep-learning-sum/</url>
      
        <content type="html"><![CDATA[<h1 id="BatchNormalization的作用"><a href="#BatchNormalization的作用" class="headerlink" title="BatchNormalization的作用"></a>BatchNormalization的作用</h1><p>神经网络在训练的时候随着网络层数的加深,激活函数的输入值的整体分布逐渐往激活函数的取值区间上下限靠近,从而导致在反向传播时低层的神经网络的梯度消失。而Batch<img src="https://uploadfiles.nowcoder.com/images/20190317/311436_1552801230080_095961B16BE2B8C2E508F4A1AB257B7D" alt="img">Normalization的作用是通过规范化的手段,将越来越偏的分布拉回到标准化的分布,使得激活函数的输入值落在激活函数对输入比较敏感的区域,从而使梯度变大,加快学习收敛速度,避免梯度消失的问题。</p><h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><h2 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h2><p>如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，那么网络的逼近能力就相当有限。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）。</p><h2 id="常见的激活函数"><a href="#常见的激活函数" class="headerlink" title="常见的激活函数"></a>常见的激活函数</h2><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg7yinpjnqj30u01e87af.jpg" alt=""></p><h2 id="激活函数特点"><a href="#激活函数特点" class="headerlink" title="激活函数特点"></a>激活函数特点</h2><ul><li>Sigmoid函数<ul><li>梯度消失问题：Sigmoid函数造成梯度消失的主要原因在于，一般来说输入是介于[0,1]之间，而Sigmoid的导数在[0,1]的定义域内的值域为[0.2,0.25]，那么在反向传播的过程中这个值会变得很小，越靠近输入层值越小，导致输入层的参数难以被更新。</li><li>均值不为0：均值不为0产生的主要问题在于反正传播要么都往正向传播要么都往负向传播，这使得收敛变慢，但是目前一般都使用batch来进行梯度下降，可以一定程度上解决这个问题。</li><li>解析式含有幂运算，计算机求解耗时。</li></ul></li><li>Tanh函数<ul><li>显然，tanh函数解决了0均值的问题，但是梯度消失和幂运算的问题仍然存在。</li></ul></li><li>ReLU<ul><li>解决部分梯度消失问题：非负区间的梯度是一个常数，可以解决一部分的梯度消失问题，但是仍然存在梯度消失问题，</li><li>计算速度、收敛快，很容易理解，只需要取max就好了</li><li>存在非0均值的问题，存在某些神经元可能永远不能被激活（Dead relu problem），这主要和初始化有关。</li></ul></li><li>Leaky Relu :<ul><li>leaky relu解决了dead relu problem的问题，但是计算会慢一些，且从实际效果来说没有总是好于relu</li></ul></li><li>Maxout<ul><li>函数为$max(w_1x+b_1, w_2x+b_2)$，能很好地解决relu的dead问题，可以说relu是maxout的特殊形式，也就是$w_1=0, b_1=0$的情况。</li><li>推荐使用</li></ul></li></ul><ul><li><strong>Dropout</strong></li></ul><h2 id="优化算法Optimizer"><a href="#优化算法Optimizer" class="headerlink" title="优化算法Optimizer"></a>优化算法Optimizer</h2><h3 id="Batch-Gradient-Descent-（BGD）"><a href="#Batch-Gradient-Descent-（BGD）" class="headerlink" title="Batch Gradient Descent （BGD）"></a><strong>Batch Gradient Descent （BGD）</strong></h3><p>全部的数据用于一次梯度下降</p><p>$\theta=\theta-\eta \cdot \nabla_{\theta} J(\theta)$</p><h3 id="Stochastic-Gradient-Descent-SGD"><a href="#Stochastic-Gradient-Descent-SGD" class="headerlink" title="Stochastic Gradient Descent (SGD)"></a><strong>Stochastic Gradient Descent (SGD)</strong></h3><p>  一次一个数据用户更新梯度</p><h3 id="Mini-Batch-Gradient-Descent-（MBGD）"><a href="#Mini-Batch-Gradient-Descent-（MBGD）" class="headerlink" title="Mini-Batch Gradient Descent （MBGD）"></a><strong>Mini-Batch Gradient Descent （MBGD）</strong></h3><p>  一次部分数据用于更新梯度，常取16、32、64、128、256</p><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a><strong>Momentum</strong></h3><p>  引入了指数加权平均，$\gamma$一般取0.9</p><p>  $v<em>{t}=\gamma v</em>{t-1}+\eta \nabla<em>{\theta} J(\theta)$<br>  $\theta=\theta-v</em>{t}$</p><p>  加入的这一项，<strong>可以使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。</strong></p><h3 id="Nesterov-Accelerated-Gradient"><a href="#Nesterov-Accelerated-Gradient" class="headerlink" title="Nesterov Accelerated Gradient"></a><strong>Nesterov Accelerated Gradient</strong></h3><p>  $\begin{array}{l}<br>  v<em>{t}=\gamma v</em>{t-1}+\eta \nabla<em>{\theta} J\left(\theta-\gamma v</em>{t-1}\right) \<br>  \theta=\theta-v_{t}<br>  \end{array}$</p><p>  用 $θ−γv_t−1$ 来近似当做参数下一步会变成的值，则在计算梯度时，<strong>不是在当前位置，而是未来的位置上</strong></p><p>  与Momentum的对比：</p><p>  <img src="https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180310224024153-1974893457.png" alt="img"></p><p>  蓝色是 Momentum 的过程，会先计算当前的梯度，然后在更新后的累积梯度后会有一个大的跳跃。<br>  而 NAG 会先在前一步的累积梯度上(brown vector)有一个大的跳跃，然后衡量一下梯度做一下修正(red vector)，这种预期的更新可以避免我们走的太快。</p><p>  NAG 可以使 RNN 在很多任务上有更好的表现。</p><h3 id="Adagrad-（Adaptive-gradient-algorithm）"><a href="#Adagrad-（Adaptive-gradient-algorithm）" class="headerlink" title="Adagrad （Adaptive gradient algorithm）"></a><strong>Adagrad （Adaptive gradient algorithm）</strong></h3><p>  $\theta<em>{t+1, i}=\theta</em>{t, i}-\frac{\eta}{\sqrt{G<em>{t, i i}+\epsilon}} \cdot g</em>{t, i}$</p><p>  在原来的learning_rate的基础上除以之前所有梯度的平方和再开根号。$\epsilon$ 一个很小的值，防止分母为0</p><p>  <strong>缺点</strong>:分母会不断积累，这样学习率就会收缩并最终会变得非常小。<strong>学习率会急剧下降。</strong></p><h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a><strong>Adadelta</strong></h3><p>  $\Delta \theta<em>{t}=-\frac{\eta}{\sqrt{E\left[g^{2}\right]</em>{t}+\epsilon}} g_{t}$</p><p>  $E\left[g^{2}\right]<em>{t}=\gamma E\left[g^{2}\right]</em>{t-1}+(1-\gamma) g_{t}^{2}$</p><p>  当$\gamma=0.5$时，$E\left[g^{2}\right]_{t}=RMS[g]_t$ 梯度的均方根。即，与adagrade相比分母变成了梯度平方的均值。</p><p>  还可以将学习率 $\eta$ 换成RMS：</p><p>  $\begin{array}{l}<br>  \theta<em>{t+1}=\theta</em>{t}-\frac{R M S[\Delta \theta]<em>{t-1}}{R M S[g]</em>{t}} g_{t}<br>  \end{array}$</p><h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a><strong>RMSprop</strong></h3><script type="math/tex; mode=display">\begin{array}{l}E\left[g^{2}\right]_{t}=0.9 E\left[g^{2}\right]_{t-1}+0.1 g_{t}^{2} \\\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} g_{t}\end{array}</script><p>  引入了指数加权平均，旨在消除梯度下降中的摆动。</p><h3 id="Adam：Adaptive-Moment-Estimation"><a href="#Adam：Adaptive-Moment-Estimation" class="headerlink" title="Adam：Adaptive Moment Estimation"></a><strong>Adam：Adaptive Moment Estimation</strong></h3><p><strong>相当于 RMSprop + Momentum</strong>, 除了像 Adadelta 和 RMSprop 一样存储了过去梯度的平方 vt 的指数衰减平均值 ，也像 momentum 一样保持了过去梯度 mt 的指数衰减平均值：</p><script type="math/tex; mode=display">\begin{array}{l}mm_{t}=\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t} \\v_{t}=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2}\end{array}</script><p>  如果 mt 和 vt 被初始化为 0 向量，那它们就会向 0 偏置，所以做了<strong>偏差校正</strong>，通过计算偏差校正后的 mt 和 vt 来抵消这些偏差：</p><script type="math/tex; mode=display">\hat{m}_{t}=\frac{m_{t}}{1-\beta_{1}^{t}} \\  \begin{array}{l}  \hat{v}_{t}=\frac{v_{t}}{1-\beta_{2}^{t}} \\  \quad \\   \theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon} \hat{m}_{t}  \end{array}</script>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文明6（和而不同mod）老秦巨贪流战报（神标地球）</title>
      <link href="2021/02/27/civilization/"/>
      <url>2021/02/27/civilization/</url>
      
        <content type="html"><![CDATA[<p>寒假拿黄金时代mod完了很久，但最后感觉效果过于imba，比如佩特拉全城加，简直无敌，太爽而且区域规划不太喜欢，偶尔在b站发现D大佬正在制作的文明6Plus和而不同版本（Harmony in Diversity），感觉理念非常认同，和而不同主要改动如下：</p><ul><li>总督老马环游直接取消，砍树收益变低。</li><li>宗教信条大改，发教更加容易</li><li>每个城市要求玩家根据地貌修建不同区域来取得最大收益</li><li>除了纪念碑，粮仓，磨坊，新增了更多的市中心一级建筑，分别加锤，瓶，琴，鸽，钱</li><li>每个城市新增城市政策，发展差异化</li></ul><h2 id="游戏设置："><a href="#游戏设置：" class="headerlink" title="游戏设置："></a>游戏设置：</h2><ul><li>难度：神</li><li>地图：Yamp定制地图，类地球</li><li>模式：垄断与公司，蛮族新模式（无英雄，无结社）</li><li>文明：老秦</li></ul><h2 id="开局："><a href="#开局：" class="headerlink" title="开局："></a>开局：</h2><p>原地坐，准备直接采矿砌砖抢大金，狗+移民，买工人</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210227163518.png" alt=""></p><h2 id="远古"><a href="#远古" class="headerlink" title="远古"></a>远古</h2><p>这把AI主要集中在欧洲，前期欧洲疯狂内卷，忠诚压不住直接淘汰了奥斯曼，远古三城开，因为亚洲没有邻居，所以尽可能多控地盘，12分相对远一点，首都大金神谕，一分圣地准备发教，因为信条大改，万神殿选了石圈（采石场+2鸽，+4大仙），首都一分总共三个采石场，因为远古时代分溢出，大仙留着古典发教。<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210227163500.png" alt=""></p><h2 id="52T进古典："><a href="#52T进古典：" class="headerlink" title="52T进古典："></a>52T进古典：</h2><p>因为前期双圣地科技一度比较低，中期靠全部信仰买工人移民<br>奇观基尔瓦基斯瓦尼，大图，紫禁城，布达拉宫，空花（80T捡漏的）</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210227163552.png" alt=""></p><h2 id="114T工业化"><a href="#114T工业化" class="headerlink" title="114T工业化"></a>114T工业化</h2><p>往北部坐下三陈，目前加AI叛乱的总共14城，二月更新蛮族可以变成城邦，多宗主了一个科邦<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210227163604.png" alt=""></p><h2 id="130T飞行"><a href="#130T飞行" class="headerlink" title="130T飞行"></a>130T飞行</h2><p>此时科技已经和AI拉开，就没有完全冲速度，还搞了金门大桥这种娱乐奇观</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210227163614.png" alt=""></p><h2 id="169T火星发射，南极站完成后科技直接起飞"><a href="#169T火星发射，南极站完成后科技直接起飞" class="headerlink" title="169T火星发射，南极站完成后科技直接起飞"></a>169T火星发射，南极站完成后科技直接起飞</h2><p>因为不能靠砍树飞天，所以务必养一座高锤城飞天，这里没有控制好节奏，导致科技比项目快好多，尾盘细节处理没到位慢了好几T</p><h2 id="172T出-合成专家统治，科技直接爆炸，总共出了11个宇航中心"><a href="#172T出-合成专家统治，科技直接爆炸，总共出了11个宇航中心" class="headerlink" title="172T出 合成专家统治，科技直接爆炸，总共出了11个宇航中心"></a>172T出 合成专家统治，科技直接爆炸，总共出了11个宇航中心</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210227163853.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210227163904.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20210227163904.png" alt=""></p><h2 id="总体感受"><a href="#总体感受" class="headerlink" title="总体感受"></a>总体感受</h2><p>1.宗教太强，万盛殿改动后发教更加容易，可以单圣地发教，中期移民，工人完全靠信仰买<br>2.需要养人口！！利用三角田配合磨坊把人口养高，因为区域价格随数量不断升高，后期的城拍出学院的成本大幅上升，所以不用过分爆铺，人口锁学院12-15城足矣，部分大城可以晚出学院，新城拍下我都先买粮仓（3粮一人口 -&gt; 2,5粮一人口），、<br>3.每个市中心的一级建筑收益在前期非常可观，前期的科技可以靠一级建筑补，不急着普及学院</p>]]></content>
      
      
      <categories>
          
          <category> 白天摸鱼 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> civilization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>磁电子学</title>
      <link href="2021/02/23/cidianzixue/"/>
      <url>2021/02/23/cidianzixue/</url>
      
        <content type="html"><![CDATA[<ul><li>电子角动量<script type="math/tex; mode=display">P_l=\sqrt{l(l+1)}\hbar</script></li><li>电子轨道磁矩<script type="math/tex; mode=display">\mu_l=-\frac{e}{2m}P_l</script></li><li>电子自旋角动量（自旋量子数为1/2<script type="math/tex; mode=display">P_s=\sqrt{S(S+1)}\hbar=\frac{\sqrt{3}}{2}\hbar</script></li><li>自旋磁矩<script type="math/tex; mode=display">\mu_s=-\frac{e}{m}P_s</script></li><li><p>电子总磁矩</p><script type="math/tex; mode=display">\mu=\mu_l+\mu_s=-\frac{e}{2m}(P_l+2P_s)</script></li><li><p>总角动量</p><script type="math/tex; mode=display">P_J=P_L+P_S=[J(J+1)]^{1/2}\hbar</script></li><li>总磁矩<script type="math/tex; mode=display">\mu_J=\mu_L+\mu_S=g_J[J(J+1)]^{1/2}\hbar</script></li><li>有效波尔磁子数<script type="math/tex; mode=display">p=g_J[J(J+1)]^{1/2}</script></li><li>朗德因子<script type="math/tex; mode=display">g_J=1 + [J(J+1) + S(S+1) - L(L+1)]/[2J(J+1)]</script></li></ul><ul><li>超导体会从超导态转变成为正常态</li><li>温度上升到临界温度TC </li><li>磁场加强到临界磁场HC</li><li>电流密度增加到临界电流密度JC</li></ul><ul><li>常导电流密度： $J_N=-n_Nev_N$</li><li>常导电子浓度 ： $n_N = n_0(T/T_C)^4$</li></ul><ul><li>超导电流密度： $J_s=-n_se^*v_s$</li><li><p>超导电子浓度 ： $n_s = n_0[1-(T/T_C)^4]=\eta n$</p></li><li><p>伦敦第一方程</p><script type="math/tex; mode=display">\frac{\partial J_{\mathrm{S}}}{\partial t}=\frac{1}{\Lambda^{2}} E</script><blockquote><p>电场决定超导电流的时间变化率</p></blockquote></li><li>伦敦第二方程</li></ul><script type="math/tex; mode=display">B = -\nabla \times \Lambda^{2} J_S</script><blockquote><p>磁场决定超导电流</p></blockquote><ul><li>伦敦穿透深度<script type="math/tex; mode=display">\lambda_L=\Lambda/\mu_0^{1/2}</script></li><li><p>磁感应强度</p><script type="math/tex; mode=display">B_y(x)=B_0 exp(x/\lambda_L)</script></li><li><p>原子平均磁矩</p><script type="math/tex; mode=display">\bar{\mu}\approx \mu_J^2B_0/(3k_BT)</script></li><li>顺磁性<script type="math/tex; mode=display">\chi=\mu_0M/B_0=\mu_0\frac{C}{T}</script></li></ul>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> physical </tag>
            
            <tag> 学废了 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>光电子学</title>
      <link href="2021/02/23/guangzixue/"/>
      <url>2021/02/23/guangzixue/</url>
      
        <content type="html"><![CDATA[<h1 id="固体光的吸收"><a href="#固体光的吸收" class="headerlink" title="固体光的吸收"></a>固体光的吸收</h1><ul><li><p>光到电<br>光照射固体，电子没有溢出时，光能会转化为电能。</p></li><li><p>电到光<br>电子注入后，半导体材料电子被激发，电子跃迁到高能级</p></li></ul><h2 id="光的吸收系数"><a href="#光的吸收系数" class="headerlink" title="光的吸收系数"></a>光的吸收系数</h2><p>固体光学中固体的光学性质可以用折射率$n$和消光系数$\kappa$来描述，两者构成复折射率</p><script type="math/tex; mode=display">\hat{n}=n+j\kappa</script><script type="math/tex; mode=display">\lambda \leq \lambda_C=\frac{1.24}{E_g}</script><p>$E_g$单位为$\lambda$单位为$\mu m$</p><p>光的吸收系数$\alpha$：单位距离吸收的相对光子数</p><script type="math/tex; mode=display">\alpha=\frac{4\pi \kappa}{\lambda_0}</script><h2 id="光的吸收过程"><a href="#光的吸收过程" class="headerlink" title="光的吸收过程"></a>光的吸收过程</h2><ul><li>本征吸收</li><li>激子吸收</li><li>自由载流子吸收</li><li>杂质与缺陷吸收</li><li>晶格吸收</li></ul><h3 id="本征吸收"><a href="#本征吸收" class="headerlink" title="本征吸收"></a>本征吸收</h3><p><img src="15C2F69A0ECF4EFBB64B356119C492DF" alt="image"><br>电子从价带跃迁到导带，产生电子对空穴</p><h4 id="直接跃迁-hv-gt-E-g"><a href="#直接跃迁-hv-gt-E-g" class="headerlink" title="直接跃迁($hv&gt;=E_g$)"></a>直接跃迁($hv&gt;=E_g$)</h4><p>导带最小值和价带最大值对应于相同的波矢,发生在直接带隙半导体</p><ul><li>能量守恒<script type="math/tex; mode=display">\hbar w=E_C(k_C)-E_V(k_V)</script></li><li>动量守恒<script type="math/tex; mode=display">\hbar k_w=\hbar k_C-\hbar k_V</script></li><li>光子动量远小于电子动量<script type="math/tex; mode=display">k_C\approx k_V</script><h4 id="间接跃迁-hv-lt-E-g"><a href="#间接跃迁-hv-lt-E-g" class="headerlink" title="间接跃迁($hv&lt;E_g$)"></a>间接跃迁($hv&lt;E_g$)</h4>导带最小值和价带最大值对应的波矢不同</li></ul><p>间接带隙半导体</p><ul><li>能量守恒<script type="math/tex; mode=display">\hbar w=E_C(k_C)-E_V(k_V)\pm E_p</script></li><li>动量守恒<script type="math/tex; mode=display">\hbar k_w=\hbar k_C-\hbar k_V\pm \hbar k_p</script><blockquote><p>间接跃迁需要声子参与，因此吸收率远小于直接跃迁</p></blockquote></li></ul><h3 id="激子吸收"><a href="#激子吸收" class="headerlink" title="激子吸收"></a>激子吸收</h3><p>光子能量$hv&lt;E_g$发生</p><p>价带电子和和导带空穴通过库伦力舒服在一起，产生电子空穴束缚系统，形成激子（电中性，一种激发的电子能量状态，能在固体中运动，但不传导电流）</p><p>激子更具电子空穴检举可以分为</p><ul><li>紧束缚激子（数个晶格常数）</li><li><p>松束缚激子 （数百个晶格常数）</p></li><li><p>激子复合</p><ul><li>激子消失，产生辐射</li></ul></li><li>再激励<ul><li>传导电流 </li></ul></li></ul><h3 id="自由载流子吸收"><a href="#自由载流子吸收" class="headerlink" title="自由载流子吸收"></a>自由载流子吸收</h3><p>光照射固体，扩散到红外和微博波段，电子和空穴吸收能量后再能带内部跃迁吸收</p><blockquote><p>动量守恒1:光子动量很小，电子动量改变由声子或电离杂质的散射补偿</p></blockquote><h3 id="杂质和缺陷吸收"><a href="#杂质和缺陷吸收" class="headerlink" title="杂质和缺陷吸收"></a>杂质和缺陷吸收</h3><p>束缚在杂质或者缺陷上的电子或者空穴吸收光子能量</p><h3 id="晶格吸收"><a href="#晶格吸收" class="headerlink" title="晶格吸收"></a>晶格吸收</h3><p>光照射固体后晶格吸收光子能量，光子能量直接转化为声子动能</p><h3 id="电子空穴产生速率"><a href="#电子空穴产生速率" class="headerlink" title="电子空穴产生速率"></a>电子空穴产生速率</h3><ul><li>半导体吸收光子能量产生电子-空穴对<ul><li>光子能量$hv&gt;= E_g$ </li><li>光强：$I(x)=I_0exp(-\alpha x)$</li><li>一个光子能产生一个电子-空穴对</li></ul></li><li>产生电子-空穴对速率<script type="math/tex; mode=display">g(x)=\frac{\alpha I(x)}{hv}</script></li><li>光子通量<script type="math/tex; mode=display">I(x)/hv</script></li></ul><h1 id="固体对光发射"><a href="#固体对光发射" class="headerlink" title="固体对光发射"></a>固体对光发射</h1><h2 id="发光过程中的激发"><a href="#发光过程中的激发" class="headerlink" title="发光过程中的激发"></a>发光过程中的激发</h2><ul><li>激发：外界因素作用系统，使系统吸收能量，激发电子</li><li>固体受激发光：被激发电子以光辐射形式释放能量</li><li>主要的发光形式：光致发光、阴极射线发光、放射线发光、<br>生化发光、电致发光</li></ul><h3 id="光致发光"><a href="#光致发光" class="headerlink" title="光致发光"></a>光致发光</h3><ul><li>光辐射照射固体<ul><li>吸收能量</li><li>电子能量增加</li></ul></li><li>电子向上能级跃迁，进入非热平衡状态</li><li>电子向下能级跃迁，恢复到热平衡状态</li><li>固体发光<h3 id="阴极射线发光"><a href="#阴极射线发光" class="headerlink" title="阴极射线发光"></a>阴极射线发光</h3></li><li>电子束或阴极射线轰击发光物体（如荧光屏）引起发光<h3 id="放射线发光"><a href="#放射线发光" class="headerlink" title="放射线发光"></a>放射线发光</h3></li><li>快速粒子或高能射线(如X，α，β，γ及中子射线)<br>轰击发光物体引起的发光<h3 id="生化发光"><a href="#生化发光" class="headerlink" title="生化发光"></a>生化发光</h3></li><li>人体热、化学反应、生物（萤火虫、某些鱼）发光<h3 id="电场发光"><a href="#电场发光" class="headerlink" title="电场发光"></a>电场发光</h3></li><li>场致发光：薄膜发光材料、发光粉末（如荧光粉），<br>在外电压下产生的发光</li><li>pn结或肖特基势垒发光：注入载流子复合导致的发光</li><li>白炽灯：电流使钨丝发热，导致的发光</li></ul><h2 id="基本符合类型"><a href="#基本符合类型" class="headerlink" title="基本符合类型"></a>基本符合类型</h2><p>pn结正向偏置，激发电子注入，电子-空穴对复合</p><ul><li>光辐射：发射光子，辐射复合</li><li>能量传递：激发，非辐射复合</li></ul><h3 id="带间复合过程"><a href="#带间复合过程" class="headerlink" title="带间复合过程"></a>带间复合过程</h3><ul><li><p>导带底附近的电子和价带顶附近的空穴复合</p></li><li><p>辐射能量$hv \approx E_g$</p></li></ul><h3 id="杂质，缺陷复合过程"><a href="#杂质，缺陷复合过程" class="headerlink" title="杂质，缺陷复合过程"></a>杂质，缺陷复合过程</h3><ul><li>导带与受主之间的复合</li><li>施主与价带之间的复合</li><li>施主与受主之间</li><li>深陷阱产生的非辐射复合</li></ul><h3 id="俄歇复合（非辐射）"><a href="#俄歇复合（非辐射）" class="headerlink" title="俄歇复合（非辐射）"></a>俄歇复合（非辐射）</h3><ul><li>电子空穴复合后能量转移给同一能带的高能态自由载流子</li><li>高能态自由载流子跃迁到导带底或价带顶</li><li>激发多个声子使晶体发热<br><img src="C4D4E40C4BE04A6092A9E558649CDC58" alt="image"></li></ul><h2 id="发光效率"><a href="#发光效率" class="headerlink" title="发光效率"></a>发光效率</h2><p>量子效率$\eta_q\approx$辐射效率=辐射复合速率$R_r$/总复合速率<code>R</code></p><script type="math/tex; mode=display">\eta_q\approx \frac{R_r}{R_{nr}+R_r}=\frac{\tau_{nr}}{\tau_{nr}+\tau_r}</script><ul><li>辐射寿命$\tau_r$</li><li>非辐射寿命$\tau_{nr}$</li></ul><blockquote><p>复合速率与寿命$\tau$成反比，非辐射寿命越大，发光效率越高</p></blockquote><p>带间电子与空穴的复合速率，与电子浓度n和空穴浓度p成正比</p><script type="math/tex; mode=display">R_r==Bnp</script><blockquote><p>直接带隙半导体的B值比间接高$10^6$个数量级，所以间接材料很难发生辐射复合</p></blockquote><h3 id="材料"><a href="#材料" class="headerlink" title="材料"></a>材料</h3><ul><li><p>直接带隙半导体</p><ul><li>GaAs</li><li>$Al<em>xGa</em>{1-x}As$</li><li>$GaAs_{1-x}P_x$</li></ul></li><li><p>$Al<em>xGa</em>{1-x}$</p><ul><li>$0&lt;x&lt;0.45$<ul><li>直接带隙</li></ul></li><li>$x&gt;0.45$<ul><li>间接带隙</li></ul></li></ul></li><li>$GaAs_{1-x}P_x$<ul><li><code>0&lt;x&lt;0.45</code><ul><li>直接带隙 </li></ul></li><li><code>x&gt;0.45</code><ul><li>间接带隙 </li></ul></li></ul></li></ul><h1 id="pn结光升伏特和太阳能电池solar-cell"><a href="#pn结光升伏特和太阳能电池solar-cell" class="headerlink" title="pn结光升伏特和太阳能电池solar cell"></a>pn结光升伏特和太阳能电池solar cell</h1><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><ul><li>光照：$hν&gt;= E_g$</li><li>产生光生电子—空穴对</li><li>p区光生电子进入n区，在n区边界积累</li><li>n区光生空穴进入p区，在p区边界积累</li><li>部分空间电荷被中和</li><li>在pn结建立光生电动势(电压)V<br>与内建电场方向相反</li><li>pn结光生电动势(电压)V</li><li>pn结势垒降低到$V_D - V$</li><li>光生电流$I_L$</li><li>光照恒定 接通外电路</li><li>负载电流I</li><li>pn结光生伏特效应</li></ul><h2 id="光电池的I-V特性"><a href="#光电池的I-V特性" class="headerlink" title="光电池的I-V特性"></a>光电池的I-V特性</h2><script type="math/tex; mode=display">I_F=I_s[exp(\frac{eV}{k_BT})-1]</script><ul><li><p>正偏导致电流</p><script type="math/tex; mode=display">I_F=I_S[exp(\frac{eV}{k_BT})-1]</script></li><li><p>光生电流为$I_L$</p></li></ul><script type="math/tex; mode=display">I=I_L-I_F=I_L-I_S\{\exp[eV/(k_BT)]-1\}\\V=(k_BT/e)In[1+(I_L-I)/I_S]=V_TIn[1+(I_L-I)/I_S]</script><ul><li>短路电流short-circuit current（V = 0）</li></ul><script type="math/tex; mode=display">I_{SC}=I_L</script><ul><li>开路电压open-circuit voltage（I = 0）</li></ul><script type="math/tex; mode=display">V_OC=V_TIn[1+I_L/I_S]</script><p><img src="94AC520865A7412783C64927482B88ED" alt="image"></p><h2 id="太阳电池的光电转换效率"><a href="#太阳电池的光电转换效率" class="headerlink" title="太阳电池的光电转换效率"></a>太阳电池的光电转换效率</h2><ul><li>太阳辐射谱：主要波长范围在0.3 μm到1.5 μm，峰值约在0.5 μm</li><li></li></ul><h3 id="太阳电池的光电转换效率（现有机理与技术）："><a href="#太阳电池的光电转换效率（现有机理与技术）：" class="headerlink" title="太阳电池的光电转换效率（现有机理与技术）："></a>太阳电池的光电转换效率（现有机理与技术）：</h3><ul><li>最大输出的电功率$P_max$与输入光功率$P_in$之比<script type="math/tex; mode=display">\eta=\frac{P_{max}}{P_{in}}=\frac{I_mV_m}{P_{in}}=\frac{(FF)I_{SC}V_{OC}}{P_{in}}</script></li></ul><blockquote><p>太阳电池的热力学极限效率为32%</p><ul><li>等功率线：<br>I-V特性曲线图中I·V =常数的曲线</li><li>等效率线：<br>输入功率恒定时的等功率线</li><li>pn结的光电转换效率：<br>与I-V曲线相切的等效率线对应的效率</li></ul></blockquote><h2 id="非均匀吸收效应"><a href="#非均匀吸收效应" class="headerlink" title="非均匀吸收效应"></a>非均匀吸收效应</h2><p>光照半导体表面，光生载流子是非均匀的：</p><ul><li>光强：$I(x)=I_0exp(-\alpha x)$</li><li>光子在表面的反射率R</li></ul><ul><li>单位时间、单位体积，半导体吸收的光子数随深度x指数<br>衰减：<script type="math/tex; mode=display">N(x)=(1-R)\alpha I(x)/(hv)=\alpha (1-R)I_0/(hv)exp(-\alpha x)</script></li><li>1个光子能产生1个电子—空穴对</li><li>半导体吸收光子能量产生电子—空穴对的速率为<script type="math/tex; mode=display">G(x,\lambda)=\alpha(\lambda)[1-R(\lambda)]I_0(\lambda)/(hv) exp[-\alpha(\lambda)x]</script></li></ul><h1 id="非增益型半导体光电探测器"><a href="#非增益型半导体光电探测器" class="headerlink" title="非增益型半导体光电探测器"></a>非增益型半导体光电探测器</h1><p>半导体光电探测器是将光能转化为电能的光电器件</p><p>主要用于检测光信号<br>半导体光电探测器：</p><ul><li>非增益型光电探测器<ul><li>光电二极管(photodiode：pn结型、PIN型、肖特基型)</li><li>光电导探测器(photoconductor)</li></ul></li><li>增益型光电探测器<ul><li>雪崩光电二极管(avalanche photodiode：APD)</li><li>光晶体管(phototransistor or optical transistor)</li></ul></li></ul><blockquote><p>暗电流：无光照时产生的反向电流<br>光电流：光照产生</p></blockquote><h2 id="主要参数"><a href="#主要参数" class="headerlink" title="主要参数"></a>主要参数</h2><ul><li><p>量子效率η：入射光子产生（有效）电子—空穴对的能力</p><script type="math/tex; mode=display">η=（有效光生“电子—空穴对”数目）/（入射光子数目）\\=（光电流/单电子电量）/（光功率/单光子能量）\\=[I_{ph}/e]/[P/(hv)]\\=I_{ph}hv/(eP)</script></li><li><p>$I_{ph}$：平均输出光电流</p></li><li>P：平均入射光功率</li><li>量子效率η ：无量纲</li></ul><h3 id="响应度"><a href="#响应度" class="headerlink" title="响应度"></a>响应度</h3><p>响应度R：光电转换效率（外部特性）</p><script type="math/tex; mode=display">R = 平均输出光电流 / 平均入射光功率\\= I_{ph}/P = eη/(hν)</script><p>量纲$[R] = V-1 或μA/μW$</p><h3 id="频率响应与响应时间"><a href="#频率响应与响应时间" class="headerlink" title="频率响应与响应时间"></a>频率响应与响应时间</h3><ul><li>光生载流子渡越耗尽层的漂移时间$t_r$ </li><li>光生载流子扩散到耗尽层边界的扩散时间$t_d$ </li><li>结电容与负载电阻电路的时间常数$t_RC$</li></ul><h3 id="噪声"><a href="#噪声" class="headerlink" title="噪声"></a>噪声</h3><p>噪声是一种随机信号，本质是物理量围绕其平均值的涨落。对于<br>平稳随机过程，对噪声度量通常采用先计算噪声电压（电流）的<br>平方值，然后对时间做平均，求噪声电流（电压）的均方值。</p><h4 id="散粒噪声："><a href="#散粒噪声：" class="headerlink" title="散粒噪声："></a>散粒噪声：</h4><p>由于探测器在光辐射作用下，光电子或光生载流子的随机跃迁<br>所造成的。每一瞬间出现的载流子是不确定的。由于随机起伏<br>是一个个带电粒子引起的，所以称为散粒噪声。</p><h4 id="热噪声"><a href="#热噪声" class="headerlink" title="热噪声"></a>热噪声</h4><p>由电阻中载流子的随机热运动引起的：任何处在热平衡条件下<br>的电阻，即使没有外加电压，也有一定的噪声。</p><h2 id="肖特基势垒光电二极管"><a href="#肖特基势垒光电二极管" class="headerlink" title="肖特基势垒光电二极管"></a>肖特基势垒光电二极管</h2><ul><li>金属同n型半导体接触构成<br>肖特基势垒光电二极管</li><li>类似于p+n结器件</li><li>肖特基势垒光电二极管，特点：<ul><li>用很薄(~0.01μm)的、光学上透明的金属膜，消除p+层对高能光子(透入深度<br>&lt;0.1μm)的吸收，增强短波响应：适用于可见光和紫外区。</li><li>耗尽层仅挨表面，有效减小表面复合，提高量子效率；</li><li>镀增透膜，减少反射大部分入射光，在半导体表面附近吸收；</li><li>并非所有的材料都能制作PN结。</li><li>典型参数：量子效率为70%，响应时间约0.1ns；</li></ul></li></ul><h1 id="增益型和异质结半导体光电探测器"><a href="#增益型和异质结半导体光电探测器" class="headerlink" title="增益型和异质结半导体光电探测器"></a>增益型和异质结半导体光电探测器</h1><h2 id="雪崩光电二极管（APD）"><a href="#雪崩光电二极管（APD）" class="headerlink" title="雪崩光电二极管（APD）"></a>雪崩光电二极管（APD）</h2><ul><li>PN-PD</li><li>高反偏压 </li><li>碰撞离化</li><li>雪崩光电效应</li><li>雪崩光电二极管（APD） </li><li>内部电流增益，灵敏度高</li><li>响应速度快，可达1000 GHz，即1 THz</li></ul><h3 id="电子（空穴）电离率-alpha-n-alpha-p"><a href="#电子（空穴）电离率-alpha-n-alpha-p" class="headerlink" title="电子（空穴）电离率$\alpha_n(\alpha_p)$"></a>电子（空穴）电离率$\alpha_n(\alpha_p)$</h3><p>一个电子（空穴）在单位距离上激发一个二次电子—空穴<br>对的概率, 是电场的强相关函数。<br>材料的电子—空穴电离率比：</p><ul><li><p>材料的电子—空穴电离率比：</p><script type="math/tex; mode=display">K=\alpha_n/\alpha_p</script><blockquote><p>如果电子和空穴有相同的电离化<br>率（K =1），雪崩过程不断重复，<br>不会停止：</p></blockquote></li><li><p>增大器件增益；</p></li><li>耗时长，减小器件带宽；</li><li>随机过程，增大器件噪声；</li><li>不稳定会导致雪崩击穿。</li></ul><h3 id="增益带宽积"><a href="#增益带宽积" class="headerlink" title="增益带宽积"></a>增益带宽积</h3><script type="math/tex; mode=display">f_T= M·B</script><h3 id="过剩噪声因子noise-factor-F"><a href="#过剩噪声因子noise-factor-F" class="headerlink" title="过剩噪声因子noise factor: F"></a>过剩噪声因子noise factor: F</h3><ul><li>理想的倍增过程：<ul><li>通过倍增区的每个电子或空穴都倍增M倍</li><li>光跃迁随机引起散粒噪声电流也相应倍增M倍</li><li>散粒噪声均方电流增加了$M^2$倍</li></ul></li></ul><h2 id="异质结的窗口效应、光限制作用、光电二极管"><a href="#异质结的窗口效应、光限制作用、光电二极管" class="headerlink" title="异质结的窗口效应、光限制作用、光电二极管"></a>异质结的窗口效应、光限制作用、光电二极管</h2><p>光照射异质结宽带区Eg2</p><ul><li>低能光子 hν＜Eg1  <ul><li>光子不能产生电子—空穴对  </li><li>无光电流</li></ul></li><li>Eg1＜hν＜Eg2<ul><li>光子透过宽带材料进入窄带材料</li><li>在窄带区产生电子—空穴对</li><li>产生光电流</li></ul></li><li>高能光子hν＞Eg2<ul><li>宽带区产生电子—空穴对，光子不能进入窄带材料</li><li>不能扩散到耗尽区 在窄带区<br>在窄带区不能产生电子—空穴对</li><li>无光电流</li></ul></li></ul><h1 id="发光二极管-LED"><a href="#发光二极管-LED" class="headerlink" title="发光二极管(LED)"></a>发光二极管(LED)</h1><ul><li>正向偏置pn结 (电子或空穴)  </li><li>正向注入</li><li>(空穴或电子) 复合</li><li>自发辐射发光</li><li>发光二极管(LED)</li></ul><ul><li>紫外光</li><li>可见光：电光源（显示、照明）</li><li>红外光：通信（0.85 μm、1.31 μm、1.55 μm）</li></ul><ul><li>自发辐射：无外电磁场作用时，电子自发从高能级向低能级跃迁，并发<br>射光子。</li><li>受激辐射：当收到外来能量E=ћv=E1-E2的光照射时，高能级的电子受外<br>来光的激励作用向低能级跃迁，同时发射一个与外来光子完全相同的光子。</li></ul><ul><li>辐射效率<script type="math/tex; mode=display">\eta=\frac{\tau_{nr}}{\tau_{nr}+\tau_r}</script></li><li>内量子效率<script type="math/tex; mode=display">\eta_内= \gamma \eta =单位时间产生光子数/单位时间注入电子-空穴对数</script></li><li>外量子效率<script type="math/tex; mode=display">\eta_外=单位时间从晶体发射出的光子数/单位时间注入的电子-空穴</script></li></ul><script type="math/tex; mode=display">η_外<η_内</script><p><strong>单位时间从晶体发射出的光子数&lt;单位时间产生的光子数:</strong></p><ul><li>体内光吸收  </li><li>菲涅耳（反射）损耗</li><li>临界角（全反射）损耗</li></ul><h1 id="半导体激光器"><a href="#半导体激光器" class="headerlink" title="半导体激光器"></a>半导体激光器</h1><h2 id="半导体受激光发射的产生"><a href="#半导体受激光发射的产生" class="headerlink" title="半导体受激光发射的产生"></a>半导体受激光发射的产生</h2><ul><li>激光器三要素<ul><li>粒子数分布反转population inversion（区域：有源区）</li><li>谐振腔（产生单色性和方向性都很好的单色光）</li><li>阈值条件（泵浦：提供能量获得增益，抵消内部损耗、<br>产生激光输出）</li></ul></li></ul><h3 id="产生粒子数反转方法"><a href="#产生粒子数反转方法" class="headerlink" title="产生粒子数反转方法"></a>产生粒子数反转方法</h3><ul><li>光激励</li><li>电子束激励</li><li>雪崩碰撞激励</li><li>pn结正向注入激励 </li></ul><h3 id="产生粒子数反转的原因"><a href="#产生粒子数反转的原因" class="headerlink" title="产生粒子数反转的原因"></a>产生粒子数反转的原因</h3><ul><li>受激发射概率=受激吸收概率</li><li>哪一种过程起主导取决于粒子分布情况</li><li>处于激发态原子数大于基态原子数，受激发射将超过吸收（光增益）<br><img src="04DFD278024F4A8C8EE034FF5947AA0E" alt="image"></li></ul><h2 id="光学谐振腔"><a href="#光学谐振腔" class="headerlink" title="光学谐振腔"></a>光学谐振腔</h2><p>GaAs-pn结激光器，以严格垂直结面方向的一对(110)解理面<br>作为镜面构成平面谐振腔</p><h2 id="阈值条件"><a href="#阈值条件" class="headerlink" title="阈值条件"></a>阈值条件</h2><ul><li>增益系数g：光波沿z向传播单位长度内发射强度的增益</li><li>损耗系数α：光波沿z向传播单位长度内发射强度的内部损耗</li><li>阈值条件：光波在谐振腔内往返传播一次的增益大于损耗</li></ul><h2 id="半导体激光器的主要特性"><a href="#半导体激光器的主要特性" class="headerlink" title="半导体激光器的主要特性"></a>半导体激光器的主要特性</h2><p><img src="DC71C155D6984D89ADCBCAB6D0D9CDD8" alt="image"></p><ul><li>谐振腔的振幅条件（阈值条件）：<br>光波在谐振腔内往返传播一次的总增益为<script type="math/tex; mode=display">g>g_{th}=\alpha – In(R_1R_2 )/(2l) >\alpha</script></li></ul><h2 id="输出光功率和转换效率"><a href="#输出光功率和转换效率" class="headerlink" title="输出光功率和转换效率"></a>输出光功率和转换效率</h2><ul><li>外量子效率: $η_外=P/(IV)$</li><li>外微分量子效率: $η_D=(P-Pth)/[(I-Ith)V]≈P/[(I-Ith)V$</li></ul><h2 id="模式与光谱分布"><a href="#模式与光谱分布" class="headerlink" title="模式与光谱分布"></a>模式与光谱分布</h2><script type="math/tex; mode=display">λ_0 = 2nl/q</script><h2 id="分布反馈式-DFB-半导体激光器"><a href="#分布反馈式-DFB-半导体激光器" class="headerlink" title="分布反馈式(DFB) 半导体激光器"></a>分布反馈式(DFB) 半导体激光器</h2><script type="math/tex; mode=display">λ_0 = 2n_SΛ/m，(m=1, 2, 3, …为衍射级)</script>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> physical </tag>
            
            <tag> 学废了 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>半导体器件</title>
      <link href="2021/02/23/bandaotiqijian/"/>
      <url>2021/02/23/bandaotiqijian/</url>
      
        <content type="html"><![CDATA[<h1 id="PN结特性概述"><a href="#PN结特性概述" class="headerlink" title="PN结特性概述"></a>PN结特性概述</h1><h2 id="平衡PN结"><a href="#平衡PN结" class="headerlink" title="平衡PN结"></a>平衡PN结</h2><ul><li>n区<ul><li>施主杂质</li></ul></li><li>p区<ul><li>受主杂质 </li></ul></li></ul><p>因为存在载流子浓度差，空穴从p区向n区扩散，电子从n区向p区扩散，结p区侧聚集负离子(电离受主)，结n区侧聚集正离子(电离施主)，形成了，负空间电荷区，正空间电荷区，阻止进一步扩散，形成并增强相反方向的漂移运动，扩散与漂移的动态平衡<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/20/31a103daa21a51ddcebd8f920635f40d.png" alt=""></p><ul><li>同质结：以两种相同的半导体单晶材料为基础</li><li>异质结：以两种不同的半导体单晶材料为基础</li><li>pn结： 在导电类型相反的半导体单晶材料交界处形成</li><li>高低结：在导电类型相同的半导体单晶材料交界处形成</li></ul><p>n区导带的电子进入p区导带存在$V_D$的势垒，称为内建电势，也称为接触电位差</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/20/4686255128c82592458eedbcfed0e058.png" alt=""></p><p>对此，我们有关系</p><script type="math/tex; mode=display">eV_D=e|\Phi_{Fn}|+e|\Phi_{Fp}|</script><script type="math/tex; mode=display">e\Phi_{Fn}=E_{Fi}(n区)-E_F</script><script type="math/tex; mode=display">e\Phi_{Fp}=E_{Fi}(p区)-E_F</script><ul><li>n区导带电子浓度<script type="math/tex; mode=display">n=N_Cexp(-\frac{E_C-E_F}{k_BT})</script></li><li>本征半导体<script type="math/tex; mode=display">n_i=N_Cexp(-\frac{E_C-E_{Fi}}{k_BT})</script>所以<script type="math/tex; mode=display">n=n_iexp(\frac{E_F-E_{Fi}}{k_BT})</script></li><li>完全电离情况下<script type="math/tex; mode=display">n\approx N_D</script><script type="math/tex; mode=display">\Phi_{Fn}=\frac{E_{Fi}-E_F}{e}\approx-\frac{k_BT}{e}In(\frac{N_D}{n_i})</script>同理对于p区<script type="math/tex; mode=display">p=N_Vexp(-\frac{E_V-E_F}{k_BT})</script></li><li>本征半导体<script type="math/tex; mode=display">p_i=N_Vexp(\frac{E_V-E_{Fi}}{k_BT})</script>所以<script type="math/tex; mode=display">p=n_iexp(\frac{E_{Fi}-E_F}{k_BT})</script></li><li>完全电离情况下<script type="math/tex; mode=display">p\approx N_A</script><script type="math/tex; mode=display">\Phi_{Fn}=\frac{E_{Fi}-E_F}{e}\approx\frac{k_BT}{e}In(\frac{N_A}{n_i})</script></li></ul><p>我们可以得到</p><script type="math/tex; mode=display">V_D=|\Phi_{Fn}|+|\Phi_{Fp}|\\=\frac{k_BT}{e}In(\frac{N_AN_D}{n_i^2})\\=V_TIn(\frac{N_AN_D}{n_i^2})</script><p>其中$V_T=\frac{k_BT}{e}$,称为热电压</p><p>室温热电压（thermal voltage）:<br>$V_T= 0.026 V$</p><h2 id="同质突变pn结"><a href="#同质突变pn结" class="headerlink" title="同质突变pn结"></a>同质突变pn结</h2><h3 id="同质突变pn结电荷分布和电场分布"><a href="#同质突变pn结电荷分布和电场分布" class="headerlink" title="同质突变pn结电荷分布和电场分布"></a>同质突变pn结电荷分布和电场分布</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/20/c6fb9e57004c2cf1d9ee425b6f616e26.png" alt=""></p><h3 id="电位分布"><a href="#电位分布" class="headerlink" title="电位分布"></a>电位分布</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/20/66aac681fe0ef5a30504fe3195ad3721.png" alt=""></p><script type="math/tex; mode=display">V_D=\frac{e}{2\epsilon_r\epsilon_0}(N_Ax_p^2+N_Dx_n^2)</script><p>$\epsilon_r$为半导体电容率</p><p>空间电荷区宽度space charge width</p><script type="math/tex; mode=display">X_{\mathrm{D}}=x_{\mathrm{n}}+x_{\mathrm{p}}=\sqrt{V_{\mathrm{D}}\left(\frac{2 \varepsilon_{\mathrm{r}} \varepsilon_{0}}{e}\right)\left(\frac{N+N_{\mathrm{D}}}{N_{\mathrm{A}} N_{\mathrm{D}}}\right)}</script><h1 id="3-3-5半导体连续性方程"><a href="#3-3-5半导体连续性方程" class="headerlink" title="3.3.5半导体连续性方程"></a>3.3.5半导体连续性方程</h1><p>均匀掺杂半导体在热平衡下，单位时间、单位体积<br>电子空穴对的：产生数 = 复合数</p><p>在外界（光或电）作用下，载流子浓度与平衡值有偏离，产生非平衡载流子。n型半导体光照后增加载流子，非平衡少子浓度：$\Delta p$</p><ul><li>少子扩散<ul><li>扩散diffusion流密度（单位时间、通过单位面积的粒子数）：</li></ul></li></ul><script type="math/tex; mode=display">S_{\mathrm{p}}=-D_{\mathrm{p}} \frac{\mathrm{d} \Delta p(x)}{\mathrm{d} x}</script><ul><li>空穴扩散流密度：$S_p$</li><li>空穴扩散系数hole diffusion coefficient：$D_p$</li><li>单位时间、单位体积内积累的空穴数</li></ul><script type="math/tex; mode=display">-\frac{\mathrm{d} S_p}{\mathrm{d} x}=D_{\mathrm{p}} \frac{\mathrm{d}^{2} \Delta p(x)}{\mathrm{d} x^{2}}</script><ul><li>少子复合<br>+单位时间、单位体积内复合recombination的空穴数：$\frac{\Delta p(x,t)}{\tau_p}$</li><li>非平衡少子寿命$\tau_p$</li></ul><ul><li>单位时间、单位体积中空穴数的变化</li></ul><script type="math/tex; mode=display">\frac{dp(x,t)}{dt}=D_{\mathrm{p}} \frac{\mathrm{d}^{2} \Delta p(x)}{\mathrm{d} x^{2}}-\frac{\Delta p(x,t)}{\tau_p}</script><ul><li>非平衡少子（空穴）浓度<script type="math/tex; mode=display">\Delta p(x)=\Delta p(0)exp(-x/L_p)</script></li><li>扩散长度</li></ul><script type="math/tex; mode=display">L=\sqrt{D_p\tau_p}</script><ul><li>扩散电流密度<script type="math/tex; mode=display">j_{p D i f f}=-e D_{p} p \frac{\mathrm{d} \Delta p(x)}{\mathrm{d} x}</script></li></ul><p>外电场E使少子（空穴）产生漂移drift运动</p><ul><li>漂移电流密度<script type="math/tex; mode=display">J_P=ep\mu_pE</script>$\mu_p$为空穴迁移率</li></ul><h2 id="整流特性"><a href="#整流特性" class="headerlink" title="整流特性"></a>整流特性</h2><p>pn结具有单向导电的整流特性，为了推导这一结果，我们假定</p><ul><li>pn结为突变耗尽层，其它区为电中性</li><li>玻尔兹曼近似</li><li>载流子小注入</li></ul><ul><li>n区多子majority carrier（电子）浓度（完全电离）:$n_{n0}=N_D$</li><li>p区多子（空穴）浓度（完全电离）：$p_{p0}=N_A$</li><li>p区少子minority carrier（电子）浓度<br>$n<em>{p0}=n_i^2/p</em>{<br>p0}\approx n_{n0} exp(-\frac{eV_D}{k_BT})$</li></ul><h3 id="正向电压作用"><a href="#正向电压作用" class="headerlink" title="正向电压作用"></a>正向电压作用</h3><p>正向电压$V = V_F&gt; 0$</p><p>势垒区内，载流子浓度小、电阻大势垒区外，载流子浓度大、电阻小,电压基本降落在势垒区势垒$x_D$区变窄、变低$e(V_D-V_F)$<br>破坏平衡，结区漂移运动<br>结区扩散运动 &gt; 漂移运动<br>破坏无偏压时的动态平衡<br>净扩散流，少子正向注入</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/20/2e0e4e77b58912bc122a33bc974d7ef5.png" alt=""></p><p>n区，p区中的多子浓度变化不大，但少子浓度会变化几个数量级</p><ul><li>p区电子浓度<script type="math/tex; mode=display">n_p=n_{p0}exp(\frac{eV_F}{k_BT})</script></li><li>n区空穴浓度<script type="math/tex; mode=display">p_n=p_{n0}exp(\frac{eV_F}{k_BT})</script></li></ul><h3 id="理想二极管方程"><a href="#理想二极管方程" class="headerlink" title="理想二极管方程"></a>理想二极管方程</h3><script type="math/tex; mode=display">J=J_S[exp(\frac{eV_F}{k_BT})-1]</script><h3 id="反向电流饱和密度"><a href="#反向电流饱和密度" class="headerlink" title="反向电流饱和密度"></a>反向电流饱和密度</h3><script type="math/tex; mode=display">J_S=\frac{eD_nn_{p0}}{L_n}+\frac{eD_pp_{n0}}{L_P}</script><h3 id="反向电压作用"><a href="#反向电压作用" class="headerlink" title="反向电压作用"></a>反向电压作用</h3><p>反偏下，少子浓度很低，少子浓度梯度几乎不随电压变化，达到稳定值</p><h2 id="电容特性"><a href="#电容特性" class="headerlink" title="电容特性"></a>电容特性</h2><h3 id="势垒电容"><a href="#势垒电容" class="headerlink" title="势垒电容"></a>势垒电容</h3><p>pn结的电容效应包括势垒电容和扩散电容两种</p><ul><li>单位面积势垒电容<script type="math/tex; mode=display">C_B=\epsilon_r\epsilon_0/X_D</script></li><li><p>势垒宽度</p><script type="math/tex; mode=display">X_D=\sqrt{(V_D-V)(\frac{2\epsilon_r\epsilon_0}{e})(\frac{N_A+N_D}{N_A+N_D})}</script><h3 id="扩散电容"><a href="#扩散电容" class="headerlink" title="扩散电容"></a>扩散电容</h3><script type="math/tex; mode=display">C_D=\frac{e^2(n_{p0}L_n+p_{n0}L_P)}{k_BT}exp(\frac{eV}{k_BT})</script></li><li><p>正偏：主要是CD</p></li><li>反偏：主要是CB</li></ul><h2 id="击穿特性"><a href="#击穿特性" class="headerlink" title="击穿特性"></a>击穿特性</h2><p>反偏电压<code>V</code>增大到$V_B$（击穿电压）<br>反向电流激烈增大，pn结击穿,击穿<br>隧道击穿（齐纳击穿Zener breakdown）、<br>雪崩击穿avalanche breakdown、热电击穿</p><h3 id="隧道击穿"><a href="#隧道击穿" class="headerlink" title="隧道击穿"></a>隧道击穿</h3><p>p侧价带内电子横穿禁带，直接进入n侧导带内，形成反向电流<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/21/8559c7b24b88a9c8d3339ffc3035fa9c.png" alt=""></p><h3 id="雪崩击穿"><a href="#雪崩击穿" class="headerlink" title="雪崩击穿"></a>雪崩击穿</h3><ul><li>少子扩散到势垒区</li><li>少子在势垒区中高速漂移</li><li>少子从电场获得足够大能量</li><li>与耗尽区内晶格原子的电子碰</li><li>产生许多电子—空穴对（二次电子—空穴对）</li><li>二次电子—空穴对继续漂移、碰撞</li><li>新的二次电子—空穴对（倍增效应）</li><li>pn结雪崩击穿</li></ul><blockquote><p>隧道效应和雪崩效应主要有载流子浓度决定，</p></blockquote><h3 id="热电击穿"><a href="#热电击穿" class="headerlink" title="热电击穿"></a>热电击穿</h3><p>温度升高，平衡少子浓度上升，电流上升，损坏后无法复原</p><h1 id="pn二极管"><a href="#pn二极管" class="headerlink" title="pn二极管"></a>pn二极管</h1><h2 id="常见的pn结二极管"><a href="#常见的pn结二极管" class="headerlink" title="常见的pn结二极管"></a>常见的pn结二极管</h2><ul><li>变容二极管</li><li>开关二极管</li><li>雪崩二极管</li><li>隧道二极管</li></ul><h2 id="变容二极管"><a href="#变容二极管" class="headerlink" title="变容二极管"></a>变容二极管</h2><p>利用反偏pn结电容（势垒电容）随电压非线性变化制成的可变电抗器件<br>因为势垒电容会随电压变化</p><ul><li><p>理想阶跃结二极管单位面积势垒电容量</p><script type="math/tex; mode=display">C_B=\epsilon_r\epsilon_0/X_D \propto (V_D+V_R)^{-1/2}</script></li><li><p>线性缓变二极管单位面积势垒电容量</p><script type="math/tex; mode=display">C_B= \propto (V_D+V_R)^{-1/3}</script></li><li><p>更一般的情况</p><script type="math/tex; mode=display">N=Bx^m</script></li></ul><script type="math/tex; mode=display">C_B\propto (V_D+V_R)^{-1/(m+2)}</script><blockquote><p>m = 0，为均匀掺杂结 ;</p><p>m = +1，为线性缓变结 ；</p><p>m = +2、+3，重掺杂n+ 基片上外延低杂质浓度n层；</p><p>m是负值，为超突变结</p></blockquote><h3 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h3><p>并联电感，做LC的谐振回路，其震荡频率为：</p><script type="math/tex; mode=display">f_{\mathrm{r}}=\frac{1}{2 \pi \sqrt{L C_{\mathrm{B}}}} \propto\left(V_{\mathrm{D}}+V_{\mathrm{R}}\right) \frac{1}{2(m+2)}</script><blockquote><p>超突变的谐振频率与反偏电压成正比</p></blockquote><h2 id="开关二极管"><a href="#开关二极管" class="headerlink" title="开关二极管"></a>开关二极管</h2><p>利用了二极管正向导通，反向不导通的特性</p><ul><li>t &lt; 0,落在结上正偏压为$V_D$，结两侧扩散区内少子积累，正偏电流为 ：<script type="math/tex; mode=display">I_F=(V_F-V_D)/R_F</script></li><li>t =0, 结上压降保持$V_D$不变，反向电流为<script type="math/tex; mode=display">I_R=(V_R+V_D)R_R\approx V_R/R_R</script>• $0&lt;t &lt;t_s$反向电流近似恒定<script type="math/tex; mode=display">I_R\approx V_R/R_R</script>扩散区内存储的少子流出被消耗，<br>结上正偏压逐渐下降到0， $t_s$称为存储时间。</li><li>$t &gt;t_s$,pn结开始反偏，p区和n区内部的少子被反向抽取，空间电荷区增大。</li><li>$t &gt;t_s+ t_2$，pn结稳定， $V_R$电压全落在pn结上，电流为反向饱和电流。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/21/0e7f37fe8a30d8c512674c34a1c5e519.png" alt=""></p><ul><li>反向恢复总时间<script type="math/tex; mode=display">t_r=t_s+t_2</script><h2 id="隧道二极管"><a href="#隧道二极管" class="headerlink" title="隧道二极管"></a>隧道二极管</h2></li><li>隧道二极管是n区，p区都重参杂的pn结二极管,使得n区和p区的费米能级分别进入导带和价带，只有这样的情况才会出现n区导带和p区价带出现相同能量的量子态</li><li>重掺杂使耗尽区宽度变得很窄，隧道距离很小（约5 ~ 10 nm），提高了隧穿几率。$T  \propto epx(-2d)$</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/21/bfb319e9169a4237460e2ac51e96d785.png" alt=""></p><p>掺杂浓度必须满足以下条件</p><script type="math/tex; mode=display">\begin{array}{c}n=N_{\mathbf{D}}>N_{\mathbf{C}} \quad p=N_{\mathbf{A}}>N_{\mathbf{V}} \\E_{\mathbf{F}}(T)=E_{\mathrm{CO}}+k_{\mathrm{B}} T \ln \left(\frac{N_{\mathrm{D}}}{N_{\mathrm{C}}}\right) \quad E_{\mathrm{F}}(T)=E_{\mathrm{V} 0}-k_{\mathrm{B}} T \ln \left(\frac{N_{\mathrm{A}}}{N_{\mathrm{V}}}\right)\end{array}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/21/b7b33bef664ea58ef05b4a55d77b598b.png" alt=""></p><p>上图展示了隧道二极管电流电压特性的四个阶段</p><ul><li>隧道效应：n区导带<br>电子进入p区价带，<br>产生正向隧道电流</li><li>隧道效应：n区导带<br>电子进入p区价带，<br>产生正向隧道电流</li><li>隧道效应：n区导带电子<br>进入p区价带，产生正向<br>隧道电流，但p区价带顶<br>介于n区导带底和EF之间</li><li>只有热电流，<br>没有隧道效<br>应产生的隧<br>道电流</li></ul><h2 id="雪崩二极管"><a href="#雪崩二极管" class="headerlink" title="雪崩二极管"></a>雪崩二极管</h2><p>雪崩二极管是利用了雪崩效应和渡越效应<br>微波频率下的负阻效应</p><h1 id="双极型晶体管"><a href="#双极型晶体管" class="headerlink" title="双极型晶体管"></a>双极型晶体管</h1><p>双极型的晶体管也就是三极管，有pnp和npn两种类型<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/21/1184c6b3ee7322484d9811c514f59e45.png" alt=""></p><blockquote><p>两个背靠背pn结互相影响：基区宽度比少子扩散区短</p></blockquote><h2 id="晶体管噪声"><a href="#晶体管噪声" class="headerlink" title="晶体管噪声"></a>晶体管噪声</h2><p>晶体管放大器的主要噪声：</p><ul><li>外界：输入、感应、耦合等方式引进的噪声</li><li>晶体管本身：<ul><li>热噪声：载流子无规则热运动引起电流起伏（温度愈<br>高，热噪声也愈大）</li><li>散粒噪声：载流子数目将在平均值附近起伏</li><li>低频1/f噪声（1/f）：表面能级、晶格缺陷、位错和<br>晶体不均匀性</li></ul></li></ul><h3 id="噪声系数"><a href="#噪声系数" class="headerlink" title="噪声系数"></a>噪声系数</h3><p>F = 输入信噪比 / 输出信噪比</p><h1 id="金属-半导体和肖特基势垒"><a href="#金属-半导体和肖特基势垒" class="headerlink" title="金属-半导体和肖特基势垒"></a>金属-半导体和肖特基势垒</h1><p>金属—半导体（简称金—半或M-S）接触：整流器、检测器、二<br>极管、场效应晶体管、太阳能电池、半导体集成器件电极</p><h2 id="理想肖特基势垒Schottky-barrier"><a href="#理想肖特基势垒Schottky-barrier" class="headerlink" title="理想肖特基势垒Schottky barrier"></a>理想肖特基势垒Schottky barrier</h2><ul><li>真空能级$E_0$</li></ul><p>电子脱离固体的最小能量（真空能级连续）</p><ul><li>金属功函数$\phi_m$：</li></ul><p>电子从金属中逸出到表<br>面外的真空中去至少需要的能量。金属费米<br>能级以上为空态、以下充满电子。</p><script type="math/tex; mode=display">\phi_m=E_0-E_{Fm}</script><ul><li>半导体功函数$\phi_s$</li></ul><p>半导体费米能级与真空能级之差</p><script type="math/tex; mode=display">\phi_s=E_0-E_{Fs}</script><ul><li>电子亲和势</li></ul><p>真空能级与半导体导带底之差 （不变）</p><script type="math/tex; mode=display">\chi= E_0 – E_C</script><blockquote><p>真空能级到导带顶部的距离不变</p></blockquote><p>电子会从费米能级高的地方向低处流,所以整个接触过程如下</p><ul><li>电子从半导体流向金属</li><li>金属表面负电荷、半导体表面带等量正电</li><li>产生接触电势差（降低/提高了半导体/金属的电子势能）</li><li>接触电势差阻止半导体中电子继续流向金属</li><li>平衡状态时 统一的费米能级 没有电子的净流动</li></ul><p>接触电势差为</p><script type="math/tex; mode=display">V_{ms}=(\phi_m-\phi_s)/e</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/21/6f4f3f8c2a0654e39b191e51c635ac41.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/21/8f0495e3d048fcb8b0ffddcac2cfdd6b.png" alt=""></p><h3 id="肖特基势垒"><a href="#肖特基势垒" class="headerlink" title="肖特基势垒"></a>肖特基势垒</h3><ul><li>半导体一侧的势垒高度(电子从半导体进入金属遇到的势垒)<script type="math/tex; mode=display">V_D=V_{ms}=(\phi_m-\phi_s)/e</script></li><li>金属一侧的势垒（电子从金属进入半导体遇到的势垒）<script type="math/tex; mode=display">V_{Dm}=(\phi_m-\chi)/e</script></li><li>空间电荷区内，电子浓度比内部小得多，形成高阻的区域，<br>称为<strong>阻挡层</strong>。</li></ul><h2 id="表面态和界面层对接触势垒的影响"><a href="#表面态和界面层对接触势垒的影响" class="headerlink" title="表面态和界面层对接触势垒的影响"></a>表面态和界面层对接触势垒的影响</h2><ul><li>理想肖特基模型与实验结果不符合：</li><li>模型：肖特基模型的势垒高度由金属和半导体的功函数决定</li><li>实验：90%的金属同半导体接触的势垒高度几乎相同,与金属的功函数无关，只与所用半导体的种类相关</li></ul><ul><li>理想半导体表面（n型半导体）<br>原子的周期性排列中断,出现半饱和的悬挂键、一些电子能量状态处于面能级（界面态）</li></ul><p>表面态一般分为施主型和受主型</p><ul><li>施主型：能级被电子占据时呈现电中性，施放电子后带正电；</li><li>受主型：能级空着时呈电中性，接受电子后带负电</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/21/4795f8577421abf3f48e6e99631b17b4.png" alt=""></p><ul><li>电子正好填满$q\phi_0$以下所有的表面态时，表面呈电中性；</li><li>$q\phi_0$以下的表面态空着时，表面带正电，呈施主型。</li><li><p>$q\phi_0$以上的表面态被电子填充时，表面带负电，呈受主型。<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/21/68cd10c07e70a3bff0325e1a6b910f9b.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/21/10bb6ae494a18f7566934091e4b7d82a.png" alt=""></p><h1 id="画能带的原则"><a href="#画能带的原则" class="headerlink" title="画能带的原则"></a>画能带的原则</h1></li><li><p>真空能级$E_0$连续（一般性）</p></li><li>电子亲和势$\chi$始终不变$\chi=E_)-E_C$（一般性）</li><li>费米能级的“钉扎”效应：价带以上$E_g/3$（特殊性）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/21/1962099e484eb41b7babbff05bc4d785.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/21/70790b44a486f0f6c6593b02565b43e5.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/21/67a002edbe030daf1339c90f5a96b2b5.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/21/d04d1dee52623a28d78132de665208fb.png" alt=""></p><h2 id="能带草图画法"><a href="#能带草图画法" class="headerlink" title="能带草图画法"></a>能带草图画法</h2><ul><li>真空能级$E_0$：表面外真空中电子势能（真空能级连续）</li><li>电子亲和势$\chi$ ：真空能级与半导体导带底之差（不变） $\chi = E_0 – E_C$</li><li>功函数$\phi$ ：电子从材料逸出到表面外的真空中，至少需要的能量<br>$\phi=E_0-E_F$</li><li>金属功函数$\phi<em>m = E_0 – E</em>{Fm}$,金属$E<em>{Fm}$以上为空态、$E</em>{Fm}$以下充满电子</li><li>半导体功函数$\phi_s = E_0 – E_F$</li><li>热平衡态，统一的费米能级</li><li>耗尽层部分能级弯曲</li><li>中性区（N区、p区）能级不弯曲（有压降除外，例如欧姆接触）</li></ul><h2 id="肖特基势垒二极管Schottky-barrier-diode"><a href="#肖特基势垒二极管Schottky-barrier-diode" class="headerlink" title="肖特基势垒二极管Schottky barrier diode"></a>肖特基势垒二极管Schottky barrier diode</h2><p>肖特基势垒二极管I-V特性与pn结二极管类似</p><script type="math/tex; mode=display">\begin{array}{l}J=J_{\mathrm{ST}}\left[\exp \left(\frac{e V}{k_{\mathrm{B}} T}\right)-1\right] \\J_{\mathrm{ST}}=A^{*} T^{2} \exp \left(-\frac{e V_{\mathrm{Dm}}}{k_{\mathrm{B}} T}\right)\end{array}</script><blockquote><p>$A^{*}$为有效理查逊常数</p></blockquote><h3 id="肖特基势垒二极管和pn结二极管的特性差异："><a href="#肖特基势垒二极管和pn结二极管的特性差异：" class="headerlink" title="肖特基势垒二极管和pn结二极管的特性差异："></a>肖特基势垒二极管和pn结二极管的特性差异：</h3><ul><li>肖特基势垒二极管为多子越过势垒的热电子发射（微观机理）<br>thermionic emission of majority carrier</li><li>pn结二极管为少子的注入和扩散（微观机理）<br>diffusion of minority carrier</li><li>反向饱和电流密度特性（宏观特性）：<ul><li>肖特基势垒二极管的反向饱和电流密度（$10^{-5}A/cm^2 $）</li><li>pn结二极管（$10^{-11}A/cm^2 $）</li></ul></li><li>开关特性（宏观特性）<br>肖特基势垒二极管是多子器件，正向偏置时没有扩散电容<br>（高频特性好，开关时间为ps，pn结二极管为ns） </li><li>肖特基势垒二极管的导通电压比pn结二极管低（宏观特性）</li></ul><h2 id="欧姆接触ohmic-contact"><a href="#欧姆接触ohmic-contact" class="headerlink" title="欧姆接触ohmic contact"></a>欧姆接触ohmic contact</h2><p>任何半导体器件或集成电路必须要与外界电学接触,接触电阻由势垒高度、掺杂浓度决定</p><p>欧姆接触的势垒有以下两种类型</p><ul><li>非整流势垒型接触nonrectifying barrier</li><li>隧道势垒型接触tunneling barrie</li></ul><h1 id="场效应晶体管"><a href="#场效应晶体管" class="headerlink" title="场效应晶体管"></a>场效应晶体管</h1><p>半导体外加电场时，表面势变化、电阻率变化，在与电场垂直方向的电流变化，所谓场效应，就是垂直的电场控制半导体的导电能力。</p><p>场效应晶体管主要有以下几种</p><ul><li>结型场效应晶体管JFET</li><li>绝缘栅场效应晶体管IGFET<ul><li>(主要是以SiO2作栅极绝缘物的金属Metal—氧化物Oxide—半导体Semiconductor场效应Field-Effect晶体管Transistor: MOSFET)</li></ul></li><li>肖特基势垒栅场效应晶体管MESFET</li></ul><h2 id="结型场效应晶体管JFET-junction-FET"><a href="#结型场效应晶体管JFET-junction-FET" class="headerlink" title="结型场效应晶体管JFET: junction FET"></a>结型场效应晶体管JFET: junction FET</h2><p>漏极D正偏$V_{DS} &gt; 0$</p><ul><li>$V<em>{DS} &lt; V</em>{DS0}$： $I<em>D$与$V</em>{DS}$接近线性变化<br>（线性区）</li><li>$V<em>{DS0} &lt; V</em>{DS} &lt; V_{DSa}$： $I_D$基本不变化<br>（饱和区）</li><li>$V<em>{DS} &gt; V</em>{DSa}$： ID随VDS急剧增加<br>（雪崩区）</li></ul><h2 id="金属—氧化物—半导体场效应晶体管-MOSFET-metal-oxide-semiconductor-FET"><a href="#金属—氧化物—半导体场效应晶体管-MOSFET-metal-oxide-semiconductor-FET" class="headerlink" title="金属—氧化物—半导体场效应晶体管(MOSFET: metal-oxide-semiconductor FET)"></a>金属—氧化物—半导体场效应晶体管(MOSFET: metal-oxide-semiconductor FET)</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/21/9475b3e1a7e2256ce11a11f83d01602a.png" alt=""></p><p>以氧化物作为绝缘层的IGFET，就是金属—氧化物—半导体<br>场效应管MOSFET</p><h2 id="肖特基势垒栅场效应晶体管"><a href="#肖特基势垒栅场效应晶体管" class="headerlink" title="肖特基势垒栅场效应晶体管"></a>肖特基势垒栅场效应晶体管</h2><p>(MESFET: metal-semiconductor FET)</p><p>肖特基势垒取代JFET的pn结势垒，形成肖特基势垒栅场效应管</p><h1 id="异质结及其器件"><a href="#异质结及其器件" class="headerlink" title="异质结及其器件"></a>异质结及其器件</h1><p>两种不同半导体材料接触，形成异质结</p><ul><li>同型isotype（高低）异质结（pP、nN）：杂质类型相同</li><li>异型anisotype（反型）异质结（pN、Pn）：杂质类型相反</li></ul><p>两种不同材料的禁带宽度，介电系数，晶格常数，热膨胀系数都不同</p><ul><li>晶格失配率<script type="math/tex; mode=display">2|a_1-a_2|/(a_1+a_2)</script></li></ul><h2 id="异质结的能带结构"><a href="#异质结的能带结构" class="headerlink" title="异质结的能带结构"></a>异质结的能带结构</h2><ul><li>包纳straddling:<ul><li>宽带隙wide-bandgap包纳窄带隙narrow-bandgap</li></ul></li><li>交替错开staggered:<ul><li>宽带隙与窄带隙交替错开</li></ul></li><li>完全错开broken gap:<ul><li>宽带隙与窄带隙完全错开<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/21/0d09eb1afe7fcc5e6e72861cc8647446.png" alt=""></li></ul></li></ul><h2 id="异质结特性"><a href="#异质结特性" class="headerlink" title="异质结特性"></a>异质结特性</h2><h3 id="理想pN异质结热平衡能带图"><a href="#理想pN异质结热平衡能带图" class="headerlink" title="理想pN异质结热平衡能带图"></a>理想pN异质结热平衡能带图</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/21/6ec509e206dd89b157ab97f1d58650aa.png" alt=""></p><ul><li>N区能级向下平移$eV_{D2}$</li><li>p区能级向上平移$eV_{D1}$</li></ul><script type="math/tex; mode=display">eV_D = eV_{D1} + eV_{D2}\\= E_{F2} – E_{F1}\\\phi_1-\phi_2</script><script type="math/tex; mode=display">\Delta E_C = \chi_1 – \chi_2</script><script type="math/tex; mode=display">\Delta E_V = \chi_2 + E_{g2} – (\chi_1 + E_{g1} )\\= \Delta E_g – \Delta E_C</script><script type="math/tex; mode=display">\Delta E_g = E_{g2} – E_{g1}</script>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> physical </tag>
            
            <tag> 学废了 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>固态电子基础</title>
      <link href="2021/02/23/gutaidianzijichu/"/>
      <url>2021/02/23/gutaidianzijichu/</url>
      
        <content type="html"><![CDATA[<h1 id="能带"><a href="#能带" class="headerlink" title="能带"></a>能带</h1><div class="table-container"><table><thead><tr><th>/</th><th>自由电子</th><th>孤立原子中的电子</th><th>固体中的电子</th></tr></thead><tbody><tr><td>外力源</td><td>无</td><td>单一原子核及同一原子的其它电子</td><td>众多原子核及其电子</td></tr><tr><td>能量分布</td><td>连续谱</td><td>能级</td><td>能带</td></tr></tbody></table></div><p><strong>电子的共有化运动形成能带</strong></p><h2 id="KP模型"><a href="#KP模型" class="headerlink" title="KP模型"></a>KP模型</h2><p>通过绝热近似把问题转化为单体多电子问题</p><p>波函数为以下形式</p><script type="math/tex; mode=display">\psi(x)=u(x)e^{jkx}</script><p>势函数具有空间周期性</p><script type="math/tex; mode=display">U(x)=U(x+na)</script><p>波函数同时也具有周期性</p><script type="math/tex; mode=display">\psi(x+na)=e^{jkna}\psi(x)</script><blockquote><p>布洛赫定理：电子在一个周期性的势场运动单电子的波函数为一个周期，振幅周期为晶格周期</p></blockquote><div class="table-container"><table><thead><tr><th>波函数</th><th>自由电子</th><th>布洛赫电子</th><th>孤立原子中的电子</th></tr></thead><tbody><tr><td>波函数</td><td>Aexp(jkx)平面波</td><td>u(x)exp(jkx)布洛赫波</td><td>f（r）束缚电子</td></tr><tr><td>运动区间</td><td>空间各点</td><td>晶体中</td><td>原子核周围</td></tr><tr><td>运动形式</td><td>自由运动</td><td>共有化运动</td><td>束缚运动</td></tr></tbody></table></div><h2 id="在共有化运动的情况下的波函数"><a href="#在共有化运动的情况下的波函数" class="headerlink" title="在共有化运动的情况下的波函数"></a>在共有化运动的情况下的波函数</h2><h2 id="本征方程"><a href="#本征方程" class="headerlink" title="本征方程"></a>本征方程</h2><script type="math/tex; mode=display">F(E)=f(\alpha a)=P'\frac{\sin(\alpha a)}{\alpha a}+\cos (\alpha a)=\cos(ka)</script><blockquote><p>本征方程决定了E和k的关系：色散关系</p></blockquote><script type="math/tex; mode=display">\alpha^2=(2m/\hat{h}^2)E</script><h2 id="k空间"><a href="#k空间" class="headerlink" title="k空间"></a>k空间</h2><script type="math/tex; mode=display">F(E)=f(\alpha a)=P'\frac{\sin(\alpha a)}{\alpha a}+\cos(\alpha a)=\cos(ka)</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/14/4b2416ce96d3291affa1fb4fd354da1a.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/14/f26902286898d9b9c230c0f1490d2b56.png" alt=""></p><h2 id="能带理论其他模型"><a href="#能带理论其他模型" class="headerlink" title="能带理论其他模型"></a>能带理论其他模型</h2><h3 id="自由电子模型"><a href="#自由电子模型" class="headerlink" title="自由电子模型"></a>自由电子模型</h3><p>金属的电子受到束缚很小，以接近自由电子的方式运动，所以用自由电子的薛定谔方程近似</p><h3 id="紧束缚模型"><a href="#紧束缚模型" class="headerlink" title="紧束缚模型"></a>紧束缚模型</h3><ul><li>每个原子对电子有比较强的束缚作用</li></ul><h3 id="为什么用E-k的关系，（能带图）来描述电子行为？"><a href="#为什么用E-k的关系，（能带图）来描述电子行为？" class="headerlink" title="为什么用E-k的关系，（能带图）来描述电子行为？"></a>为什么用E-k的关系，（能带图）来描述电子行为？</h3><ul><li>从波动的角度来看电子</li><li>波动行为最重要的关系是色散关系（w-k关系）</li></ul><h1 id="固体电子的导电性，有效质量，空穴"><a href="#固体电子的导电性，有效质量，空穴" class="headerlink" title="固体电子的导电性，有效质量，空穴"></a>固体电子的导电性，有效质量，空穴</h1><p>信息物理的最终目标是建立起能带和固体伏安特性之间的关系</p><p>电子能带全空或者全满都不导电</p><h2 id="电子运动速度"><a href="#电子运动速度" class="headerlink" title="电子运动速度"></a>电子运动速度</h2><script type="math/tex; mode=display">v=\frac{dE}{\hbar dk}</script><ul><li>群速度<script type="math/tex; mode=display">v=\frac{dw}{dk}</script></li><li>相速度<script type="math/tex; mode=display">v=\frac{w}{k}</script><h2 id="电子加速度"><a href="#电子加速度" class="headerlink" title="电子加速度"></a>电子加速度</h2><script type="math/tex; mode=display">a=\frac{dv}{dt}</script><script type="math/tex; mode=display">=\nabla_k \frac{dE}{\hbar dt}</script><script type="math/tex; mode=display">dE=Fr=Fvdt</script><script type="math/tex; mode=display">\to \frac{dE}{dt}=F\nabla_k\frac{E}{\hbar}</script><script type="math/tex; mode=display">\to a=\nabla_k^2\frac{1}{\hbar^2}FE(k)</script></li></ul><h2 id="电子有效质量"><a href="#电子有效质量" class="headerlink" title="电子有效质量"></a>电子有效质量</h2><script type="math/tex; mode=display">F+F_c=m_0a</script><script type="math/tex; mode=display">F=m^*a</script><script type="math/tex; mode=display">\to m^*=\frac{F}{F+F_c}</script><blockquote><p>有效质量反映了晶格的作用</p></blockquote><ul><li>$m^*&gt;0$，外力可以让电子加速，但比自由电子慢</li><li>$m^*&lt;0$，晶格势场比外力大的多，并且反向，电子无法加速</li></ul><ul><li>m*不是电子的惯性质量，而是在能量周期场中电子受外力作用<br>时，在外力与加速度的关系上相当于牛顿力学中的惯性质量；</li><li>m*不是一个常数，而是k的函数。一般情况下，它是一个张量，<br>只有特殊情况下，它才可化为一标量的形式。</li><li>m<em>可以是正值，也可以是负值，特别有意义的是：在能带底附<br>近，m</em>总是正值，表示电子从外场得到的动量多于电子交给晶<br>格的动量，而在能带顶附近，m*总是负的，表示电子从外场得<br>到的动量少于电子交给晶格的动量。</li></ul><h2 id="满带，部分填充的能带和空穴"><a href="#满带，部分填充的能带和空穴" class="headerlink" title="满带，部分填充的能带和空穴"></a>满带，部分填充的能带和空穴</h2><p>我们从能带的角度来认识导电性</p><p><strong>能带全空和全满都不导电，只有半填充才导电</strong></p><h3 id="电子能态变化与导电性"><a href="#电子能态变化与导电性" class="headerlink" title="电子能态变化与导电性"></a>电子能态变化与导电性</h3><ul><li>无外场<script type="math/tex; mode=display">E\approx\hbar^2k^2/2m</script><script type="math/tex; mode=display">E(-k)=E(k)</script><script type="math/tex; mode=display">v=dE/\hbar dk</script><script type="math/tex; mode=display">v(-k)=-v(k)</script>所以k与-k的两个电子对电流贡献抵消，晶体总电流为0</li></ul><ul><li>加入外场<br>外场对电子开始做功<script type="math/tex; mode=display">\frac{dk}{dt}=\frac{F}{\hbar}</script></li></ul><p>k空间，电子能态变化速率为</p><script type="math/tex; mode=display">v_k=dk/dt=-e|E|/\hbar</script><blockquote><p>$v_k$不是速度</p></blockquote><ul><li>空穴导电</li></ul><script type="math/tex; mode=display">J_1'=J_1+J_e=J_1=\frac{-e}{V}v(k_1)=0</script><script type="math/tex; mode=display">J_1=ev(k_1)/V</script><h2 id="金属，绝缘体和半导体"><a href="#金属，绝缘体和半导体" class="headerlink" title="金属，绝缘体和半导体"></a>金属，绝缘体和半导体</h2><ul><li>各种·晶体都有各自的能带</li><li>价电子是构成化学键的电子</li><li>价电子决定化学性质</li><li>价电子能级分裂形成的能量带为价带</li></ul><h3 id="绝缘体"><a href="#绝缘体" class="headerlink" title="绝缘体"></a>绝缘体</h3><ul><li>导带为空穴，不导电</li><li>价带为满带，不导电</li><li>禁带宽度很大，约6eV</li></ul><blockquote><p>金刚石。$E_g=5.6eV$</p></blockquote><h3 id="半导体"><a href="#半导体" class="headerlink" title="半导体"></a>半导体</h3><ul><li>导带和价带为部分填充，导电</li><li>禁带宽度比绝缘体小的多，约为1eV</li></ul><blockquote><p>在绝对零度时，半导体也不导电</p></blockquote><h3 id="金属"><a href="#金属" class="headerlink" title="金属"></a>金属</h3><ul><li>导带为部分填充能带，导电</li><li>价带为部分填充能带，导电</li><li>价带为部分填充能带</li></ul><h4 id="半金属"><a href="#半金属" class="headerlink" title="半金属"></a>半金属</h4><ul><li>导带和价带有少许交叠</li></ul><h2 id="一维概念带三位扩展"><a href="#一维概念带三位扩展" class="headerlink" title="一维概念带三位扩展"></a>一维概念带三位扩展</h2><ul><li>不同方向有不同的原子间距，不同的位函数</li></ul><blockquote><p>间接间隙带隙半导体材料适合作微电子材料，不适合光电子器件材料</p></blockquote><h1 id="半导体中的载流子"><a href="#半导体中的载流子" class="headerlink" title="半导体中的载流子"></a>半导体中的载流子</h1><p><strong>载流子</strong>：载运电流的粒子</p><ul><li>金属：自由电子</li><li>半导体：导带电子，价带空穴</li></ul><p>如何计算载流子的浓度？</p><ul><li>计算单位体积中能带中单位能量包含的能级（量子态）数目（态密度函数1）</li><li>计算每个能级（量子态）被电子占据的概率</li><li>对整个能带积分从而得到电子的浓度</li></ul><blockquote><p>把材料看作一个高楼，每个楼层有若干个房间（态密度），每个房间的人数（电子占据概率），对整个楼层进行积分</p></blockquote><h2 id="态密度函数"><a href="#态密度函数" class="headerlink" title="态密度函数"></a>态密度函数</h2><p>单位体积，单位能量允许电子占据量子态数目</p><p>我们用三维无限深势井中的自由电子在近似金属和半导体中的电子</p><script type="math/tex; mode=display">\psi(x)=\sqrt{2/a}\sin(kx)</script><script type="math/tex; mode=display">k=\frac{n\pi}{a}</script><script type="math/tex; mode=display">k^2=\frac{2mE}{\hbar^2}</script><p>在三维的情况下类似</p><script type="math/tex; mode=display">k^2=2mE/\hbar^2=k_x^2+k_y^2+k_z^2</script><script type="math/tex; mode=display">k_i=n_i\pi/a</script><p>在k空间，两个量子态间距为$\pi/a$，考虑泡利不相容原理，，一个自旋量子态体积为$0.5(\pi/a)^3$</p><h3 id="态密度公式"><a href="#态密度公式" class="headerlink" title="态密度公式"></a>态密度公式</h3><script type="math/tex; mode=display">g_E(E)dE=(4\pi a^3/h^3)(2m)^{3/2}E^{1/2}dE</script><p>单位体积、单位能量允许电子占据的量子态数目</p><script type="math/tex; mode=display">g(E)=4\pi(2m/h^2)^{3/2}E^{1/2}</script><script type="math/tex; mode=display">=\frac{4\pi(2m)^{3/2}}{h^3}\sqrt{E}</script><p>对于半导体我们用有效质量来代替实际质量，在导带底部</p><script type="math/tex; mode=display">E\approx E_c+dEk/dk+d^2Ek/dk^2</script><script type="math/tex; mode=display">E\approx E_c=\hbar^2kk^2/2m</script><script type="math/tex; mode=display">g_c=\frac{4\pi(2m)^{3/2}}{h^3}\sqrt{E-E_c}</script><h2 id="费米分布函数与费米能级"><a href="#费米分布函数与费米能级" class="headerlink" title="费米分布函数与费米能级"></a>费米分布函数与费米能级</h2><h3 id="玻尔兹曼分布"><a href="#玻尔兹曼分布" class="headerlink" title="玻尔兹曼分布"></a>玻尔兹曼分布</h3><p>传统气体分子分布</p><script type="math/tex; mode=display">f==exp[-E/k_BT],k_B玻尔兹曼常数</script><h3 id="全同粒子"><a href="#全同粒子" class="headerlink" title="全同粒子"></a>全同粒子</h3><p>经典粒子都可以一一区分，量子中的粒子用波函数描述，在波函数重叠时不能一一区分</p><h3 id="玻色子——爱因斯坦分布"><a href="#玻色子——爱因斯坦分布" class="headerlink" title="玻色子——爱因斯坦分布"></a>玻色子——爱因斯坦分布</h3><h4 id="玻色子"><a href="#玻色子" class="headerlink" title="玻色子"></a>玻色子</h4><ul><li>自旋为$\hbar$的整数倍（0, $\hbar$ , 2$\hbar$ , 3$\hbar$ , …） </li><li>粒子不可区分</li><li>没有泡利不相容原理</li><li>波函数具有交换对称性</li><li>多个玻色子可以挤进同一能态</li><li>光子、π介子等无质量粒子</li></ul><p>分布函数</p><script type="math/tex; mode=display">f(E)=\frac{g_i}{-1+exp(E-\mu)/k_BT}</script><h3 id="费米-狄拉克分布"><a href="#费米-狄拉克分布" class="headerlink" title="费米-狄拉克分布"></a>费米-狄拉克分布</h3><h4 id="费米子"><a href="#费米子" class="headerlink" title="费米子"></a>费米子</h4><ul><li>自旋为$\hbar$的半整数倍$(\pm\frac{1}{2}\hbar,\pm\frac{3}{2}\hbar,\pm\frac{5}{2}\hbar…)$</li><li>质子、电子、中子</li><li>满足泡利不相容原理<script type="math/tex; mode=display">f(E)=\frac{g_i}{1+exp(\frac{E-E_F}{k_BT})}</script><script type="math/tex; mode=display">k_B=1.381\times10^{-23}J/K</script>不同温度下的费米几率函数<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/14/d29e4da027a87ef6c12cf104431a617d.png" alt=""><br>费米能级EF：描述电子统计分布的物理量，量纲为eV</li><li>T = 0 K：<ul><li>E &gt; EF，f(E) = 0，完全没有电子</li><li>E &lt; EF，f(E) = 1，完全由电子占据</li></ul></li><li>T &gt; 0 K：<ul><li>E &gt; EF，f(E) &lt; 1/2</li><li>E &lt; EF，f(E) &gt; 1/2</li><li>E = EF，f(E) = 1/2</li></ul></li></ul><ul><li><p>在k空间，自由电子的等能面为球面， $E = E_F$的等能面为<br>费米面</p></li><li><p>$E_F = k_BT_F，T_F$为费米温度</p><script type="math/tex; mode=display">f_F(E)=\frac{1}{1+exp(\frac{E-E_F}{k_BT})}</script></li><li><p>对于本征半导体，$E_F$位于禁带中部</p><script type="math/tex; mode=display">E_F-E_v==E_C-E_F==0.5E_g</script></li><li>Eg ：1.6 ~ 7 eV</li><li>半导体的费米温度TF ：$10^4 - 10^5 K$ </li><li>室温：$T = 300K，k_B<br>T = 4.142*10^{-21}J = 0.0259 eV$</li><li>价带顶: $exp(*) = e^{-30} - e^{-135}$<br>， f (E) - 1 ，价带主要由电子充满</li><li>导带底: $exp(*) = e^{30} - e^{135}<br>， f (E) ~ 0$，导带电子很少</li><li>$f_F(E)$和$1 - fF (E)$以$E_F$为对称</li><li>$f_F(E)$为粒子占据能带带几率（导带能级被电子占据的几率）</li><li>$1-f_F(E)$是能态未被电子占据的几（能态空占的几率）</li><li>对于本征半导体，从导带底部开始计算 $(E– E_F<br>)/(k_B<br>T ) &gt;&gt; 1$，此时分布可以近似为玻尔兹曼-爱因斯坦分布+.</li></ul><script type="math/tex; mode=display">f(E)=exp(-\frac{E-E_F}{k_BT})</script><h2 id="半导体中的载流子-1"><a href="#半导体中的载流子-1" class="headerlink" title="半导体中的载流子"></a>半导体中的载流子</h2><p>当T&gt;0K时，电子被热激发，价带电子进入导带，导带电子，价带空穴，使导带，价带都带电</p><h3 id="k空间能带结构"><a href="#k空间能带结构" class="headerlink" title="k空间能带结构"></a>k空间能带结构</h3><ul><li>导带抛物线近似<script type="math/tex; mode=display">E_C(k)=E_{C0}+\hbar^2k^2/(2m_n^*)</script></li><li>价带抛物线近似<script type="math/tex; mode=display">E_V(k)=E_{V0}-\hbar^2k^2/(2m_p^*)</script><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/14/7a812176c87ab3a53ba4df48dc57bc0b.png" alt=""><h3 id="导带电子浓度"><a href="#导带电子浓度" class="headerlink" title="导带电子浓度"></a>导带电子浓度</h3><script type="math/tex; mode=display">n_0=\int_{E_{C0}}^{E_{C1}}f(E)g_n(E)dE</script><script type="math/tex; mode=display">=N_Cexp(-\frac{E_{C0}-E_{F0}}{k_BT})</script><h4 id="导带有效态密度函数"><a href="#导带有效态密度函数" class="headerlink" title="导带有效态密度函数"></a>导带有效态密度函数</h4><script type="math/tex; mode=display">N_C=2(\frac{m_n^*k_BT}{2\pi \hbar^2})^{3/2}</script></li></ul><h3 id="价带空穴浓度"><a href="#价带空穴浓度" class="headerlink" title="价带空穴浓度"></a>价带空穴浓度</h3><script type="math/tex; mode=display">p_0=N_Vexp(-\frac{E_{F0}-E_{V0}}{k_BT})</script><h4 id="导带有效态密度函数-1"><a href="#导带有效态密度函数-1" class="headerlink" title="导带有效态密度函数"></a>导带有效态密度函数</h4><script type="math/tex; mode=display">N_V=2(\frac{m_p^*k_BT}{2\pi \hbar^2})^{3/2}</script><h3 id="费米能级"><a href="#费米能级" class="headerlink" title="费米能级"></a>费米能级</h3><script type="math/tex; mode=display">E_{F0}(T)=E_i+\frac{3}{4}k_BTIn(\frac{m_p^*}{m_n^*})</script><h3 id="禁带中心"><a href="#禁带中心" class="headerlink" title="禁带中心"></a>禁带中心</h3><p>T&gt;0K,费米能级<br>在禁带中部附近</p><script type="math/tex; mode=display">E_i=\frac{1}{2}(E_{V0}+E_{C0})</script><h3 id="禁带宽度"><a href="#禁带宽度" class="headerlink" title="禁带宽度"></a>禁带宽度</h3><script type="math/tex; mode=display">E_g=E_{C0}-E_{V0}</script><p>本征半导体平衡载流子浓度与温度、禁带宽<br>度bandgap energy有关，与费米能级无关</p><h3 id="本征半导体载流子"><a href="#本征半导体载流子" class="headerlink" title="本征半导体载流子"></a>本征半导体载流子</h3><script type="math/tex; mode=display">n_0=p_0=n_i=\sqrt{N_CN_V}exp(-\frac{E_g}{2k_BT})</script><div class="table-container"><table><thead><tr><th>材料</th><th>硅（Si）</th><th>砷化镓（GaAs）</th><th>锗（Ge）</th></tr></thead><tbody><tr><td>Nc (cm-3)</td><td>$2.8*10^{19}$</td><td>$4.7*10^{17}$</td><td>$1.04*10^{19}$</td></tr><tr><td>Nv (cm-3)</td><td>$ 1.04*10^{19}$</td><td>$7.0*10^{18}$</td><td>$6.0*10^{18}$</td><td></td></tr><tr><td>$m_n^*/ m0$</td><td>1.08</td><td>0.067</td><td>0.55</td></tr><tr><td>$m_p^*/m0$</td><td>0.56</td><td>0.48</td><td>0.37</td></tr><tr><td>ni (cm-3)</td><td>$1.5*10^{10}$</td><td>$1.8*10^6$</td><td>$2.4*10^{13}$</td></tr></tbody></table></div><h2 id="杂质半导体"><a href="#杂质半导体" class="headerlink" title="杂质半导体"></a>杂质半导体</h2><ul><li>浅能级杂质<ul><li>浅能级施主/受主杂质的能级离导带底/价带顶很近</li><li>浅能级施主原子很容易电离、施主离子很难俘获电子</li><li>浅能级受主原子很容易电离、受主离子很难俘获空穴</li><li>浅能级杂质的电离能可以用类氢原子模型近似分析</li></ul></li><li>深能级杂质<ul><li>深能级施主/受主杂质的能级离导带底/价带顶都很远</li><li>深能级杂质原子很难电离，很难为导带或价带提供载流子</li><li>深能级杂质的电离能不可以用类氢原子模型</li></ul></li></ul><h3 id="施主原子密度-N-D"><a href="#施主原子密度-N-D" class="headerlink" title="施主原子密度$N_D$"></a>施主原子密度$N_D$</h3><script type="math/tex; mode=display">n_d\approx gN_Dexp(-\frac{E_D-E_F}{k_BT})</script><h3 id="受主原子密度-N-A"><a href="#受主原子密度-N-A" class="headerlink" title="受主原子密度$N_A$"></a>受主原子密度$N_A$</h3><script type="math/tex; mode=display">p_a\approx gN_Aexp(-\frac{E_F-E_A}{k_BT})</script><h3 id="补偿半导体compensated-semiconductor："><a href="#补偿半导体compensated-semiconductor：" class="headerlink" title="补偿半导体compensated semiconductor："></a>补偿半导体compensated semiconductor：</h3><p>既掺施主杂质，也掺受主杂质</p><ul><li>ND&gt; NA，为n型半导体</li><li>ND&lt; NA，为p型半导体<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/14/52211896e09cbe71a682e3ad80ad0b32.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/14/da0d3f63662b352da5f653a54fb2790c.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/14/18b4fb2a725be83a5f3ef331f5ebc881.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/14/3e49cc66d0969f67a93b946eb2f3d5d6.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/14/a2a4c4f9f504301b5357cc359c7b9616.png" alt=""><h1 id="金属中的自由电子"><a href="#金属中的自由电子" class="headerlink" title="金属中的自由电子"></a>金属中的自由电子</h1>金属中的电子可以用自由电子气模型代替能带理论</li><li>单位空间体积中金属的能态密度<script type="math/tex; mode=display">g(E)=4\pi(2m_0/h^2)^{3/2}E^{1/2}</script></li><li>电子浓度<script type="math/tex; mode=display">n=\frac{8\pi}{3}(\frac{2m_0}{h^2})^{3/2}E_F^{3/2}</script></li><li>费米能级<script type="math/tex; mode=display">E_F\approx\frac{h^2}{2m_0}(\frac{3n}{8\pi})^{2/3}</script></li><li><p>E = EF的等能面为费米面</p><ul><li>自由电子的费米面：理想时为球面</li><li>实际的费米面：与球面有较大区别</li></ul></li><li><p>T = 0 K：</p><ul><li>E &gt; EF，完全没有电子</li><li>E &lt; EF，完全由电子占据（电子占满能态）</li></ul></li><li><p>T &gt; 0 K：</p><ul><li>EF - kBT &lt; E &lt; EF的电子跃迁到<br>EF<br>&lt; E &lt; EF+ kBT的能态上<h1 id="电阻率和温度"><a href="#电阻率和温度" class="headerlink" title="电阻率和温度"></a>电阻率和温度</h1><h2 id="金属电导率"><a href="#金属电导率" class="headerlink" title="金属电导率"></a>金属电导率</h2>金属中电子在外电场下做漂移运动,平均速度与电场强度成正比<script type="math/tex; mode=display">v_d=\mu E</script><script type="math/tex; mode=display">J=-nev_d</script><script type="math/tex; mode=display">\to \sigma=-ne\mu</script>杂质、缺陷、晶格振动使载流子收到散射</li></ul></li><li><p>散射使载流子恢复无规则热运动</p></li><li>平均漂移速度消失</li></ul><p>两次散射之间自由时间的平均值，称为弛豫时间τ</p><p>实际上，载流子速度不同，因而弛豫时间不同，迁移率也不同。<br>若考虑载流子速度统计分布的性质，需用统计理论计算τ</p><p>在费米面附近的电子对电导有贡献,所以实际电导为</p><script type="math/tex; mode=display">\sigma=\frac{ne^2\tau(E_F)}{m_n^*}</script><h2 id="半导体电阻率"><a href="#半导体电阻率" class="headerlink" title="半导体电阻率"></a>半导体电阻率</h2><p>半导体中含有两种载流子</p><script type="math/tex; mode=display">\sigma =ne\mu_n+pe\mu_p</script><p>载流子迁移率和浓度都是温度的函数,温度升高导致晶格振动载流子散射增强</p><ul><li>迁移率<script type="math/tex; mode=display">\mu=\mu_L+\mu_I</script><script type="math/tex; mode=display">\mu  \in T^{3/2}</script></li><li>低温<br>温度上升，迁移率由$\mu_I$主导，迁移率上升，电阻率下降</li><li>中温<br>温度上升，迁移率由$\mu_L$主导，迁移率下降，电阻率上升</li><li>高温<br>温度上升，在载流子浓度上升，迁移率下降，电阻率下降</li></ul><h1 id="半导体材料"><a href="#半导体材料" class="headerlink" title="半导体材料"></a>半导体材料</h1><ul><li>半导体材料：<ul><li>元素半导体：Si、Ge、Se（硒）、Te（碲）……<br>+化合物半导体：晶态、非晶态无机、有机化合物、氧化物</li></ul></li><li>双元素</li><li>三元素</li><li>四元素</li></ul><h3 id="非晶态半导体材料"><a href="#非晶态半导体材料" class="headerlink" title="非晶态半导体材料"></a>非晶态半导体材料</h3><ul><li>非晶Si（α-Si）半导体：太阳电池、场效应管（驱动液晶<br>显示，逻辑电路和图像传感器）</li><li>硫属玻璃半导体：奥氏（Ovshinsky）效应（奥氏阈值开关、<br>奥氏记忆开关）<ul><li>氧化物玻璃半导体：ITO导电玻璃</li></ul></li></ul><h1 id="固态电子效应"><a href="#固态电子效应" class="headerlink" title="固态电子效应"></a>固态电子效应</h1><ul><li>磁电效应</li><li>光电效应</li><li>热电效应</li><li>压电效应</li><li>声电效应</li><li>磁光效应</li><li>光磁电效应</li><li>热磁电效应</li><li>光磁电效应</li></ul><h2 id="磁电效应"><a href="#磁电效应" class="headerlink" title="磁电效应"></a>磁电效应</h2><ul><li>霍尔迁移率<script type="math/tex; mode=display">J_x==en\mu_HE_x</script><script type="math/tex; mode=display">\to \mu_H=I_xL/(enV_xWd)</script></li><li><p>测定霍耳系数RH(单位电流密度和单位磁感应强度产生的霍尔电场)：</p><script type="math/tex; mode=display">R_H = E_y/(J_xB_z) = (V_H/W)/[I_xB_z/(Wd)] = (V_Hd)/(I_xB_z)</script></li><li><p>确定半导体的导电类型：</p><ul><li>n型半导体：RH &lt; 0； </li><li>p型半导体：RH &gt; 0<h2 id="耿氏效应"><a href="#耿氏效应" class="headerlink" title="耿氏效应"></a>耿氏效应</h2><strong>速度饱和</strong>:velocity saturation:</li></ul></li></ul><p>n型GaAs，平均漂移速度vd并非<br>一直随电场强度上升而上升，有一个饱和值（电场强度3000<br>V/cm时，达到最大值）<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/14/e2a996e0ef096f3880b1b346768f70b2.png" alt=""></p><h1 id="固态电子能谱"><a href="#固态电子能谱" class="headerlink" title="固态电子能谱"></a>固态电子能谱</h1><p>我们使用单色光轰击样本，电子受到激发通过分析电子能量分布来获取信息</p><h2 id="俄歇电子能谱"><a href="#俄歇电子能谱" class="headerlink" title="俄歇电子能谱"></a>俄歇电子能谱</h2><script type="math/tex; mode=display">E_A=E_K-E_{L1}-E_{L2,3}-\phi</script><ul><li>$E_A$俄歇电子能量</li><li>$E<em>K-E</em>{L1}-E_{L2,3}$能级电子结合能</li><li>$\phi$样品功函数</li></ul>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> physical </tag>
            
            <tag> 学废了 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>量子力学</title>
      <link href="2021/02/23/liangzilixue/"/>
      <url>2021/02/23/liangzilixue/</url>
      
        <content type="html"><![CDATA[<h2 id="不要尝试去理解量子力学！！！"><a href="#不要尝试去理解量子力学！！！" class="headerlink" title="不要尝试去理解量子力学！！！"></a>不要尝试去理解量子力学！！！</h2><h1 id="量子力学原理"><a href="#量子力学原理" class="headerlink" title="量子力学原理"></a>量子力学原理</h1><h2 id="能量子"><a href="#能量子" class="headerlink" title="能量子"></a>能量子</h2><ul><li>普朗克常数<script type="math/tex; mode=display">h=6.626\times 10^{-34}J\cdot s</script></li></ul><h3 id="黑体辐射"><a href="#黑体辐射" class="headerlink" title="黑体辐射"></a>黑体辐射</h3><h4 id="斯特藩-玻尔兹曼定律"><a href="#斯特藩-玻尔兹曼定律" class="headerlink" title="斯特藩-玻尔兹曼定律"></a>斯特藩-玻尔兹曼定律</h4><p>总辐出度与温度的四次方成正比</p><script type="math/tex; mode=display">M(T)=\sigma T^4</script><h4 id="韦恩位移定律"><a href="#韦恩位移定律" class="headerlink" title="韦恩位移定律"></a>韦恩位移定律</h4><p>黑体辐射光谱的峰值频率与黑体温度成正比:</p><script type="math/tex; mode=display">v=CT</script><h2 id="波粒二象性"><a href="#波粒二象性" class="headerlink" title="波粒二象性"></a>波粒二象性</h2><h3 id="散射增强条件"><a href="#散射增强条件" class="headerlink" title="散射增强条件"></a>散射增强条件</h3><script type="math/tex; mode=display">d\sin\theta=n\lambda</script><h2 id="测不准原理"><a href="#测不准原理" class="headerlink" title="测不准原理"></a>测不准原理</h2><ul><li>动量测不准<script type="math/tex; mode=display">\Delta p\Delta x>=\hat{h}/2</script></li><li>能量时间测不准<script type="math/tex; mode=display">\Delta E\Delta t>=\hat{h}/2</script></li><li>角动量测不准<script type="math/tex; mode=display">\Delta P\Delta \varphi>=\hat{h}/2</script></li></ul><h2 id="没有人比我更懂量子力学。"><a href="#没有人比我更懂量子力学。" class="headerlink" title="没有人比我更懂量子力学。"></a>没有人比我更懂量子力学。</h2><h1 id="薛定谔波动方程"><a href="#薛定谔波动方程" class="headerlink" title="薛定谔波动方程"></a>薛定谔波动方程</h1><script type="math/tex; mode=display">\frac{-\hbar^2}{2m}\nabla^2\Psi(x,t)+U(x)\Psi(x,t)=j\hat{h}\frac{\partial\Psi(x,t)}{\partial t}</script><h2 id="一维非相对论薛定谔方程"><a href="#一维非相对论薛定谔方程" class="headerlink" title="一维非相对论薛定谔方程"></a>一维非相对论薛定谔方程</h2><script type="math/tex; mode=display">\frac{-\hbar^2}{2m}\frac{\partial^2\Psi(x,t)}{\partial x^2}+U(x)\Psi(x,t)=j\hat{h}\frac{\partial\Psi(x,t)}{\partial t}</script><p>对波函数进行变量分离</p><script type="math/tex; mode=display">\Psi(r,t)=\psi(r)\varphi(t)</script><h3 id="时间部分为正弦波形式"><a href="#时间部分为正弦波形式" class="headerlink" title="时间部分为正弦波形式"></a>时间部分为正弦波形式</h3><script type="math/tex; mode=display">\varphi(t)=exp(-j\frac{E}{\hbar   }t)=exp(-jwt)</script><script type="math/tex; mode=display">E=\hbar w</script><h2 id="定态薛定谔方程（与时间无关）"><a href="#定态薛定谔方程（与时间无关）" class="headerlink" title="定态薛定谔方程（与时间无关）"></a>定态薛定谔方程（与时间无关）</h2><script type="math/tex; mode=display">\frac{d\psi(x)}{dx^2}+\frac{2m}{\hbar^2}[E-U(x)]\psi(x)=0</script><blockquote><p>概率密度函数表示在给定时间和位置出现粒子的概率</p></blockquote><h3 id="波函数"><a href="#波函数" class="headerlink" title="波函数"></a>波函数</h3><script type="math/tex; mode=display">\Psi(r,t)=\psi(r)\varphi(t)=\psi(r)exp(-jwt)</script><h3 id="粒子在某时刻某位置出现的概率"><a href="#粒子在某时刻某位置出现的概率" class="headerlink" title="粒子在某时刻某位置出现的概率"></a>粒子在某时刻某位置出现的概率</h3><script type="math/tex; mode=display">\Psi^2=\psi^2</script><blockquote><p>粒子的几率密度只与位置有关，与时间无关</p></blockquote><h2 id="边界条件"><a href="#边界条件" class="headerlink" title="边界条件"></a>边界条件</h2><script type="math/tex; mode=display">\iiint_{-\infty}^{\infty}\Psi(r,t)\Psi^*(r,t)dxdydz=\iiint_{-\infty}^{\infty}|\psi(r,t)|^2dxdydz=1</script><h3 id="几率密度"><a href="#几率密度" class="headerlink" title="几率密度"></a>几率密度</h3><ul><li>$\psi(r)$有限、单值、连续</li></ul><h3 id="动量"><a href="#动量" class="headerlink" title="动量"></a>动量</h3><ul><li>$\nabla\psi(r)$有限、单值、连续<h3 id="能量"><a href="#能量" class="headerlink" title="能量"></a>能量</h3></li><li>$\nabla^2\psi(r)$有限</li></ul><blockquote><p>各种势场的薛定谔方程的参数往往由边界条件待定</p></blockquote><h1 id="薛定谔方程实例"><a href="#薛定谔方程实例" class="headerlink" title="薛定谔方程实例"></a>薛定谔方程实例</h1><h2 id="自由空间中电子"><a href="#自由空间中电子" class="headerlink" title="自由空间中电子"></a>自由空间中电子</h2><p>当$U(x)=0$</p><script type="math/tex; mode=display">\frac{d\psi(x)}{dx^2}+\frac{2m}{\hbar^2}E\psi(x)=0</script><h3 id="波函数解为"><a href="#波函数解为" class="headerlink" title="波函数解为"></a>波函数解为</h3><script type="math/tex; mode=display">\psi(x)=Aexp[\frac{j}{\hbar}x\sqrt{2mE}]+Bexp[-\frac{j}{\hbar}x\sqrt{2mE}]</script><script type="math/tex; mode=display">\varphi(t)=exp(-j\frac{E}{\hbar}t)=exp(-jwt)</script><script type="math/tex; mode=display">\Psi(x,t)=Aexp[\frac{j}{\hbar}(x\sqrt{2mE}-Et)]+Bexp[-frac{j}{\hbar}(x\sqrt{2mE}+Et)]</script><ul><li>圆频率<script type="math/tex; mode=display">w=E/\hbar</script></li><li>波数<script type="math/tex; mode=display">k=\sqrt{2mE}/\hbar</script></li><li>波长<script type="math/tex; mode=display">\lambda=h/\sqrt{2mE}</script></li><li>动量<script type="math/tex; mode=display">p=h/\lambda=\sqrt{2mE}=\hbar k</script></li></ul><script type="math/tex; mode=display">\Psi(x,t)\approx 2\cos\{0.5[(dk)x-(dw)t]\}\sin(kx-wt)</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/04/0ed88ae8c24060d6a50b85ce3901aa6f.png" alt=""></p><h2 id="无限深势井"><a href="#无限深势井" class="headerlink" title="无限深势井"></a>无限深势井</h2><p>当$U(x)_1=0$,$U(x)_2=\infty$<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/04/8152ce57263865160e057bf7c3f534d0.png" alt=""></p><h3 id="区2"><a href="#区2" class="headerlink" title="区2"></a>区2</h3><ul><li><p>薛定谔方程</p><script type="math/tex; mode=display">\frac{d\psi(x)}{dx^2}+\frac{2m}{\hbar^2}E\psi(x)=0</script></li><li><p>波数</p><script type="math/tex; mode=display">k=\sqrt{2mE}/\hbar</script></li><li>波函数<script type="math/tex; mode=display">\psi(x)=A_1\cos(kx)+A_2\sin(kx)</script><h3 id="区13"><a href="#区13" class="headerlink" title="区13"></a>区13</h3><script type="math/tex; mode=display">\psi(x)=0</script></li></ul><p>根据边界条件<br>解得</p><script type="math/tex; mode=display">A_1=0</script><script type="math/tex; mode=display">A_2=\sqrt{2/a}</script><script type="math/tex; mode=display">k=n\pi/a=\sqrt{2mE}/\hbar</script><ul><li>分立能级<script type="math/tex; mode=display">E_n=\frac{\hbar^2\pi^2}{2ma^2}n^2</script><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/05/b4ca1e6285cbbfda8fc5a193e952be46.png" alt=""></li></ul><h2 id="阶跃位函数"><a href="#阶跃位函数" class="headerlink" title="阶跃位函数"></a>阶跃位函数</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/05/aba080a1c40a69686a8f388c4205d70a.png" alt=""></p><h3 id="区1"><a href="#区1" class="headerlink" title="区1"></a>区1</h3><script type="math/tex; mode=display">\psi(x)=A_1exp(jk_1x)+B_1exp(-jk_1x)</script><script type="math/tex; mode=display">k_1=\sqrt{\frac{2mE}{\hat{h}^2}}</script><h3 id="区2-1"><a href="#区2-1" class="headerlink" title="区2"></a>区2</h3><script type="math/tex; mode=display">\psi(x)=A_2\exp(-k_2x)+B_2exp(k_2x)</script><script type="math/tex; mode=display">k_2=\sqrt{\frac{2m(U_0-E)}{\hat{h}^2}}</script><script type="math/tex; mode=display">B_2=0</script><p>根据边界条件 解得</p><script type="math/tex; mode=display">B_1=\frac{k_2+jk_1}{jk_1-k_2}A_1</script><script type="math/tex; mode=display">A_2=\frac{2jk_1}{jk_1-k_2}A_1</script><p>反射率R=1</p><blockquote><p>粒子有一定几率可以穿越II区，但最终都会掉头返回</p></blockquote><h1 id="势垒"><a href="#势垒" class="headerlink" title="势垒"></a>势垒</h1><ul><li>区1<script type="math/tex; mode=display">\psi_1=A_1exp(jk_1x)+B_1exp(-jk_1x)\\k_1=\sqrt{2mE}/\hbar</script></li><li>区2<script type="math/tex; mode=display">\psi_2=A_2exp(jk_2x)+B_2exp(-jk_2x)\\k_2=\sqrt{2m(U_0-E)}/\hbar</script></li><li>区3<script type="math/tex; mode=display">\psi_3=A_3exp(jk_3x)+B_3exp(-jk_3x)\\k_3=\sqrt{2mE}/\hbar</script><img src="C00878AC16F24FABA530D72BEC2DF953" alt="image"></li><li>透射率<script type="math/tex; mode=display">T\approx16(\frac{E}{U_0})(1-\frac{E}{U_0})exp(-2k_2a)</script></li><li>反射率<script type="math/tex; mode=display">R=1-T</script><blockquote><p>低能量粒子能渗入<br>或贯穿高能势垒</p></blockquote></li></ul><h1 id="原子的波动理论"><a href="#原子的波动理论" class="headerlink" title="原子的波动理论"></a>原子的波动理论</h1><h2 id="单电子原子"><a href="#单电子原子" class="headerlink" title="单电子原子"></a>单电子原子</h2><h3 id="质子和电子产生的位函数"><a href="#质子和电子产生的位函数" class="headerlink" title="质子和电子产生的位函数"></a>质子和电子产生的位函数</h3><script type="math/tex; mode=display">U(r)=\frac{-e^2}{4\pi \epsilon_0r}</script><h3 id="电子的能量"><a href="#电子的能量" class="headerlink" title="电子的能量"></a>电子的能量</h3><p>将势函数代入薛定谔方程，并转化为球坐标下进行求解，分离变量可以得到电子的能量</p><script type="math/tex; mode=display">E_n=\frac{-m_0e^4}{(4\pi\epsilon_0)^22\hbar^2n^2}=-13.6eV</script><ul><li>电子质量<script type="math/tex; mode=display">9.1 \times 10^{-31}</script></li><li>电子电荷量<script type="math/tex; mode=display">1.6 \times 10^{-19}</script></li></ul><h4 id="电子的量子数"><a href="#电子的量子数" class="headerlink" title="电子的量子数"></a>电子的量子数</h4><ul><li>n 主量子数<br>n=1,2,3…</li><li>l 角量子数<br>l=n-1,n-2…0</li><li>m 磁量子数<br>|m|=l,l-1…0</li><li>s自旋量子数<br>s=1/2<script type="math/tex; mode=display">L=\sqrt{l(l+1)}\hbar\\L_z=m\hbar\\S=\sqrt{s(s+1)}\hbar=\sqrt{3}\hbar /2</script></li></ul>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> physical </tag>
            
            <tag> 学废了 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>com theory</title>
      <link href="2021/02/23/com-theory/"/>
      <url>2021/02/23/com-theory/</url>
      
        <content type="html"><![CDATA[<h2 id="通讯的本质是准确或者近似地恢复在另一个点所选择的信息的过程"><a href="#通讯的本质是准确或者近似地恢复在另一个点所选择的信息的过程" class="headerlink" title="通讯的本质是准确或者近似地恢复在另一个点所选择的信息的过程"></a>通讯的本质是准确或者近似地恢复在另一个点所选择的信息的过程</h2><span id="more"></span><p><br></p><h1 id="信道和其分类"><a href="#信道和其分类" class="headerlink" title="信道和其分类"></a>信道和其分类</h1><ul><li>物理信道：噪声信道，干扰信道，衰落信道，存储信道</li><li>按输入/输出信号在幅度和时间上取值的连续性<ul><li>幅度离散，时间离散信道</li><li>幅度连续，时间离散信道</li><li>幅度离散，时间连续信道</li><li>幅度连续，时间连续信道</li></ul></li><li>按输入/输出的记忆性<ul><li>有记忆信道</li><li>无记忆信道</li></ul></li><li>输入/输出信号的关系确定性<ul><li>确定信道</li><li>随机信道</li></ul></li></ul><h1 id="信道抽象模型"><a href="#信道抽象模型" class="headerlink" title="信道抽象模型"></a>信道抽象模型</h1><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/02/79fe2a72eec7d9e9b08cb360f8f046c0.png" alt=""></p><ul><li>离散无记忆信道<script type="math/tex; mode=display">p_N(y^N/x^N)=\prod_{n=1}^Np(y_n|x_n),\forall N</script></li><li>平稳信道<script type="math/tex; mode=display">p(y=_n|x_n=k)=p(y_m|x_m),\forall n,m</script>记为<script type="math/tex; mode=display">\{\chi;p(y|x);Y\}</script><h1 id="互信息容量"><a href="#互信息容量" class="headerlink" title="互信息容量"></a>互信息容量</h1><script type="math/tex; mode=display">\lim_{n\to \infty}\frac{1}{n}\max_{\{Q(x^n)\}}I(X_1X_2...X_n;Y_1Y_2...Y_n)</script></li></ul><blockquote><p>信道容量定义为每次利用信道，在输入和输出符号之间所能提供的互信息的最大值的极限</p></blockquote><h1 id="离散无记忆信道容量"><a href="#离散无记忆信道容量" class="headerlink" title="离散无记忆信道容量"></a>离散无记忆信道容量</h1><script type="math/tex; mode=display">I(X_1X_2...X_n;Y_1Y_2...Y_n)<=\sum_{i=1}^nI(X_i;Y_i)</script><blockquote><p>等号在输入为独立随机序列时达到</p></blockquote><script type="math/tex; mode=display">I(X_1X_2...X_n;Y_1Y_2...Y_n)<=\sum_{i=1}^nI(X_i;Y_i)=nI(X;Y)</script><p>从而</p><script type="math/tex; mode=display">C=\max_{\{Q_k\}}I(X;Y)</script><blockquote><p>${Q_k}$为X的概率分布 </p></blockquote><h2 id="无噪信道"><a href="#无噪信道" class="headerlink" title="无噪信道"></a>无噪信道</h2><p>由于信道是无噪的,从接收到的Y可完全确定X,所以</p><script type="math/tex; mode=display">H(X|Y)=0</script><p>从而</p><script type="math/tex; mode=display">I(X;Y)=H(X)</script><p>当输人入分布为等概分布时H(X)达到最大,即</p><script type="math/tex; mode=display">C=\max_{\{Q_k\}}I(X;Y)\\=\max_{\{Q_k\}}H(X)\\\sum_{i=1}^M-\frac{1}{M}\log \frac{1}{M}\\=\log M</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/02/db804b6d7bf3152f1755e443e1a0c82c.png" alt=""></p><h2 id="无损信道"><a href="#无损信道" class="headerlink" title="无损信道"></a>无损信道</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/02/3da4f36b4f192c090769fb0e3b2995f9.png" alt=""></p><script type="math/tex; mode=display">C=\log M</script><h2 id="确定信道"><a href="#确定信道" class="headerlink" title="确定信道"></a>确定信道</h2><p>我们不知道发射端端信息，但知道落在那个集合，存在一定的疑义度</p><script type="math/tex; mode=display">C=\log m</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/02/3a46b6a6ca1d6225614124ac2c8547e1.png" alt=""></p><h2 id="无用信道"><a href="#无用信道" class="headerlink" title="无用信道"></a>无用信道</h2><p>无用信道的特征是对任何输人分布,<code>I(X;Y)=0</code>,也就是说信道容量<code>C</code>为零。等效地要求对任何输入分布<code>H(X|Y) = H(X)</code>,也等价地要求随机变量<code>X</code>和<code>Y</code>彼此独立,所以</p><script type="math/tex; mode=display">C=0</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/02/6e71f815e11b8d976606c7a7ae45c67e.png" alt=""></p><h2 id="二进制对称信道（BSC）"><a href="#二进制对称信道（BSC）" class="headerlink" title="二进制对称信道（BSC）"></a>二进制对称信道（BSC）</h2><script type="math/tex; mode=display">I(X;Y)<=1-H(p)</script><blockquote><p>输入等概率取等号</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/02/6e71f815e11b8d976606c7a7ae45c67e.png" alt=""></p><blockquote><p>$H(p)$为二元分布的熵</p></blockquote><h2 id="二进制删除信道（BEC）"><a href="#二进制删除信道（BEC）" class="headerlink" title="二进制删除信道（BEC）"></a>二进制删除信道（BEC）</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/02/183f1b36026cb9df2f177d839ea0f62a.png" alt=""></p><script type="math/tex; mode=display">C=\max_{\{Q_k\}} H(Y)-H(p)\\H(Y)=H(p)+(1-p)H(q)\\C=\max_q(1-p)H(q)\\=1-p</script><blockquote><p>当输入为等概率分布时，等号成立</p></blockquote><h2 id="BEC和BSC比较"><a href="#BEC和BSC比较" class="headerlink" title="BEC和BSC比较"></a>BEC和BSC比较</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/02/594f68f9af9709af77f51da525fa06fb.png" alt=""></p><h1 id="离散无记忆信道容量定理"><a href="#离散无记忆信道容量定理" class="headerlink" title="离散无记忆信道容量定理"></a>离散无记忆信道容量定理</h1><p>信道容量问题可以看做约束优化问题</p><script type="math/tex; mode=display">C=\max_{\{Q_k\}}I(X;Y)\\s.t.\\\begin{cases}Q_k>=0&k=0,1,2...,K-1\\\sum_kQ_k=1\end{cases}</script><p>概率分布${Q<em>0,Q_1…Q</em>{K-1}}$达到转移概率为${p(j|k)}$的离散无记忆信道容量C的充要条件为:</p><script type="math/tex; mode=display">\begin{array}{l}I(X=k;Y)=C &\forall k,Q_k>0\\I(X=k;Y)<=C &\forall k,Q_k=0\end{array}</script><p>其中$I(X =k;Y)$表示通过信道传送字符<code>X =k</code>时,信道的输入与输出之间可获得的互信息的期望值，即</p><script type="math/tex; mode=display">I(X=k;Y)\sum_{j=0}^Jp(j|k)\log\frac{p(j|k)}{\sum_{i=0}^{K-1}Q_ip(j|i)}</script><p><strong>证明</strong></p><p>采用拉格朗日乘子法</p><script type="math/tex; mode=display">J\{Q_k\}=I(X;Y)-\lambda(\sum_{k=0}^{K-1}Q_k-1)\\\frac{\partial J\{Q_k\}}{\partial Q_k}\\=I(X=k;Y)-(1+\lambda)</script><script type="math/tex; mode=display">I(X=k;Y)=\begin{cases}1+\lambda&\forall Q_k>0\\<1+\lambda& \forall Q_k=0\end{cases}</script><p>令<br>$C=1+\lambda$</p><script type="math/tex; mode=display">I(X=k;Y)=\sum_{k=1}^{K+1}Q_kI(X=k;Y)=1+\lambda=C</script><h2 id="对称离散无记忆信道"><a href="#对称离散无记忆信道" class="headerlink" title="对称离散无记忆信道"></a>对称离散无记忆信道</h2><script type="math/tex; mode=display">P=\{p(j|k)\}=\begin{bmatrix}p(0|0)&p(1|0)&...&p(J-1|0)\\p(0|1)&p(1|1)&...&p(J-1|1)\\...&...&...&...\\p(0|K-1)&p(1|K-1)&...&p(J-1|K-1)\end{bmatrix}</script><p>若P中每一行都是第一行的一个<br>置换，则该信道关于输入对称。</p><script type="math/tex; mode=display">H(Y|X)=H(Y|X=k)\\=-\sum_{j=0}^{J_1}p(j|k)\log p(j|k)</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/02/89485e64332d499e8f037545ecc728d5.png" alt=""></p><p>若P中每一列都是第一列的一个<br>置换，则该信道关于输出对称</p><script type="math/tex; mode=display">\sum_{k=0}^{K_1}p(j|k)=\frac{K}{J}\\j=0,1,...J-1</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/02/89485e64332d499e8f037545ecc728d5.png" alt=""></p><blockquote><p>若一个信道既关于输入对称，又关于输出对称，即P中每一行都是第<br>一行的一个置换，每一列都是第一列的一个置换，则该信道是对称的</p><p>对一个信道的转移概率矩阵P按列划分，得到若干子信道，若划分出<br>的所有子信道均是对称的，则称该信道是准对称的</p></blockquote><p><strong>达到准对称离散无记忆信道容量的输入分布为均匀分布。</strong></p><ul><li>若信道关于输入对称</li></ul><script type="math/tex; mode=display">I(X;Y)=H(Y)+\sum_{j=0}^{J-1}p(j|k)\log p(j|k)</script><ul><li><p>若信道同时也关于输出对称（即信道对称）</p><script type="math/tex; mode=display">C=\log J+\sum_{j=0}^{J-1}p(j|k)\log p(j|k)</script></li><li><p>若信道只关于输入对称</p><script type="math/tex; mode=display">C<=\log J+\sum_{j=0}^{J-1}p(j|k)\log p(j|k)</script></li></ul><h2 id="k元对称信道"><a href="#k元对称信道" class="headerlink" title="k元对称信道"></a>k元对称信道</h2><script type="math/tex; mode=display">p(j|k)=\begin{cases}1-p&k=j\\\frac{p}{K-1}&k\not = j\end{cases}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/02/cc65e07cc2170bea6d2798cf60a74371.png" alt=""></p><script type="math/tex; mode=display">C=\log K-H(p)-p\log(K-1)</script><h2 id="二进制删除信道"><a href="#二进制删除信道" class="headerlink" title="二进制删除信道"></a>二进制删除信道</h2><script type="math/tex; mode=display">P=\begin{bmatrix}1-p-q&q&p\\p&q&1-p-q\end{bmatrix}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/02/9b1e0df50d4b33eaa129fccc7feb8eaa.png" alt=""></p><script type="math/tex; mode=display">C = I(X = 0;Y ) = I(X = 1;Y )\\=(1-p-q)\log(1-p-q)+p\log p-(1-q)\log\frac{(1-q)}{2}</script><h2 id="模k加法信道"><a href="#模k加法信道" class="headerlink" title="模k加法信道"></a>模k加法信道</h2><script type="math/tex; mode=display">Y=X+ZmodK</script><script type="math/tex; mode=display">X,Y,Z \in \{0,1,2...K-1\}</script><p>p(z)为任意分布</p><p>此时的转移概率为p(z)</p><script type="math/tex; mode=display">H(Y|X)=H(z)</script><p>所以</p><script type="math/tex; mode=display">C=\log K-H(z)</script><h2 id="转移概率矩阵可逆信道容量计算"><a href="#转移概率矩阵可逆信道容量计算" class="headerlink" title="转移概率矩阵可逆信道容量计算"></a>转移概率矩阵可逆信道容量计算</h2><p>当转移概率矩阵P可逆，信道容量可以估计</p><p>假定输入字符概率$Q_k&gt;0, k =0,1,…,K -1$,是达到信道容量的分布</p><script type="math/tex; mode=display">I(X=k;Y)=\sum_j p(j|k)\log\frac{p(j|k)}{\sum_iQ_ip(j|i)}=C,  k=0,1...K-1</script><p>令$w<em>j=\sum</em>{i=0}^{K-1}Q_ip(j|k)$</p><script type="math/tex; mode=display">\sum_{j=0}^{K-1}p(j|k) \log p(j|k)-\sum_{j=0}^{K-1}\log w_j=C\to \sum_{j=0}^{K-1}[C+\log w_j]=\sum_{j=0}^{K-1}p(j|k) \log p(j|k)</script><p>令</p><script type="math/tex; mode=display">\beta_j=C\log w_j\\\to  w_j=2^{\beta_j-C}</script><p>由约束条件$\sum w_j=1$</p><p>可以解出</p><script type="math/tex; mode=display">C=\log\sum_j 2^{\beta_j}</script><p>进一步由$w<em>j=\sum</em>{i=0}^{K-1}Q_ip(j|k)$求出${Q_k<br>}$</p><h1 id="信道组合"><a href="#信道组合" class="headerlink" title="信道组合"></a>信道组合</h1><ul><li><p>平行信道</p></li><li><p>开关信道</p></li></ul><blockquote><p>index modulation</p></blockquote><ul><li>级联信道</li></ul><h2 id="平行信道"><a href="#平行信道" class="headerlink" title="平行信道"></a>平行信道</h2><p>我们一般假设两个信道完全独立无关</p><script type="math/tex; mode=display">p(jj'|kk')=p_1(j|k)p_2(j'|k')</script><script type="math/tex; mode=display">I(X_1X_2;Y_1Y_2)=H(Y_1Y_2)-H(Y_1Y_2|X_1X_2)\\=H(Y_1Y_2)-H(Y_1|X_1X_2)-H(Y_2|Y_1X_1X_2)\\<=H(Y_1)+H(Y_2)-H(Y_1|X_1)-H(Y_2|X_2)\\=I(X_1;Y_1)+I(X_2;Y_2)</script><p>等号在信道分别独立用最佳分布发送时成立</p><script type="math/tex; mode=display">C=C_1+C_2</script><h2 id="开关信道"><a href="#开关信道" class="headerlink" title="开关信道"></a>开关信道</h2><script type="math/tex; mode=display">p(j|k)=\begin{cases}p_1(j|k)&\\p_2(j|k)&\\0&other\end{cases}</script><script type="math/tex; mode=display">Q_k=\begin{cases}P_AQ_k^1&\\P_BQ_k^2&\\\end{cases}</script><script type="math/tex; mode=display">I(X;Y)=P_AI(X_1;Y_1)+P_BI(X_2;Y_2)+H(P_A;P_B)</script><script type="math/tex; mode=display">C=\max_{\{P_A,Q_k^1,Q_k^2\}}I(X;Y)</script><script type="math/tex; mode=display">C=\log[2^{C_1}+2^{C_2}]</script><p>其中</p><script type="math/tex; mode=display">P_A=2^{C_1-C}\\P_B=2^{C_2-C}</script><h2 id="级联信道"><a href="#级联信道" class="headerlink" title="级联信道"></a>级联信道</h2><script type="math/tex; mode=display">I(X;Y)>=I(X:Y)\\I(Y;Z)>=I(X;Y)\\C<=\min\{C_1,C_2\}</script><h1 id="离散无记忆信道编码"><a href="#离散无记忆信道编码" class="headerlink" title="离散无记忆信道编码"></a>离散无记忆信道编码</h1><ul><li>编码速率<script type="math/tex; mode=display">R=\frac{\log M}{n}\to C</script></li></ul><h2 id="基本定义"><a href="#基本定义" class="headerlink" title="基本定义"></a>基本定义</h2><p>离散无记信道<code>(X,p(y/x),Y)</code>上的一个<code>(M,n)</code>码由如下组成:</p><ul><li>一个与M个消息相对应的标号集合<code>&#123;1,2,… ,M &#125;</code>:</li><li>一个编码函数$X^n(.):{1,2,……,M}\to X^n$，所得到的码<br>字为$X^n(1),X^n(2),…X^n(M)$;一个码的全体码字构成码书。</li><li>译码函数$g(.):Y^n\to {1,2,M}$，对应于确定的译码<br>法则，帮助接收者根据接收到序列Y”来确定发送消息是什么。<h2 id="错误概率"><a href="#错误概率" class="headerlink" title="错误概率"></a>错误概率</h2></li><li>发送第i个消息所发生的错误概率<script type="math/tex; mode=display">\lambda_i=P\{g(Y^n)=\not i|X^n=X^n(i)\}</script></li><li>（M，n）码的最大错误概率定义为<script type="math/tex; mode=display">\lambda^{(n)}=\max \lambda_i</script></li><li>（M，n）码的平均错误概率定义为<script type="math/tex; mode=display">P_e^{(n)}=\frac{1}{M}\sum_{i=1}^M \lambda_i</script></li></ul><blockquote><p>速率可达性:<br>若存在一系列$(2^{nR}，n）$码，当$n\to \infty$时，最大错误概率$\lambda^{(n)}$，则R被称为可达的。</p></blockquote><h2 id="联合典型列"><a href="#联合典型列" class="headerlink" title="联合典型列"></a>联合典型列</h2><p>相对于联合分布p(x,y)的联合典型列集合$A_{\epsilon}^{(n)}$<br>是指具有下述性质的序列对$(x^n,y^n)$的集合</p><script type="math/tex; mode=display">A_{\epsilon}^{(n)}=\{(x^n,y^n)\in x^n\times Y^n\\|-\frac{1}{n}\log p(x^n)-H(X)|<\epsilon\\|-\frac{1}{n}\log p(y^n)-H(Y)|<\epsilon\\\{|-\frac{1}{n}\log p(x^n,y^n)-H(XY)|<\epsilon\}</script><h3 id="联合典型列性质"><a href="#联合典型列性质" class="headerlink" title="联合典型列性质"></a>联合典型列性质</h3><p>若联合序列$(X^n,Y^n)$的每一个符号对$(x_i,y_i)$均是按照联合分布<code>p(x,y)</code>独立选取，即$(X^n,y^n)\to p(x^n,y^n)$,则</p><ul><li>$Pr((X^n,Y^n)\in A_{\epsilon}^{(n)})\to 1$</li><li>$(1-\epsilon)2^{n(H(XY)-\epsilon)}\leq |A_{\epsilon}^{(n)}|\leq 2^{n(H(XY)+\epsilon)}$</li><li>如果联合序列$(\hat{X}^n,\hat{Y}^n)\to p(x^n,y^n)$,即$\hat{X}^n$和$\hat{Y}^n$分别按照<code>p(x,y)</code>的边缘分布<code>p(x)</code>和<code>p(y)</code>来独立选取，则</li></ul><script type="math/tex; mode=display">(1-\epsilon)2^{-n(I(X;Y)+3\epsilon)}\leq Pr((\hat{X}^n,\hat{Y}^n)\in A_{\epsilon}^{(n)})\leq 2^{-n(I(X;Y)-3\epsilon)}</script><script type="math/tex; mode=display">M\approx \frac{2^{nH(Y)}}{}</script><h2 id="离散无记忆信道编码-1"><a href="#离散无记忆信道编码-1" class="headerlink" title="离散无记忆信道编码"></a>离散无记忆信道编码</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/24/8a34822eb6c3ceabc53ee2b3f6310f17.png" alt=""></p><blockquote><p>随机选择$y^n$,约有$^{2-nI(X;Y)}$的概率与$x^n$构成联合典型</p></blockquote><script type="math/tex; mode=display">M\approx \frac{2^{nH(Y)}}{2^{nH(Y/X)}}= 2^{nH(Y)-nH(Y|X)}=2^{nI(X;Y)}\\R=\frac{\log M}{n}\approx I(X;Y)\to C</script><h3 id="信道编码定理"><a href="#信道编码定理" class="headerlink" title="信道编码定理"></a>信道编码定理</h3><p>所有低于信道容量C的速率R均是可达的,即当<code>R&lt;C</code>时，总存在一系列码$(2^{nR}, n)$,当$n \to \infty$时，最大误码概率元 $\lambda^{(n)}\to 0$。</p><h3 id="离散无记忆信道编码定理"><a href="#离散无记忆信道编码定理" class="headerlink" title="离散无记忆信道编码定理"></a>离散无记忆信道编码定理</h3><ul><li>渐进无差错准则<ul><li>所谓可靠通信是指随着码长增大，错误概率可以任意小，但并非为零;  </li><li>信道上不是仅传输一个符号，而是传输一串很长符号序列。由于多次使用信道，从而可以利用大数定律获得随机编码的统计性能</li></ul></li><li>随机编码<ul><li>计算在一类随机选择的码书上的平均错误概率。因此至少存在一个码<br>书（即一个编码），它的错误概率和在码书集合上计算的平均错误概<br>率一样好</li></ul></li><li>联合典型译码<ul><li>采用联合典型的准则进行译码，并且这样的译码准则是统计最优的<script type="math/tex; mode=display">P(E)=\sum_{B}P(B)P_e^(n)(B)=\sum_BP(B)\frac{1}{M}\sum_{w=1}^M\lambda_w(B)\\=\frac{1}{M}\sum_{w=1}^M\sum_BP(B)\lambda_w(B)=\sum_BP(B)\lambda_1(B)</script></li></ul></li></ul><h2 id="离散无记忆信道编码逆定理"><a href="#离散无记忆信道编码逆定理" class="headerlink" title="离散无记忆信道编码逆定理"></a>离散无记忆信道编码逆定理</h2><p>具有$λ^{(n)} \to 0$的任何$(2^{nR},n)$码必有 $R \leq C$</p><h2 id="信源和信道分离编码与联合编码"><a href="#信源和信道分离编码与联合编码" class="headerlink" title="信源和信道分离编码与联合编码"></a>信源和信道分离编码与联合编码</h2><ul><li>信源/信道分离编码<script type="math/tex; mode=display">if H<R_s<R_c<C \to P_e\to 0</script></li><li>信源、信道联合编码<ul><li>在有限集上取值的信源V的熵速率为<code>H(V)</code>. </li><li>若<code>H(V) &lt; C</code>,则存在一个信源信道联合编码，使得$P_e^{(n)} \to 0$;</li><li>反之,若<code>H(V) &gt; C</code>,则不可能以任意小的错误概率传送信源。</li></ul></li></ul><ul><li>只要<code>H&lt;C</code>,总可以找到可行的信源信道联合编码；也可以分别构造最优的信源编码和信道编码，使信息传输可达；</li><li>信源信道联合编码不能使得可行速率极限增加，但可以简化编码</li></ul><h1 id="时间离散的加性高斯信道"><a href="#时间离散的加性高斯信道" class="headerlink" title="时间离散的加性高斯信道"></a>时间离散的加性高斯信道</h1><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/24/0f26ed6ed170184854a7b09d0bb58567.png" alt=""></p><script type="math/tex; mode=display">Y_i=X_i+Z_i\\\frac{1}{n}\sum_{i=1}^nx_i^2\leq P</script><h2 id="高斯信道容量"><a href="#高斯信道容量" class="headerlink" title="高斯信道容量"></a>高斯信道容量</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/24/9c75366cc21df383ac5d2bfe2a937fb6.png" alt=""></p><script type="math/tex; mode=display">Y_i=X_i+Z_i\\X_i\in \{+\sqrt{P},-\sqrt{P}\}</script><h2 id="加性高斯信道的容量"><a href="#加性高斯信道的容量" class="headerlink" title="加性高斯信道的容量"></a>加性高斯信道的容量</h2><script type="math/tex; mode=display">C=\max_{p(x):EX^2\leq P}I(X;Y)</script><script type="math/tex; mode=display">I(X;Y)=h(Y)-h(Y|X)\\=h(Y)-h(X+Z|X)\\=h(Y)-h(Z|X)\\=h(Y)-h(Z)</script><script type="math/tex; mode=display">h(Z)=\frac{1}{2}\log 2\pi eN\\EY^2=E[(X+Z)^2]=P+N\\h(Y)<=\frac{1}{2}\log 2\pi e(P+N)</script><p>香农公式</p><script type="math/tex; mode=display">I(X;Y)<=\frac{1}{2} \log 2\pi e(P+N)-\frac{1}{2}\log 2\pi eN\\=\frac{1}{2}\log (1+P/N)</script><blockquote><p>当输入X为高斯分布时等号成立</p></blockquote><h2 id="高斯信道编码定理"><a href="#高斯信道编码定理" class="headerlink" title="高斯信道编码定理"></a>高斯信道编码定理</h2><p>在噪声方差为<code>N</code>,信号功率受限为<code>P</code>的加性高斯信道上，<strong>任何<code>R&lt;C</code>的速率均是可达的</strong>，其中</p><script type="math/tex; mode=display">C=\frac{1}{2}\log (1+P/N)</script><p><strong>在高斯信道上任何<code>R &gt;C</code>的速率均是不可达的</strong></p><h2 id="高斯平行信道"><a href="#高斯平行信道" class="headerlink" title="高斯平行信道"></a>高斯平行信道</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/24/08ec95cd5f84135c4ad9c4519c3ddbf8.png" alt=""></p><script type="math/tex; mode=display">C=\frac{1}{2}\sum_{i=1}^k\log(1+\frac{(v-N_i)^+}{N_i})</script><h2 id="注水法则"><a href="#注水法则" class="headerlink" title="注水法则"></a>注水法则</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/12/24/01356d2e317a35714e0228b12f65f11b.png" alt=""></p><h2 id="模拟高斯信道容量"><a href="#模拟高斯信道容量" class="headerlink" title="模拟高斯信道容量"></a>模拟高斯信道容量</h2><script type="math/tex; mode=display">y(t)=x(t)+z(t)</script><p><code>x(t),z(t)</code>的带宽均限制在<code>[0,W ]Hz</code>之内。连续信号的离散化表示（Shannon抽样/Nyquist抽样）</p><script type="math/tex; mode=display">f(t)=\sum_{n=-\infty}^{\infty}f(\frac{n}{2W})\sin 2\pi W(t-\frac{n}{2W})</script><p>考虑T秒钟的模拟传输过程，它相当于2WT次平行的传输。设每次传输时输入样本X,的方差分别为P,,则由功率限制</p><script type="math/tex; mode=display">\sum_{i=1}^{2WT} P_i \leq PT</script><p>T秒钟的传输容量：</p><script type="math/tex; mode=display">C_T=\frac{1}{2}\sum_{i=1}^{2WT}\log (1+\frac{P_i}{N_i})\\N_i=\frac{N_0}{2}\\P_i=(v-\frac{N_0}{2})^+=\frac{P}{2W}\\C_T=WT \log\{1+\frac{P}{N_0W }\}</script><p>等效地，每秒钟的容量</p><script type="math/tex; mode=display">C=W\log \{1+\frac{P}{N_0W}\}bit/s</script><h2 id="Shannon带限信道容量定理的重要启示"><a href="#Shannon带限信道容量定理的重要启示" class="headerlink" title="Shannon带限信道容量定理的重要启示"></a>Shannon带限信道容量定理的重要启示</h2><script type="math/tex; mode=display">W\to \infty,C\to \frac{P}{N_0}\log e(bps)</script><ul><li>频率利用率<script type="math/tex; mode=display">\eta =\frac{R}{W}</script></li><li>比特传输能量<script type="math/tex; mode=display">E_b=\frac{P}{R}</script></li></ul>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 香农yyds </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>信息编码</title>
      <link href="2021/01/19/information-encoder/"/>
      <url>2021/01/19/information-encoder/</url>
      
        <content type="html"><![CDATA[<h1 id="信息获取的本质"><a href="#信息获取的本质" class="headerlink" title="信息获取的本质"></a>信息获取的本质</h1><script type="math/tex; mode=display"> 本体信息论-->信息获取 信息获取-->认识论信息</script> <span id="more"></span><h1 id="离散无记忆信源DMS的编码"><a href="#离散无记忆信源DMS的编码" class="headerlink" title="离散无记忆信源DMS的编码"></a>离散无记忆信源DMS的编码</h1><p> <strong>目标</strong>：在代价最小的意义上来有效表达一个信源，包括量化，压缩，映射，变化，自然语言翻译等许多抽象的过程</p><script type="math/tex; mode=display"> 信源编码 \begin{cases}无损编码    \begin{cases}    绝对无差错编码:P_e^{(n)}=0\\    渐进无差错编码:lim_{n\to \infty}P_e^{(n)}\to n    \end{cases}\\有损编码 \end{cases}</script><h2 id="DMS编解码系统概念框图"><a href="#DMS编解码系统概念框图" class="headerlink" title="DMS编解码系统概念框图"></a>DMS编解码系统概念框图</h2><p> <img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/23/26e237ee0433ed430c1e00dd257c0ad0.png" alt=""></p><h2 id="绝对无差错编码"><a href="#绝对无差错编码" class="headerlink" title="绝对无差错编码"></a>绝对无差错编码</h2><ul><li><p>DMS</p><script type="math/tex; mode=display">...U_1,U_2,U_3...U_L...U_i == \begin{bmatrix}a_1&a_2&...&a_k\\p_1&p_2&...&p_k\end{bmatrix}</script></li><li>编码符号集<script type="math/tex; mode=display">\Beta=\{b_1,b_2,...b_D\}</script></li><li>对于源U的任意L长序列用编码符号集$\Beta$进行绝对无差错等长编码，则必有<script type="math/tex; mode=display">D^N>=K^L</script><strong>编码速率</strong></li></ul><script type="math/tex; mode=display">R=\frac{N\log D}{L}>=\log K</script><blockquote><p>平均每个信源符号编码所需时间</p></blockquote><ul><li>信源熵<script type="math/tex; mode=display">H(U)=H(p_1,p_2...p_k)=-\sum_{k=1}^K p_k\log p_k</script><script type="math/tex; mode=display">R=\frac{N\log D}{L}\to H(U)</script></li></ul><h2 id="AEP性质-渐进等分"><a href="#AEP性质-渐进等分" class="headerlink" title="AEP性质(渐进等分)"></a>AEP性质(渐进等分)</h2><p>离散无记忆信源U输出一个L长的序列</p><script type="math/tex; mode=display">u^L=(u_1,u_2,u_3...u_L)</script><p>因为无记忆性，所以</p><script type="math/tex; mode=display">p(u^L)=\prod_{l=1}^Lp(u_l)</script><ul><li>序列的信息为<script type="math/tex; mode=display">I(u^L)=-\log p(u^L)=\sum_{l=1}^L I(u_l)</script></li><li>每个符号平均自信息为<script type="math/tex; mode=display">I_L=\frac{I(u^L)}{L}</script>期望方差性质<script type="math/tex; mode=display">E(I_L)=E(I(u))=H(U)</script><script type="math/tex; mode=display">Var(I_L)=\frac{\sigma_l^2}{L}</script>由切比雪夫不等式<script type="math/tex; mode=display">P\{|\xi-E(\xi)|>\epsilon\}<=\frac{Var(\xi)}{\epsilon^2}</script><script type="math/tex; mode=display">p\{I_L-H(U)>\epsilon\}<=\frac{\sigma_I^2}{L\epsilon^2}</script>给定$\epsilon$，当L充分大时<script type="math/tex; mode=display">\frac{\sigma_L^2}{L\epsilon}<\epsilon</script>所以<script type="math/tex; mode=display">P\{|I_L-H(U)|>\epsilon\}<=\epsilon</script>这表明当L充分大的时候$\frac{1}{L}I(u^L)\to H(U)$</li></ul><p>这一性质被称为渐进等分性质</p><h2 id="典型列集合"><a href="#典型列集合" class="headerlink" title="典型列集合"></a>典型列集合</h2><script type="math/tex; mode=display">A_{\epsilon}^{(L)}(U)=\{u^L:|-\frac{1}{L}\log p(u^L)-H(U)|<\epsilon\}</script><p>为给定DMS输出长度为L的$\epsilon$ 典型列集合，简称典型列集，其中$u^L\in U^L$。</p><blockquote><p>典型列集合可以理解为序列的自信息接近信源熵的序列</p></blockquote><h3 id="典型列集合性质"><a href="#典型列集合性质" class="headerlink" title="典型列集合性质"></a>典型列集合性质</h3><ul><li>当L足够大时</li></ul><script type="math/tex; mode=display">\operatorname{Pr}\left(\boldsymbol{u}^{L} \in A_{\varepsilon}^{(L)}(U)\right)>1-\varepsilon</script><script type="math/tex; mode=display">\operatorname{Pr}\left(A_{\varepsilon}^{(L)}(U)\right)=\sum_{u^{L} \in A_{s}^{(L)}} p\left(\boldsymbol{u}^{L}\right)=\sum_{u^{L} \in U^{L}} p\left(\boldsymbol{u}^{L}\right) \mathrm{I}\left(\boldsymbol{u}^{L} \in A_{\varepsilon}^{(L)}(U)\right)</script><script type="math/tex; mode=display">=E\left[\mathrm{I}\left(\boldsymbol{u}^{L} \in A_{\varepsilon}^{(L)}(U)\right)\right]=\operatorname{Pr}\left(\boldsymbol{u}^{L} \in A_{\varepsilon}^{(L)}(U)\right)>1-\varepsilon</script><ul><li>典型列性质2<script type="math/tex; mode=display">2^{-L[H(U)+\varepsilon]} \leq p\left(\boldsymbol{u}^{L}\right) \leq 2^{-L[H(U)-\varepsilon]}, \quad \mathrm{i} \mathrm{f} \boldsymbol{u}^{L} \in A_{\varepsilon}^{(L)}(U)</script></li><li>典型列性质3</li></ul><script type="math/tex; mode=display">(1-\varepsilon) 2^{L[H(U)-\delta]} \leq\left|A_{\varepsilon}^{(L)}(U)\right| \leq 2^{L[H(U)+\varepsilon]}</script><h2 id="DMS编码定理"><a href="#DMS编码定理" class="headerlink" title="DMS编码定理"></a>DMS编码定理</h2><p>对熵为H(U)的离散无记忆信源所输出的L长序列进行等长编码,编码序列长为N，编码符号表包括 D个可用编码符号.则当<strong>平均每信源符号所耗用的编码比特数</strong>(编码速率)$R=\frac{N\log D}{L}$满足</p><script type="math/tex; mode=display">R>H(U)+\epsilon_L</script><p>可实现渐进无差错的编码</p><h1 id="不等长码"><a href="#不等长码" class="headerlink" title="不等长码"></a>不等长码</h1><p><strong>平均码长定义</strong></p><script type="math/tex; mode=display">\hat{n}=\sum_{k=1}^Kp_kn_k</script><div class="table-container"><table><thead><tr><th>信源消息</th><th>出现概率</th><th>码1</th><th>码2</th><th>码3</th><th>码4</th></tr></thead><tbody><tr><td>$a_1$</td><td>0.5</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>$a_2$</td><td>0.25</td><td>0</td><td>1</td><td>01</td><td>10</td></tr><tr><td>$a_3$</td><td>0.125</td><td>1</td><td>00</td><td>011</td><td>110</td></tr><tr><td>$a_4$</td><td>0.125</td><td>10</td><td>11</td><td>0111</td><td>1110</td></tr></tbody></table></div><h3 id="唯一可译性"><a href="#唯一可译性" class="headerlink" title="唯一可译性"></a>唯一可译性</h3><p><strong>如果一个码的扩展是非奇异的，则该码唯一可译</strong></p><p>所谓编码的后缀分解集就是如下递归构成的一系列集合，${S_i}$</p><h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><div class="table-container"><table><thead><tr><th>$S_0$</th><th>$S_1$</th><th>$S_2$</th><th>$S_3$</th><th>$S_4$</th><th>$S_5$</th><th>…</th></tr></thead><tbody><tr><td>0</td><td>2</td><td>1</td><td>0</td><td>1</td><td>0</td><td>…</td></tr><tr><td>10</td><td></td><td></td><td>2</td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>12</td><td></td><td></td><td>12</td><td></td><td>12</td><td></td><td></td></tr><tr><td>21</td><td></td><td></td><td>122</td><td></td><td>122</td><td></td><td></td></tr><tr><td>112</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>1122</td><td></td></tr></tbody></table></div><p><strong>编码唯一可译的充要条件是所有的后缀分解集都不包含码字</strong></p><h3 id="即时可译性"><a href="#即时可译性" class="headerlink" title="即时可译性"></a>即时可译性</h3><h2 id="异字头码"><a href="#异字头码" class="headerlink" title="异字头码"></a>异字头码</h2><p>如果一个码没有一个码字是其他码字的前缀，则该码被称为前缀码或者异字头码</p><p>我门可以用D元树构造异字头码</p><h3 id="Kraft定理"><a href="#Kraft定理" class="headerlink" title="Kraft定理"></a>Kraft定理</h3><p>Kraft定理阐明了什么样的码长可以构造一个唯一可译的码</p><p>存在长度为$n_1,n_2…n_k$的D元异字头码的充要条件为</p><script type="math/tex; mode=display">\sum_{k=1}^K D^{-n_k}<=1</script><p><strong>如果一个码可译，则Kraft不等式成立，存在一个同样长度的异字头码</strong></p><h2 id="不等长编码定理"><a href="#不等长编码定理" class="headerlink" title="不等长编码定理"></a>不等长编码定理</h2><p><strong>任何D元唯一可译码满足</strong></p><script type="math/tex; mode=display">\hat{n}>=\frac{H(U)}{\log D}</script><p><strong>证明</strong></p><script type="math/tex; mode=display">H(U)-\hat{n}\log D=-\sum_{k=1}^K(p_k\log p_k+p_kn_k\log D)\\=\sum_{k=1}^K p_k\log \frac{D^{-n_k}}{p_k}\\<=\sum_{k=1}^K p_k(\frac{D^{-n_k}}{p_k}-1)\\=(\sum_{k=1}^K D^{-n_k}-1)<=0</script><p><strong>任何唯一可译码必然满足Kraft不等式。</strong></p><blockquote><p>当所有码字都在叶节点上取到等号</p></blockquote><p><strong>一定存在一个D元的唯一可译码，使得</strong></p><script type="math/tex; mode=display">\hat{n}<=\frac{H(U)}{\log D}+1</script><h2 id="编码速率"><a href="#编码速率" class="headerlink" title="编码速率"></a>编码速率</h2><script type="math/tex; mode=display">H(U)+\log D>=R=\hat{n}\log D>=H(U)</script><h3 id="编码效率"><a href="#编码效率" class="headerlink" title="编码效率"></a>编码效率</h3><script type="math/tex; mode=display">\eta=\frac{H(U)}{R}</script><p>对于DMS</p><script type="math/tex; mode=display">\lim_{l\to\infty}R=H(U)</script><h2 id="不等长编码定理扩展"><a href="#不等长编码定理扩展" class="headerlink" title="不等长编码定理扩展"></a>不等长编码定理扩展</h2><p>对于长度为L的平稳遍历源$U^L$<br>有</p><script type="math/tex; mode=display">\frac{H(U^L)}{\log D}<=\hat{n}(U^L)<=\frac{H(U^L)}{\log D}+1</script><script type="math/tex; mode=display">\frac{H(U^L)}{L\log D}<=\hat{n}=\frac{\hat{n}(U^L)}{L}<\frac{H(U^L)}{L\log D}+1</script><p>若源离散无记忆，进一步有结论</p><script type="math/tex; mode=display">\frac{H(U)}{\log D}<=\hat{n}<\frac{H(U)}{\log D}+\frac{1}{L}</script><h1 id="最佳不等长编码（Huffman编码）"><a href="#最佳不等长编码（Huffman编码）" class="headerlink" title="最佳不等长编码（Huffman编码）"></a>最佳不等长编码（Huffman编码）</h1><p>那么我们如何可以得到一个平均码长最短的编码？</p><p><strong>最佳不等长码</strong>：在给定信源分布的情况下，在平均码长最短的意义上最佳</p><p>我们从最简单的二元最佳码看起</p><p>其符合性质</p><ul><li>出现概率越小的码越长</li><li>出现概率最小的两个消息对应码长相等，且其码字只有最后一位不同</li></ul><p>我们把消息出现的概率降序排列，码字长度升序排列</p><p>为什么出现概率最小的两个消息码长相等且只有一位相反？</p><p>因为没有一个码是另一个码的前缀</p><h2 id="辅助源"><a href="#辅助源" class="headerlink" title="辅助源"></a>辅助源</h2><p>对于辅助源的最佳编码也是</p><script type="math/tex; mode=display">U=\begin{bmatrix}a_1&a_2&...&a_{K-1}&a_K\\p_1&>=p_2&...&>=p_{K-1}&>=p_K\end{bmatrix}</script><script type="math/tex; mode=display">U'=\begin{bmatrix}a_1'=a_1&a_2'=a_2&...&a_{K-1}'=a_{K-1}+a_K\\p_1'=p_1&p_2'=p_2&...&p_{K-1}'=p_{K-1}+p_K\end{bmatrix}</script><p>将辅助源的概率最小的两个编码在结尾分别加上0和1</p><h3 id="可递归编码定理"><a href="#可递归编码定理" class="headerlink" title="可递归编码定理"></a>可递归编码定理</h3><p><strong>对辅助源U’对最佳编码也是对原始源的最佳编码</strong></p><h2 id="Huffman编码案例"><a href="#Huffman编码案例" class="headerlink" title="Huffman编码案例"></a>Huffman编码案例</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/06/5d4b1fbf107cd22072a0fe00e605489b.png" alt=""></p><h2 id="D元Huffman编码虚拟消息的合并"><a href="#D元Huffman编码虚拟消息的合并" class="headerlink" title="D元Huffman编码虚拟消息的合并"></a>D元Huffman编码虚拟消息的合并</h2><ul><li>若$K=(D-1)i+1$，则每次均有D个消息要合并，短标号得到充分利用</li><li>若$K=(D-1)i+M$,则每次必须增补D-M个概率为0的虚拟消息，使得最后一次合并仍然有D个消息，从而充分利用短标号<br>‘</li></ul><h2 id="Huffman编码的排序问题"><a href="#Huffman编码的排序问题" class="headerlink" title="Huffman编码的排序问题"></a>Huffman编码的排序问题</h2><p>在消息数量巨大的时候，对信息排序是不可能的，需要一些其他的算法</p><h1 id="Shannon编码"><a href="#Shannon编码" class="headerlink" title="Shannon编码"></a>Shannon编码</h1><p>对于给定信源</p><script type="math/tex; mode=display">U=\begin{bmatrix}a_1&a_2&...&a_{K-1}&a_K\\p_1&>=p_2&...&>=p_{K-1}&>=p_K\end{bmatrix}</script><script type="math/tex; mode=display">P_k=\sum_{i=1}^{k-1}p_i</script><p>用长度为</p><script type="math/tex; mode=display">l_k=[\log \frac{1}{p_k}]</script><p>的编码将$P_k$二进制展开([x]表示大于x的最小整数)，截取前$l_k$作为编码</p><script type="math/tex; mode=display">=0.c_1c_2c_3...c_{l_k-1}c_{l_k}...</script><p>第k个消息的自信息</p><h2 id="香农yyds"><a href="#香农yyds" class="headerlink" title="香农yyds!!!"></a>香农yyds!!!</h2><ul><li>唯一可译</li><li>异字头</li><li>融合自信息</li></ul><h2 id="Shannon编码的异字头性"><a href="#Shannon编码的异字头性" class="headerlink" title="Shannon编码的异字头性"></a>Shannon编码的异字头性</h2><h2 id="香农编码的效率"><a href="#香农编码的效率" class="headerlink" title="香农编码的效率"></a>香农编码的效率</h2><script type="math/tex; mode=display">H(U)<=\hat{n}=\sum_{k=1}^Nl_kp_k<H(U)+1</script><blockquote><p>和哈夫曼编码比，香农编码渐进收敛性差，但竞争性更好</p></blockquote><h2 id="Fano编码"><a href="#Fano编码" class="headerlink" title="Fano编码"></a>Fano编码</h2><p>对<br>U进行概率对分</p><script type="math/tex; mode=display">|\sum_{i=1}^kp_i-\sum_{i=k+1}^np_i|</script><p><strong>fano编码本质上是一个二叉决策树</strong></p><script type="math/tex; mode=display">\hat{n}<=H(U)+1-2p_n</script><blockquote><p>其中$p_n$为最小符号概率。<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/06/89cf705b8263a2aa7910d2a18caa2d5d.png" alt=""></p></blockquote><h2 id="Shannon-Fano-Elias编码"><a href="#Shannon-Fano-Elias编码" class="headerlink" title="Shannon-Fano-Elias编码"></a>Shannon-Fano-Elias编码</h2><p><strong>优点：不需要排序</strong></p><p>记</p><script type="math/tex; mode=display">\bar{F}(x)=\sum_{i<x}p(i)+\frac{1}{2}p(x)</script><script type="math/tex; mode=display">F(x)=\sum_{i<=x}p(i)</script><p>为累计概率分布</p><script type="math/tex; mode=display">l(x)=[\log\frac{1}{p(x)}]+1</script><script type="math/tex; mode=display">\bar{F}(x)-\bar{F}(x-1)>\bar{F}(x)-F(x-1)=\frac{p(x)}{2}>2^{-l(x)}</script><script type="math/tex; mode=display">[\bar{F}(x)]_{l(x)}=0.z_1z_2...z_{l(x)}</script><script type="math/tex; mode=display">\bar F(x)-[\bar F(x)]_{l(x)}<2^{-l(x)}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/06/f3af62ed01694b1ed6cf0737b73bcf9b.png" alt=""></p><h3 id="SFE编码效率"><a href="#SFE编码效率" class="headerlink" title="SFE编码效率"></a>SFE编码效率</h3><script type="math/tex; mode=display">\bar{n}=\sum_x p(x)l(x)=\sum_xp(x)\{\log\frac{1}{p(x)}]+1\}<H(U)+2</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/06/91c5fa240670b6274a891b58258aab9f.png" alt=""></p><h2 id="算术编码"><a href="#算术编码" class="headerlink" title="算术编码"></a>算术编码</h2><p>sequential encoding的思想</p><ul><li>基本思想<ul><li>SFE编码的直接推广（从单个符号到n长序列） </li></ul></li><li>关键点<ul><li>累计概率快速有效迭代算法</li></ul></li></ul><p><strong>定义</strong></p><script type="math/tex; mode=display">x^n=x_1x_2...x_n>y_1y_2...y_n</script><p>是指</p><script type="math/tex; mode=display">\sum_ix_i2^{-i}>\sum_i y_i2^{-i}</script><p>$x^n$对应的子区间</p><script type="math/tex; mode=display">F(x^n)=\sum_{y^n<x^n}p(y^n)\\\bar{F}(x^n)=F(x^n)+\frac{1}{2}p(x^n)\\C(x^n)=[\bar{F}(x^n)]_{l(x_n)}\\l(x^n)=[\log\frac{1}{p(x^n)}]+1</script><h3 id="算术编码的基本算法"><a href="#算术编码的基本算法" class="headerlink" title="算术编码的基本算法"></a>算术编码的基本算法</h3><p>算数编码的关键思想在于建立了一套计算信源出现概率$p(x^n)$和累计概率$F(x^n)$的算法</p><script type="math/tex; mode=display">P(T_{x_1x_2...x_{k-1}0})=\sum_{y_{k+1}...y_n}p(x_1x_2...x_{k-1}0y_{k+1}...y_{n})\\=p(x_1x_2...x_{k-1}0)</script><script type="math/tex; mode=display">F(x^n)=\sum_{y^n<x^n}p(y^n)\\=\sum_{T:在x^n左边的子树T}P(T)\\=\sum_{x_k=1}P(x_1x_2...x_{k-1}0)</script><p>比如，设二元消息$$</p><p>这样的好处是当序列长度从n变到n+1时，容易计算概率</p><script type="math/tex; mode=display">p(x^{n+1})=p(x^nx_{n+1})=p(x^n)p(x_{n+1})</script><script type="math/tex; mode=display">F(x^{n+1})=F(x^nx_{n+1})=\begin{cases}F(x^n)&x_{n+1}=0\\F(x^n)+p(x^n0)&x_{n+1}=1\end{cases}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/06/bfae6e21f0ee624501b043fecc7f76d0.png" alt=""></p><h2 id="通用信源编码"><a href="#通用信源编码" class="headerlink" title="通用信源编码"></a>通用信源编码</h2><p>针对未知概率分布的信息</p><script type="math/tex; mode=display">\bar n=-p\log p_0-(1-p)\log(1-p_0)</script><p>香农编码在不能准确获得信源分布的事性能恶化</p><h2 id="Z-L编码"><a href="#Z-L编码" class="headerlink" title="Z-L编码"></a>Z-L编码</h2><h2 id="序列的分解"><a href="#序列的分解" class="headerlink" title="序列的分解"></a>序列的分解</h2><p>在每个前面未出现的最短序列编号</p><p>1|0|11|01|010|00|10|…</p><h2 id="平稳信源编码"><a href="#平稳信源编码" class="headerlink" title="平稳信源编码"></a>平稳信源编码</h2><p>令$\epsilon$是任意小正数，对平稳有记忆信源${U^L,p(u^L)}$进行不等长D元编码，则总可以找到一个$L(\epsilon)$,当$L&gt;L(\epsilon)$平均编码码长 $\bar n$满足</p><script type="math/tex; mode=display">\frac{H(U|U^{\infty})}{\log D}<=\bar n<\frac{H(U|U^{\infty})}{\log D}+\epsilon</script><h2 id="马尔可夫源"><a href="#马尔可夫源" class="headerlink" title="马尔可夫源"></a>马尔可夫源</h2><script type="math/tex; mode=display">\frac{H{\infty}(U)}{\log D}<=\bar n<\frac{H_{\infty}(U)}{\log D}+\frac{1}{L}</script><script type="math/tex; mode=display">H_\infty(U)=\sum_{s\in S}q(S=s)H(U|S=s)=H(U|S)</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/06/79347e2c14f9116e3a9775a13f958919.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 香农yyds </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RL:model free</title>
      <link href="2020/11/23/RL-model-free/"/>
      <url>2020/11/23/RL-model-free/</url>
      
        <content type="html"><![CDATA[<p>当模型太大，无法通过迭代得到最佳策略，我们就需要使用model free RL<br><span id="more"></span></p><h1 id="Model-free-prediction"><a href="#Model-free-prediction" class="headerlink" title="Model-free prediction"></a>Model-free prediction</h1><h2 id="Monre-Carlo-policy-evaluation"><a href="#Monre-Carlo-policy-evaluation" class="headerlink" title="Monre Carlo policy evaluation"></a>Monre Carlo policy evaluation</h2><ul><li>Return: $G<em>t=R</em>{t+1}＋\gamma R<em>{t+2}+\gamma^2R</em>{t+3}+…$ under policy$\pi$ </li><li>$v^\pi(s)=E_{\tau-\pi}[G_t|s_t = s]$, thus expectation over trajectories $\tau$Tgenerated by following $\pi$</li><li>MC simulation: we can simply sample a lot of trajectories, computethe actual returns for all the trajectories, then average them</li><li>MC policy evaluation uses empirical mean return instead of expectedreturn</li><li>MC does not require MDP dynamics/rewards,no bootstrapping,anddoes not assume state is Markov.</li><li>Only applied to episodic MDPs (each episode terminates)</li></ul><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>To evaluate state v(s)</p><ul><li>Every time-step t that state s is visited in an episode,</li><li>Increment counter $N(s)\to N(s)＋1$</li><li>Increment total return $S(s)\to S(s)+G_t$</li><li>Value is estimated by mean return $v(s)= s(s)/N(s)$By law of large numbers,$v(s)\to v^\pi(s) $as$ N(s)\to \infty$</li></ul><blockquote><p>蒙特卡洛的核心思想是通过多次迭代，取每个状态value的均值</p></blockquote><ul><li>Collect one episode (S1,A1,R1,…. St)</li><li>For each state st with computed return Gt<script type="math/tex; mode=display">N(S_t) = N(S_t)+1\\v(S_t)=v(S_t)+ \frac{1}{N(S_t)}(G_t-v(S_t))</script></li><li>Or use a running mean (old episodes are forgotten). Good fornon-stationary problems.<script type="math/tex; mode=display">v(S_t)=v(S_t)+ \alpha (G_t-v(S_t))</script></li></ul><blockquote><p>$\alpha$在这里可以看做学习率</p></blockquote><h2 id="Temporal-Diffenence-learning"><a href="#Temporal-Diffenence-learning" class="headerlink" title="Temporal Diffenence learning"></a>Temporal Diffenence learning</h2><ul><li>Objective: learn $v_\pi$ online from experience under policy $\pi$ </li><li>Simplest TD algorithm: TD(0)<ul><li>Update $v(S<em>t)$ toward estimated retur$R</em>{t+1}+~v(St+1)$</li></ul></li></ul><script type="math/tex; mode=display">v(S_t)=v(S_t)+\alpha(R_{t+1}＋\gamma v(S_t+1)-v(S_t))</script><ul><li>$R<em>{t+1}+\gamma v(S</em>{t+1})$ is called TD target</li><li>$t= R_{t+1}+\gamma v(S_t+1)- v(S_t)$ is called the TD error</li><li>Comparison: Incremental Monte-Carlo<ul><li>Update $v(S_t)$ toward actual return $G_t$ given an episode i<script type="math/tex; mode=display">v(S_t)=v(S_t)+\alpha (G_t-v(S_t))</script>我们也使用可以n步的TD</li></ul></li></ul><h2 id="Difference-Between-TD-and-MC"><a href="#Difference-Between-TD-and-MC" class="headerlink" title="Difference Between TD and MC"></a>Difference Between TD and MC</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/23/9676b882093cadade8359eca8958efe9.png" alt=""></p><blockquote><p>TD可以随时进行学习，MC只有在走完决策树后才能学习</p></blockquote><ul><li>DP</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/23/69202588ffd098a3893f500cea74149e.png" alt=""></p><ul><li>MC<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/23/eaef614f49b808056ec38156a7daae71.png" alt=""></li><li>TD<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/23/511c9b8c72b33f8ca006705c0f7b1305.png" alt=""></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/23/f03b1182c7ce24e3f8c61a967b826bf0.png" alt=""></p><h1 id="Model-free-control"><a href="#Model-free-control" class="headerlink" title="Model free control"></a>Model free control</h1><h2 id="Monte-Carlo-with-epsilon-greedy-Exploration"><a href="#Monte-Carlo-with-epsilon-greedy-Exploration" class="headerlink" title="Monte Carlo with $\epsilon$greedy Exploration"></a>Monte Carlo with $\epsilon$greedy Exploration</h2><ul><li>Trade-off between exploration and exploitation (we will talk aboutthis in later lecture)</li><li>$\epsilon$Greedy Exploration: Ensuring continual exploration<ul><li>All actions are tried with non-zero probability</li><li>With probability 1 - $\epsilon$ choose the greedy action </li><li>With probability $\epsilon$choose an action at random</li></ul></li></ul><script type="math/tex; mode=display">\pi(a|s)=\begin{cases}\epsilon/|A|+1-\epsilon&a=argmax_{a\in A}Q(s,a\\ \epsilon/|A|&otherwise\end{cases}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/23/2683d4a0f6a898d41889c0f67c7c921e.png" alt=""></p><h2 id="Sarsa-On-Policy-TD-Control"><a href="#Sarsa-On-Policy-TD-Control" class="headerlink" title="Sarsa:On-Policy TD Control"></a>Sarsa:On-Policy TD Control</h2><ul><li>An episode consists of an alternating sequence of states andstate-action pairs:</li></ul><ul><li>$\epsilon$-greedy policy for one step, then bootstrap the action value function:</li></ul><script type="math/tex; mode=display">Q(S_t,A_t)=Q(S_t,A_t)+\alpha [R_{t+1}+Q(S_{t+1},A_{t+1})- Q(s_t,A_t)]</script><ul><li>The update is done after every transition from a nonterminal state $S_t$</li><li>TD target $\phi<em>t= R</em>{t+1}+Q(S<em>{t+1},A</em>{t+1})$</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/23/9bf392a71bd253f3681fb7c9011835f6.png" alt=""></p><h2 id="On-policy-Learning-off-policy-Learning"><a href="#On-policy-Learning-off-policy-Learning" class="headerlink" title="On policy Learning off-policy Learning"></a>On policy Learning off-policy Learning</h2><ul><li>On-policy learning: Learn about policy $\pi$ from the experiencecollected from $\pi$<ul><li>Behave non-optimally in order to explore all actions, then reduce the<br>exploration $\epsilon$-greedy</li></ul></li><li>Another important approach is off-policy learning which essentiallyuses two different polices:<ul><li>the one which is being learned about and becomes the optimal policy</li><li>the other one which is more exploratory and is used to generate<br>trajectories</li></ul></li><li>Off-policy learning: Learn about policy $\pi$ from the experience sampledfrom another policy $\pi$<ul><li>$\pi$: target policy</li><li>$\mu$: behavior policy</li></ul></li></ul><blockquote><p>off policy 会选择更加激进的策略，通过激进策略的反馈优化target policy</p></blockquote><h2 id="Off-policy-control-with-Q-Learning"><a href="#Off-policy-control-with-Q-Learning" class="headerlink" title="Off policy control with Q-Learning"></a>Off policy control with Q-Learning</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/23/93c9fb9ec4dc889090c0201723e9f052.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/23/822356e1e45ce92ff731c3165d46d116.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/23/37badb3a0c00f01fb73fdc2b2fb2bf88.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习基础和马尔科夫决策过程</title>
      <link href="2020/11/23/RL-base/"/>
      <url>2020/11/23/RL-base/</url>
      
        <content type="html"><![CDATA[<h1 id="强化学习基本过程"><a href="#强化学习基本过程" class="headerlink" title="强化学习基本过程"></a>强化学习基本过程</h1><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/26/50ca94f0a4326ee91f2dd480a8019e8e.png" alt=""></p><span id="more"></span><h2 id="强化学习基本要素"><a href="#强化学习基本要素" class="headerlink" title="强化学习基本要素"></a>强化学习基本要素</h2><ul><li>模型</li><li>政策</li><li>价值</li></ul><h2 id="深度学习不同点"><a href="#深度学习不同点" class="headerlink" title="深度学习不同点"></a>深度学习不同点</h2><ul><li>没有标签，只有反馈</li><li>学习的过程来自于试错</li><li>学习的反馈有延迟</li><li>动作会影响数据</li><li>观察数据有时间的关联</li></ul><h1 id="马尔科夫基本过程（MDP）"><a href="#马尔科夫基本过程（MDP）" class="headerlink" title="马尔科夫基本过程（MDP）"></a>马尔科夫基本过程（MDP）</h1><p>马尔科夫过程的下一状态只取决于当前状态</p><h2 id="马尔科夫奖励过程"><a href="#马尔科夫奖励过程" class="headerlink" title="马尔科夫奖励过程"></a>马尔科夫奖励过程</h2><ul><li>S：state</li><li>R: Reward,$R(s_t=s)$</li><li>Discount factor $\gamma\in [0,1]$</li><li>P:dynamics/transition model </li></ul><h3 id="Horizon"><a href="#Horizon" class="headerlink" title="Horizon"></a>Horizon</h3><ul><li>Number of maximum time steps in each episode</li><li>Can be infinite,otherwise called finite Markov (reward) Process</li></ul><h3 id="Return"><a href="#Return" class="headerlink" title="Return"></a>Return</h3><script type="math/tex; mode=display">G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}...\gamma^{T-t-1}R_T</script><blockquote><p>可以看出随着时间变化，奖励值会衰减，只有离开某个状态才能获得奖励，所以奖励来自于未来的状态</p></blockquote><h3 id="state-value-function-Vt-s-for-a-MRP"><a href="#state-value-function-Vt-s-for-a-MRP" class="headerlink" title="state value function Vt(s) for a MRP"></a>state value function Vt(s) for a MRP</h3><h4 id="Expected"><a href="#Expected" class="headerlink" title="Expected"></a>Expected</h4><script type="math/tex; mode=display">V_t(s)=E[G_t|s_t=s]\\=E[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}...\gamma^{T-t-1}R_T|s_t=s]</script><h3 id="Discount-Factor-gamma"><a href="#Discount-Factor-gamma" class="headerlink" title="Discount Factor $\gamma$"></a>Discount Factor $\gamma$</h3><p>可以作为强化学习的超参数调整</p><ul><li>当$\gamma=0$，奖励只取决于当前状态</li></ul><h2 id="Bellman-equation"><a href="#Bellman-equation" class="headerlink" title="Bellman equation"></a>Bellman equation</h2><p>Bellman方程描述了状态的迭代关系</p><script type="math/tex; mode=display">V(s)=R(s)+\gamma\sum_{s'\in S}P(s'|s)V(s')</script><p>也可以写为矩阵的形式<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/26/59d7a52b2f3c05290c0959d8c6aa3c77.png" alt=""><br>我们可以通过矩阵求逆的过程求出V</p><script type="math/tex; mode=display">V=R+\gamma PV\\\to V=(1-\gamma P)^{-1}R</script><p>矩阵求逆的计算量太大，所以我们一般用迭代的方法求解</p><ul><li>动态规划法</li><li>蒙特卡洛采样法</li><li>Temporal-Difference learning</li></ul><h3 id="蒙特卡洛法"><a href="#蒙特卡洛法" class="headerlink" title="蒙特卡洛法"></a>蒙特卡洛法</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/26/a55f81b89fcd2007ad93eddc865932e2.png" alt=""></p><h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/26/786e9d90dfae4df9e7f9d18318a693c5.png" alt=""></p><h2 id="马尔科夫决策过程"><a href="#马尔科夫决策过程" class="headerlink" title="马尔科夫决策过程"></a>马尔科夫决策过程</h2><p>增加了一个动作</p><ul><li>S：state</li><li>A: action</li><li>R: Reward,$R(s_t=s)$</li><li>Discount factor $\gamma\in [0,1]$</li><li>P:dynamics/transition model $P(s_{t+1}=s’|s_t=s,a_t=a$</li></ul><h3 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h3><ul><li>policy决定了当前采取的策略</li><li>Policy：$\pi(a|s)=P(a_t=a|a_t=s)$</li><li>Policies are stationary (time-independent)，$A_t～ \pi(a|s)$ for any t &gt; 0</li></ul><ul><li>Given an MDP $(S,A, P,R,\gamma)$ and a policy $\pi$</li><li>The state sequence S1, S2,… is a Markov process $(S, P^\pi)$</li><li>The state and reward sequence S1,R2,S2, R2,… is a Markov reward<br>process (S, PT,R”, ) where,</li></ul><script type="math/tex; mode=display">P^{\pi}\left(s^{\prime} \mid s\right)=\sum_{a \in A} \pi(a \mid s) P\left(s^{\prime} \mid s, a\right)\\R^{\pi}(s)=\sum_{a \in A} \pi(a \mid s) R(s, a)</script><blockquote><p>当policy$\pi$已知时，马尔科夫决策过程会转化为马尔科夫奖励过程</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/27/35e02eaa2e13b99ebf06d7482e031e6b.png" alt=""><br>马尔科夫决策过程的下一状态先由当前状态采取的决策决定</p><h3 id="State-Value-Function"><a href="#State-Value-Function" class="headerlink" title="State Value Function"></a>State Value Function</h3><script type="math/tex; mode=display">{v}^{\pi}(s)=\mathbb{E}_{\pi}\left[G_{t} \mid s_{t}=s\right]</script><h4 id="action-value-function"><a href="#action-value-function" class="headerlink" title="action-value function"></a>action-value function</h4><script type="math/tex; mode=display">q^{\pi}(s, a)=\mathbb{E}_{\pi}\left[G_{t} \mid s_{t}=s, A_{t}=a\right]</script><h4 id="状态价值函数和动作价值函数的关系"><a href="#状态价值函数和动作价值函数的关系" class="headerlink" title="状态价值函数和动作价值函数的关系"></a>状态价值函数和动作价值函数的关系</h4><script type="math/tex; mode=display">v^{\pi}(s)=\sum_{a \in A} \pi(a \mid s) q^{\pi}(s, a)</script><h3 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h3><script type="math/tex; mode=display">v^{\pi}(s)=E_{\pi}\left[R_{t+1}+\gamma v^{\pi}\left(s_{t+1}\right) \mid s_{t}=s\right]\\=\sum_{a \in A} \pi(a \mid s) q^{\pi}(s, a)\\=\sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{\pi}\left(s^{\prime}\right)\right)</script><hr><script type="math/tex; mode=display">q^{\pi}(s, a)=E_{\pi}\left[R_{t+1}+\gamma q^{\pi}\left(s_{t+1}, A_{t+1}\right) \mid s_{t}=s, A_{t}=a\right]\\=R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{\pi}\left(s^{\prime}\right)\\=R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s^{\prime}\right) q^{\pi}\left(s^{\prime}, a^{\prime}\right)</script><blockquote><p>$v^\pi$表示了采用policy$\pi$得到奖励的期望</p></blockquote><h2 id="马尔科夫决策过程的预测和控制"><a href="#马尔科夫决策过程的预测和控制" class="headerlink" title="马尔科夫决策过程的预测和控制"></a>马尔科夫决策过程的预测和控制</h2><ul><li>预测<ul><li>预测价值 </li></ul></li><li>控制<ul><li>寻找最佳策略 </li></ul></li></ul><h3 id="predition"><a href="#predition" class="headerlink" title="predition"></a>predition</h3><p>尝试所有策略，收敛后得到价值函数<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/27/99bf59fcb81437835eb58274c0722285.png" alt=""></p><h3 id="optimal-value-function-and-policy"><a href="#optimal-value-function-and-policy" class="headerlink" title="optimal value function and policy"></a>optimal value function and policy</h3><script type="math/tex; mode=display">v^*(s)=max_\pi v^\pi(s)\\\pi^* (s)=arg max_\pi v^\pi (s)</script><p><strong>如何寻找最佳的policy?</strong><br>最佳行为可以定义为</p><script type="math/tex; mode=display">\pi^{*}(a \mid s)=\left\{\begin{array}{ll}1, & \text { if } a=\arg \max _{a \in A} q^{*}(s, a) \\0, & \text { otherwise }\end{array}\right.</script><h3 id="policy-search"><a href="#policy-search" class="headerlink" title="policy search"></a>policy search</h3><p>策略搜索的方法主要有以下两种</p><h4 id="policy-iteration"><a href="#policy-iteration" class="headerlink" title="policy iteration"></a>policy iteration</h4><p>策略迭代算法有两个步骤</p><ul><li>估计当前政策价值函数</li><li>采用贪心算法改进策略</li></ul><script type="math/tex; mode=display">\pi'=greedy(v^\pi)</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/27/d62fb722c177cdcefdd042d6e18329ee.png" alt=""></p><h5 id="policy-improvwment"><a href="#policy-improvwment" class="headerlink" title="policy improvwment"></a>policy improvwment</h5><ul><li>计算当前策略价值<script type="math/tex; mode=display">q^{\pi_{i}}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{\pi_{i}}\left(s^{\prime}\right)</script></li><li>计算新政策价值<script type="math/tex; mode=display">\pi_{i+1}(s)=\underset{a}{\arg \max } q^{\pi_{i}}(s, a)</script><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/27/5ec4e37080c6df000eed9aef386056a0.png" alt=""></li></ul><script type="math/tex; mode=display">q^{\pi}(s,\pi'(s))=\max_{a\in A}q^\pi(s,a)</script><h4 id="value-iteration"><a href="#value-iteration" class="headerlink" title="value iteration"></a>value iteration</h4><script type="math/tex; mode=display">v(s)=\max_{a\in A}(R(s, a)+\gamma \sum_{s^{\prime} \in S}P\left(s^{\prime} \mid s, a\right) v\left(s^{\prime}\right))</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/27/d9028f7e8b382d4b3b0e56c32f302801.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>优化理论</title>
      <link href="2020/11/17/optimize/"/>
      <url>2020/11/17/optimize/</url>
      
        <content type="html"><![CDATA[<h1 id="实变量函数无约束优化的梯度分析"><a href="#实变量函数无约束优化的梯度分析" class="headerlink" title="实变量函数无约束优化的梯度分析"></a>实变量函数无约束优化的梯度分析</h1><blockquote><p>松弛序列:称序列 ${a<em>k}</em>{k=0}^{\infty},a_{k+1}<a_k$ 为松弛序列<span id="more"></span></p></blockquote><p>我们通过迭代求解优化问题的过程中，需要产生一个松弛序列</p><h2 id="单变量函数的平稳点和极值点"><a href="#单变量函数的平稳点和极值点" class="headerlink" title="单变量函数的平稳点和极值点"></a>单变量函数的平稳点和极值点</h2><ul><li>全局极小点</li><li>严格全局极小点</li><li>开邻域</li><li>闭邻域</li></ul><ul><li>极值点</li><li>平稳点</li><li>鞍点<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/09/527be89d210f131ebfa64e75a671caf5.png" alt=""></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/09/1ad971a39863a9684fc8cecb2d674e0d.png" alt=""></p><h1 id="无约束最小化问题的梯度分析"><a href="#无约束最小化问题的梯度分析" class="headerlink" title="无约束最小化问题的梯度分析"></a>无约束最小化问题的梯度分析</h1><p>给定一个实值目标函数</p><p>对于超定矩阵方程$Az=b$定义误差平方和</p><script type="math/tex; mode=display">J(z)=||Az-b||_b^b=(Az-b)^H(Az-b)</script><script type="math/tex; mode=display">\frac{\partial J}{\partial z}=z^HA^HAz-z^HA^Hb-b^HAz+b^Hb</script><script type="math/tex; mode=display">A^HAz-A^Hb=0</script><script type="math/tex; mode=display">\to z=(A^HA)^{-1}A^Hb</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/17/b4d7b184290b70acdc8ac2761680533b.png" alt=""></p><h2 id="曲率"><a href="#曲率" class="headerlink" title="曲率"></a>曲率</h2><p>标量$p^HHp$则称为函数f沿着方向p</p><p>在无约束最小化问题中，我们常用共轭梯度向量的负方向$-\nabla_{z^*}f(z)$作为更新方向</p><script type="math/tex; mode=display">z_k=z_{k-1}-\mu\nabla_{z^*}f(z),\mu>0</script><h1 id="凸优化理论"><a href="#凸优化理论" class="headerlink" title="凸优化理论"></a>凸优化理论</h1><h2 id="最陡下降法-SDA"><a href="#最陡下降法-SDA" class="headerlink" title="最陡下降法(SDA)"></a>最陡下降法(SDA)</h2><script type="math/tex; mode=display">\nabla x_k=-\nabla f(x_k)</script><script type="math/tex; mode=display">x_{k+1}=x_k-\mu_k\nabla f(x_k)</script><h2 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h2><script type="math/tex; mode=display">\nabla x_k=-(\nabla^2f(x_k))^{-1}\nabla f(x_k)</script><script type="math/tex; mode=display">x_k=x_{k-1}-\mu_k(\nabla^2 f(x_{k-1}))^{-1}\nabla f(x_{k-1})</script><script type="math/tex; mode=display">H=\nabla^2 f(X)</script><script type="math/tex; mode=display">(\nabla^2 f(x_k))^{-1}\nabla f(x_k)=\Delta x_k</script><ul><li>修正牛顿法<script type="math/tex; mode=display">（\nabla^2 f(x)+EI）\Delta x=\nabla f(x)</script><blockquote><p>梯度步长的选择$\mu_k$，可以是固定，也可以是不断衰减，比如$\mu_k=\frac{h}{\sqrt{k+1}}$ </p></blockquote></li></ul><h1 id="次梯度法"><a href="#次梯度法" class="headerlink" title="次梯度法"></a>次梯度法</h1><p>对于梯度部分不存在的函数,比如y=|x|,(x=0)</p><script type="math/tex; mode=display">x_k=x_{k-1}-u\nabla_xf(x_{k-1})</script><p>f(x)的泰勒展开为</p><script type="math/tex; mode=display">f(x+\Delta x)\approx f(x)+(\nabla f(x))^T\Delta x+(\Delta x)^TH\Delta x</script><p>我们的目的是找到一个次梯度向量g，使其满足</p><script type="math/tex; mode=display">f(y)>=f(x)+g^T(y-x)</script><p>比如</p><script type="math/tex; mode=display">f(x)=|x|</script><script type="math/tex; mode=display">g_i=\begin{cases}1&x_i>0\\-1&x_i<0\\[-1,1]&x_i=0\end{cases}</script><script type="math/tex; mode=display">x_k=x_{k-1}-ug(x_{k-1})</script><script type="math/tex; mode=display">=x_{k-1}-\frac{u}{||g(x_{k-1})||_2}g(x_{k-1})</script><h1 id="约束优化算法"><a href="#约束优化算法" class="headerlink" title="约束优化算法"></a>约束优化算法</h1><h2 id="Lagrangian乘子法"><a href="#Lagrangian乘子法" class="headerlink" title="Lagrangian乘子法"></a>Lagrangian乘子法</h2><ul><li>优化目标<script type="math/tex; mode=display">min f(x)</script></li><li>约束条件<script type="math/tex; mode=display">s.t. Ax=b</script>我们可以定义代价函数<script type="math/tex; mode=display">L(x,\lambda)=f(x)+\lambda^T(Ax-b)</script>对x，$\lambda$分别求偏导<script type="math/tex; mode=display">\begin{cases}\frac{\partial L}{\partial x}=0\\\frac{\partial L}{\partial \lambda}=Ax-b=0\end{cases}</script>更新法则为<script type="math/tex; mode=display">\begin{cases}x_{k+1}=argmin L(x,\lambda_k)\\\lambda_{k+1}=\lambda_k-\mu\nabla_{\lambda}L(x_k,\lambda_k)\end{cases}</script></li></ul><h2 id="罚函数法"><a href="#罚函数法" class="headerlink" title="罚函数法"></a>罚函数法</h2><p>通过罚函数，将约束优化问题变成反映目标函数和约束标间的合成函数的无约束极小化</p><p>考虑约束优化问题</p><script type="math/tex; mode=display">\min f_0(x)</script><script type="math/tex; mode=display">s.t. f_i(x)>=0</script><script type="math/tex; mode=display">h_j(x)=0</script><p>$x\in F$,F表示x的可行集</p><h3 id="加性罚函数"><a href="#加性罚函数" class="headerlink" title="加性罚函数"></a>加性罚函数</h3><script type="math/tex; mode=display">L(x)=f_0(x)+p(x)</script><script type="math/tex; mode=display">\begin{cases}p(x)=0&x\in F \\p(x)>0&x\in F\end{cases}</script><h3 id="乘性罚函数"><a href="#乘性罚函数" class="headerlink" title="乘性罚函数"></a>乘性罚函数</h3><script type="math/tex; mode=display">L(x)=f_0(x)p(x)</script><script type="math/tex; mode=display">\begin{cases}p(x)=1&x\in F \\p(x)>1&x\in F\end{cases}</script><h3 id="等式约束下罚函数定义"><a href="#等式约束下罚函数定义" class="headerlink" title="等式约束下罚函数定义"></a>等式约束下罚函数定义</h3><script type="math/tex; mode=display">p(x)=||h(x)||_2^2=\sum_{i=1}^q|h_i(x)|^2</script><p>可以看出p(x)具有以下性质</p><script type="math/tex; mode=display">p(x)=\begin{cases}=0& h_i(x)=0,i=1,...,q\\>0& h_i(x)=0,i=1,...,q\end{cases}</script><p>对于满足等式约束条件的x无影响，对于违背约束条件的点给予惩罚</p><h3 id="不等式约束"><a href="#不等式约束" class="headerlink" title="不等式约束"></a>不等式约束</h3><p>对于不等式约束</p><ul><li><p>外罚函数</p><script type="math/tex; mode=display">P(x)=\sum_i^m(max\{0,-f_i(x)\})^r</script><blockquote><p>外罚函数对于在可行集外部的所有点进行处罚</p></blockquote></li><li><p>内罚函数</p><script type="math/tex; mode=display">p(x)=\sum_{i=1}^m\frac{1}{f_i(x)}</script><p>或</p><script type="math/tex; mode=display">P(x)=\sum_{i=1}^m\frac{1}{f_i(x)}\log|f_i(x)|\\P(x)=\sum\frac{1}{f_i(x)^p}\\P(x)=\sum exp(\frac{1}{f_i(x)r})</script><blockquote><p>这种罚函数相当于在可行集边界 bnd(F)上树立起一道围墙，对于企图从可行内集 int(F)穿越到可行集边界 bnd(F)的点 a进行阻挡,故称为内罚函数(interior penalty function),也称障碍函数(barrier function)。</p></blockquote></li></ul><h2 id="混合罚函数"><a href="#混合罚函数" class="headerlink" title="混合罚函数"></a>混合罚函数</h2><p>等式和不等式约束优化的混合罚函数法</p><h3 id="混合外罚"><a href="#混合外罚" class="headerlink" title="混合外罚"></a>混合外罚</h3><script type="math/tex; mode=display">\min_x f_0(x)+\rho_1\sum(\max\{0,f_i(x)\})^2+\rho_2\sum|h_j(x)|^2</script><h3 id="混合内罚"><a href="#混合内罚" class="headerlink" title="混合内罚"></a>混合内罚</h3><script type="math/tex; mode=display">\min_x f_0(x)+\rho_1\frac{1}{-f_i(x)}|\log(-f_i(x)|+\rho_2\sum |h_j(x)|^2</script><h2 id="增广拉格朗日乘子法"><a href="#增广拉格朗日乘子法" class="headerlink" title="增广拉格朗日乘子法"></a>增广拉格朗日乘子法</h2><h3 id="Lagrangian乘子法的主要缺点"><a href="#Lagrangian乘子法的主要缺点" class="headerlink" title="Lagrangian乘子法的主要缺点"></a>Lagrangian乘子法的主要缺点</h3><ul><li>只有当约束优化问题具有局部凸结构时，对偶的无约束优化问题才是良好定义的，并且 Lagrangian乘子的更新$\lambda_{k+1}=\lambda_k +a_kh(x_k)$才有意义。</li><li>Lagrangian目标函数的收敛比较费时，因为 Lagrangian乘子的更新是一种上升迭代(ascent iteration)，只能适度地快速收敛。</li></ul><h3 id="罚函数法的不足"><a href="#罚函数法的不足" class="headerlink" title="罚函数法的不足"></a>罚函数法的不足</h3><ul><li>收敛慢，大的惩罚参数容易引起转化后的无约束优化问题的病态，从而造成算法的数值不稳定性。</li></ul><p>拉格朗日和罚函数的结合</p><h3 id="等式约束优化Lagrangian乘子法"><a href="#等式约束优化Lagrangian乘子法" class="headerlink" title="等式约束优化Lagrangian乘子法"></a>等式约束优化Lagrangian乘子法</h3><script type="math/tex; mode=display">J_P(x,\lambda)=f_0(x)+\lambda^Th(x)+P\psi(h(x))</script><script type="math/tex; mode=display">\begin{cases}x_k=x_{k-1}+\Delta x&-\mu\nabla_x L(x,\lambda)|_{x_{k-1},\lambda_{k-1}}\\\lambda_k=\lambda_{k-1}+\Delta \lambda&-\mu\nabla_x L(x,\lambda)|_{x_{k-1},\lambda_{k-1}}\end{cases}</script><h3 id="混合约束优化Lagrangian乘子法"><a href="#混合约束优化Lagrangian乘子法" class="headerlink" title="混合约束优化Lagrangian乘子法"></a>混合约束优化Lagrangian乘子法</h3><p>对于优化问题</p><script type="math/tex; mode=display">\min_xf(x)s.t.Ax=b,Bx<=b</script><p>令非负向量s&gt;=0，为松弛变量，使得Bx+s=h,将问题转化为等式约束</p><script type="math/tex; mode=display">L_p(x,s,\lambda,v)=f(x)+\lambda^T(Ax-b)+V^H(Bx+s-h)+\rho(||Ax-b||_2^2+||Bx+s-h||_2^2)</script>]]></content>
      
      
      <categories>
          
          <category> 《柯西的传世绝学》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 矩阵论 </tag>
            
            <tag> 优化理论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>矩阵微分</title>
      <link href="2020/11/16/matrix-partial/"/>
      <url>2020/11/16/matrix-partial/</url>
      
        <content type="html"><![CDATA[<h1 id="雅可比矩阵和矩阵微分"><a href="#雅可比矩阵和矩阵微分" class="headerlink" title="雅可比矩阵和矩阵微分"></a>雅可比矩阵和矩阵微分</h1><h2 id="实值函数分类"><a href="#实值函数分类" class="headerlink" title="实值函数分类"></a>实值函数分类</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/11/16/a8eb220fbe1dcb1de74a1ede789e8738.png" alt=""></p><h2 id="雅可比矩阵"><a href="#雅可比矩阵" class="headerlink" title="雅可比矩阵"></a>雅可比矩阵</h2><p>在向量分析中，雅可比矩阵是函数以一阶偏导数以一定方式排列成的矩阵，其行列式称为雅可比行列式<br><span id="more"></span></p><ul><li>$1\times m$行向量偏导算子定义为<script type="math/tex; mode=display">D_x\overset{def}{=}\frac{\partial}{\partial x^T}=[\frac{\partial}{\partial x_1}...\frac{\partial}{\partial x_m}]</script>所以实值标量函数f（x）在x的偏导向量由$1\times m$行向量给出</li></ul><script type="math/tex; mode=display">D_x f(x)=\frac{\partial f(x)}{\partial x^T}</script><script type="math/tex; mode=display">=[\frac{ \partial f(x)}{\partial x_1},...,\frac{\partial f(x)}{\partial x_m}]</script><p>当实值标量函数f（X）的变元为实值矩阵$X\in R^{m\times n}$，可能存在两种定义</p><ul><li>Jacobian偏导<script type="math/tex; mode=display">D_xf(x)\overset{def}{=}\frac{\partial f(X)}{\partial X^T}=\begin{bmatrix}\frac{\partial f(X)}{\partial x_{11}}&...&\frac{\partial f(X)}{\partial x_{m1}}\\...&...&...\\\frac{\partial f(X)}{\partial x_{1n}}&...&\frac{ \partial f(X)}{\partial x_{mn}}\end{bmatrix}</script></li><li>行向量偏导<script type="math/tex; mode=display">D_{vecX}f(X)=\frac{\partial f(X)}{\partial_{vec^T(X) }}=[\frac{\partial f(X)}{\partial x_{11}}...\frac{\partial f(X)}{\partial x_{m1}}...\frac{\partial f(X)}{\partial x_{1n}}...\frac{\partial f(X)}{\partial x_{mn}}]</script></li><li>两者关系<script type="math/tex; mode=display">D_{vec}f(X)=rvec(D_Xf(X))=(vec(D_X^Tf(X)))^T</script>即实值标量函数f(X)的行向量偏导$Dvec_Xf(X)$等于Jacobian矩阵的转置$D_X^Tf(X)$的列向量化</li></ul><h3 id="矩阵函数雅克比矩阵"><a href="#矩阵函数雅克比矩阵" class="headerlink" title="矩阵函数雅克比矩阵"></a>矩阵函数雅克比矩阵</h3><p>函数为矩阵，变元为矩阵</p><script type="math/tex; mode=display">D_XF(X)=\frac{\partial f(X)}{\partial X^T}=\begin{bmatrix}\frac{\partial f(X)}{\partial X^T}&...&\frac{\partial f(X)}{\partial X^T}\\...&...&...\\\frac{\partial f(X)}{\partial X^T}&...&\frac{ \partial f(X)}{\partial X^T}\end{bmatrix}</script><script type="math/tex; mode=display">=\begin{bmatrix}\frac{\partial f_{11}}{\partial x_{11}}&...&\frac{\partial f_{11}}{\partial x_{m1}}&...&\frac{\partial f_{11}}{\partial x_{1n}}&...&\frac{\partial f_{11}}{\partial x_{m1}}\\...&...&...&...&...&...\\\frac{\partial f_{p1}}{\partial x_{11}}&...&\frac{\partial f_{p1}}{\partial x_{m1}}&...&\frac{\partial f_{p1}}{\partial x_{1n}}&...&\frac{\partial f_{p1}}{\partial x_{m1}}\\...&...&...&...&...&...\\\frac{\partial f_{1q}}{\partial x_{11}}&...&\frac{\partial f_{1q}}{\partial x_{m1}}&...&\frac{\partial f_{1q}}{\partial x_{1n}}&...&\frac{\partial f_{1q}}{\partial x_{m1}}\\...&...&...&...&...&...\\\frac{\partial f_{pq}}{\partial x_{11}}&...&\frac{\partial f_{pq}}{\partial x_{m1}}&...&\frac{\partial f_{pq}}{\partial x_{1n}}&...&\frac{\partial f_{pq}}{\partial x_{m1}}\\\end{bmatrix}</script><h2 id="梯度矩阵"><a href="#梯度矩阵" class="headerlink" title="梯度矩阵"></a>梯度矩阵</h2><p>采用列向量作为偏导算子称为偏导算子</p><script type="math/tex; mode=display">\nabla_x \overset{def}{=}[\frac{ \partial }{\partial x_1},...,\frac{\partial }{\partial x_m}]^T=\frac{\partial}{\partial x}</script><ul><li><p>标量函数的梯度向量</p><script type="math/tex; mode=display">\nabla_xf(x)=[\frac{ \partial f(x) }{\partial x_1},...,\frac{\partial f(x) }{\partial x_m}]^T</script></li><li><p>矩阵梯度向量</p><script type="math/tex; mode=display">\nabla_{vecX}f(X)=\frac{\partial f(X)}{\partial_{ve  cX }}=[\frac{\partial f(X)}{\partial x_{11}}...\frac{\partial f(X)}{\partial x_{m1}}...\frac{\partial f(X)}{\partial x_{1n}}...\frac{\partial f(X)}{\partial x_{mn}}]^T</script></li><li>梯度矩阵<script type="math/tex; mode=display">\nabla_xf(x)=\frac{\partial f(X)}{\partial X^T}=\begin{bmatrix}\frac{\partial f(X)}{\partial x_{11}}&...&\frac{\partial f(X)}{\partial x_{1n}}\\...&...&...\\\frac{\partial f(X)}{\partial x_{m1}}&...&\frac{ \partial f(X)}{\partial x_{mn}}\end{bmatrix}</script></li></ul><blockquote><p>实标量函数和矩阵函数的梯度矩阵是雅克比矩阵的转置</p></blockquote><h1 id="偏导和梯度计算"><a href="#偏导和梯度计算" class="headerlink" title="偏导和梯度计算"></a>偏导和梯度计算</h1><ul><li>若$F(X)=c$为常数，其中X为$m\times n$矩阵，则梯度$\frac{\partial c}{\partial X}=O_{m\times n}$</li><li>线性法则</li><li>乘积法则</li><li>商法则</li><li>链式法则</li></ul><h2 id="独立性基本假设"><a href="#独立性基本假设" class="headerlink" title="独立性基本假设"></a>独立性基本假设</h2><p>假定实值函数的向量变元和矩阵变元无任何特殊结构则</p><script type="math/tex; mode=display">\frac{\partial x_i}{\partial x_j}=\begin{cases}1&i=j\\0&other\end{cases}</script><script type="math/tex; mode=display">\frac{\partial x_{kl}}{\partial x_{ij}}=\begin{cases}1&k=i,l=j\\0&other\end{cases}</script><h2 id="example"><a href="#example" class="headerlink" title="example"></a>example</h2><p>求实值函数$f(x)=x^TAx$的Jacobian矩阵，$x^TAx=\sum<em>{k=1}^n\sum</em>{l=1}^na_{kl}x_kx_l$，我们可以求出行偏导向量$\frac{\partial x^TAx}{\partial x^T}$的第i个分量为</p><script type="math/tex; mode=display">\frac{\partial x^TAx}{\partial x^T}=\frac{\partial}{\partial x_i}\sum_{k=1}^n\sum_{l=1}^na_{kl}x_kx_l=\sum_{k=1}^nx_ka_{ki}+\sum_{l=1}^nx_la_{il}</script><p>我可以得到行偏导向量和梯度向量</p><script type="math/tex; mode=display">Df(x)=x^TA+x^TA^T=x^T(A+A^T)</script><script type="math/tex; mode=display">\nabla_Xf(x)=(A^T+A)x</script><h1 id="一阶实矩阵微分"><a href="#一阶实矩阵微分" class="headerlink" title="一阶实矩阵微分"></a>一阶实矩阵微分</h1><ul><li>标量函数tr(U)的微分<script type="math/tex; mode=display">d[tr(U)]=d(\sum_{i=1}^nu_{ii})=\sum_{i=1}^ndu_{ii}=tr(dU)</script></li><li>矩阵乘积UV的微分矩阵<script type="math/tex; mode=display">d(UV)=(dU)V+U(dV)</script></li><li>矩阵的迹的矩阵微分等于矩阵微分的迹<script type="math/tex; mode=display">d(tr(X))=tr(dX)</script></li></ul><h2 id="标量函数f（x）的jacobian矩阵辨识"><a href="#标量函数f（x）的jacobian矩阵辨识" class="headerlink" title="标量函数f（x）的jacobian矩阵辨识"></a>标量函数f（x）的jacobian矩阵辨识</h2><ul><li>以向量为变元的标量函数f(x)的微分<script type="math/tex; mode=display">df(x)=\frac{\partial f(x)}{\partial x_1}dx_1+...+\frac{\partial f(x)}{\partial x_m}dx_m</script><script type="math/tex; mode=display">=\frac{\partial f(x)}{\partial x^T}dx=(dx)^T\frac{\partial f(x)}{\partial x}</script>如果令<script type="math/tex; mode=display">A=\frac{\partial f(x)}{\partial x^T}</script>我们可以把一阶微分写成迹的形式<script type="math/tex; mode=display">df(x)=tr(Adx)</script><script type="math/tex; mode=display">D_ xf(x)=\frac{\partial f(x)}{\partial x^T}=A</script></li></ul><h2 id="标量函数f（X）的jacobian矩阵辨识"><a href="#标量函数f（X）的jacobian矩阵辨识" class="headerlink" title="标量函数f（X）的jacobian矩阵辨识"></a>标量函数f（X）的jacobian矩阵辨识</h2><script type="math/tex; mode=display">df(X)=(vec(A^T))^Td(vecX)</script><p>A为标量函数f（X）的Jacobin矩阵</p><script type="math/tex; mode=display">df(x)=tr(Adx)\to D_xf(x)=\frac{\partial f(x)}{\partial x^T}=A</script><script type="math/tex; mode=display">df(X)=tr(AdX)\to D_Xf(X)=\frac{\partial f(X)}{\partial X^T}=A</script><script type="math/tex; mode=display">D_Xf(X)=\frac{\partial f(X)}{\partial X^T}=A\to \nabla_Xf(X)=A^T</script><h1 id="Hessian-矩阵"><a href="#Hessian-矩阵" class="headerlink" title="Hessian 矩阵"></a>Hessian 矩阵</h1><script type="math/tex; mode=display">H[f(x)]=\frac{\partial ^2f(x)}{\partial x\partial x^T}</script><script type="math/tex; mode=display">=\begin{bmatrix}\frac{\partial^2f}{\partial x_1\partial x_1}&...&\frac{\partial f^2}{\partial x_1\partial x_m}\\...&...&...\\\frac{\partial ^2f}{\partial x_m\partial x_1}&...&\frac{\partial ^2f}{\partial x_m\partial x_m}\end{bmatrix}\in R^{m\times m}</script><script type="math/tex; mode=display">H[f(x)]=\nabla^2_xf(x)=\nabla_x(D_xf(x))</script><script type="math/tex; mode=display">[Hf(x)]_{i,j}=[\frac{\partial^2f(x)}{\partial x\partial x^T}]=\frac{\partial}{\partial x_i}[\frac{\partial f(x)}{\partial x_j}]</script><h1 id="共轭梯度和复Hessian矩阵"><a href="#共轭梯度和复Hessian矩阵" class="headerlink" title="共轭梯度和复Hessian矩阵"></a>共轭梯度和复Hessian矩阵</h1><h2 id="形式偏导"><a href="#形式偏导" class="headerlink" title="形式偏导"></a>形式偏导</h2><script type="math/tex; mode=display">\frac{\partial }{\partial z}=\frac{1}{2}> (\frac{\partial}{\partial x}-j\frac{\partial}{\partial y})</script><h2 id="实部和虚部相互独立"><a href="#实部和虚部相互独立" class="headerlink" title="实部和虚部相互独立"></a>实部和虚部相互独立</h2><h2 id="单个复变量梯度"><a href="#单个复变量梯度" class="headerlink" title="单个复变量梯度"></a>单个复变量梯度</h2><script type="math/tex; mode=display">\nabla_zf(z,z^*)=\frac{\partial f(z,z^*)}{\partial }|_{z^*=C}</script><h2 id="单个复变量微分"><a href="#单个复变量微分" class="headerlink" title="单个复变量微分"></a>单个复变量微分</h2><script type="math/tex; mode=display">df(z,z^*)=\frac{\partial f(z,z^*}{\partial z}dz+\frac{\partial f(z,z^*)}{\partial z^*}dz^*</script><h2 id="复变元向量违法"><a href="#复变元向量违法" class="headerlink" title="复变元向量违法"></a>复变元向量违法</h2><script type="math/tex; mode=display">df(z,z^*)=\frac{\partial f(z,z^*)}{\partial  z^T}dz+\frac{\partial(z,z^*)}{\partial z^H}dz^*</script><h2 id="标量函数的梯度向量和共轭梯度向量"><a href="#标量函数的梯度向量和共轭梯度向量" class="headerlink" title="标量函数的梯度向量和共轭梯度向量"></a>标量函数的梯度向量和共轭梯度向量</h2><script type="math/tex; mode=display">\nabla_z f(z,z^*)=(D_zf(z,z^*))^T</script><h2 id="常见矩阵偏导"><a href="#常见矩阵偏导" class="headerlink" title="常见矩阵偏导"></a>常见矩阵偏导</h2><script type="math/tex; mode=display">\frac{\partial a^Tx}{\partial x}=a</script><script type="math/tex; mode=display">\frac{\partial x^Ta}{\partial x}=a</script><script type="math/tex; mode=display">\frac{\partial x^TAx}{\partial x}=(A+A^T)x</script><script type="math/tex; mode=display">\frac{\partial x^TAx}{\partial x^T}=x^T(A+A^T)</script><script type="math/tex; mode=display">\frac{\partial x^TA}{\partial x}=A</script><script type="math/tex; mode=display">\frac{\partial (a-Ax)^TB(a-Ax)}{\partial x}=-2A^TB(a-Ax)</script>]]></content>
      
      
      <categories>
          
          <category> 《柯西的传世绝学》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 矩阵论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>子空间理论</title>
      <link href="2020/11/16/child-space/"/>
      <url>2020/11/16/child-space/</url>
      
        <content type="html"><![CDATA[<h1 id="子空间的基本理论"><a href="#子空间的基本理论" class="headerlink" title="子空间的基本理论"></a>子空间的基本理论</h1><h2 id="子空间的基"><a href="#子空间的基" class="headerlink" title="子空间的基"></a>子空间的基</h2><p><strong>定义</strong>：若$S={u_1,u_2…u_m}$是向量空间V的向量子集合，则向量所有的线性组合<br>W，称为$u_1,u_2…u_m$构成的子空间<br><span id="more"></span></p><script type="math/tex; mode=display">W=span\{u_1,u_2...u_m\}=\{u|u=a_1u_1+...+a_mu_m\}</script><p>当这些向量线性无关时，称为W的一组基</p><h2 id="张成集定理-spanning-set-theorem"><a href="#张成集定理-spanning-set-theorem" class="headerlink" title="张成集定理(spanning set theorem)"></a>张成集定理(spanning set theorem)</h2><p>令S ={u…,um}是向量空间V的一个子集，并且W= Span{u,…,um}是由S的m个列向量张成的一个子空间。</p><ul><li>如果S内有某个向量(例如uk)是其他向量的线性组合，则从S中删去向量uk后，其他向量仍然张成子空间W。</li><li>若W≠{0}即W为非平凡子空间，则在S内一定存在某个由线性无关的向量组成的子集合，它张成子空间W。</li></ul><h1 id="矩阵行空间，列空间"><a href="#矩阵行空间，列空间" class="headerlink" title="矩阵行空间，列空间"></a>矩阵行空间，列空间</h1><p>对于矩阵$A\in C^{m\times n}$</p><script type="math/tex; mode=display">A=[a_1,a_2...a_n]</script><script type="math/tex; mode=display">=\begin{bmatrix}r_1\\r_2\\...\\r_m\end{bmatrix}</script><p>矩阵的列空间为矩阵列向量张成</p><script type="math/tex; mode=display">Col(A)=span\{a_1,a_2...a_n\}</script><p>行空间同理</p><script type="math/tex; mode=display">Row(A)=span\{r_1^*,r_2^*...r_m^*\}</script><h1 id="基本空间的标准正交基的构造：奇异值分解"><a href="#基本空间的标准正交基的构造：奇异值分解" class="headerlink" title="基本空间的标准正交基的构造：奇异值分解"></a>基本空间的标准正交基的构造：奇异值分解</h1><p>根据奇异值分解的性质，我们知道矩阵A可以被分解为</p><script type="math/tex; mode=display">A=U_r\Sigma_rV_r^H</script><script type="math/tex; mode=display">=sum_{i=1}^r \sigma_i u_iv_i</script><p>定义$A\in C^{m\times n},A$的值域Range定义为</p><script type="math/tex; mode=display">Rang(A)=\{y\in C^m|Ax=y,x\in C^n\}</script><p>此时的线性映射从$C^n$空间到$C^m$</p><h2 id="零空间"><a href="#零空间" class="headerlink" title="零空间"></a>零空间</h2><p><strong>零空间</strong>被定义为满足</p><script type="math/tex; mode=display">Ax=0</script><p>的向量x集合</p><script type="math/tex; mode=display">Null(A)=ker(A)=\{x\in C^n|Ax=0\}</script><h2 id="列空间的标准正交基"><a href="#列空间的标准正交基" class="headerlink" title="列空间的标准正交基"></a>列空间的标准正交基</h2><p>与r个非零奇异值所对于的左奇异向量$u_1,u_2…u_r$构成列空间col（A）的一组基</p><script type="math/tex; mode=display">Range(A)=\{y\in c^m|y=Ax,x\in C^n\}</script><script type="math/tex; mode=display">=\{y|y=\sum_{i=1}^ru_i(\sigma_iv_i^Hx)\}</script><script type="math/tex; mode=display">=\{y|y=\sum_{i=1}^r\alpha_iu_i，\alpha_i=\sigma_iv_i^Hx\}</script><script type="math/tex; mode=display">=span\{u_1,u_2...u_r\}</script><h1 id="行空间标准正交基"><a href="#行空间标准正交基" class="headerlink" title="行空间标准正交基"></a>行空间标准正交基</h1><p>与列空间的推导同理</p><script type="math/tex; mode=display">Range(A^H)=\{y\in C^n|y=\sum_{i=1}^r \alpha_iv_i,\alpha_i=\sigma_iu_i^Hx\}</script><h1 id="信号子空间和噪声子空间"><a href="#信号子空间和噪声子空间" class="headerlink" title="信号子空间和噪声子空间"></a>信号子空间和噪声子空间</h1><p>如果观测数据A存在噪声误差</p><script type="math/tex; mode=display">X=A+W=[x_1,x_2...x_n]\in C^{m\times n}</script><p>定义</p><ul><li>观测数据子空间<script type="math/tex; mode=display">span(X)=span\{x_1,x_2...x_n\}</script></li><li><p>噪声子空间</p><script type="math/tex; mode=display">span(W)=span\{w_1,w_2...w_n\}</script></li><li><p>X的协方差矩阵为</p><script type="math/tex; mode=display">E\{x^Hx\}=E\{(A+W)^H(A+W)\}</script><script type="math/tex; mode=display">=E\{A^HA\}+E\{W^HW\}</script><script type="math/tex; mode=display">=R+\sigma_wI</script><p>令A的秩为r</p></li></ul><p>则</p><script type="math/tex; mode=display">R_X=U\Sigma U^H+\sigma_w^2I=U(\Sigma+\sigma_w^2 I)U^H</script><script type="math/tex; mode=display">\Sigma+\sigma_w^2I=diag(\sigma_1^2+\sigma_w^2...\sigma_r^2+\sigma_w^2,\sigma_w^2...\sigma_w^2)</script><p>如果信噪比足够大，即$\sigma_r^2&gt;\sigma_w^2$</p><p>则前r个大特征值称为<strong>主特征</strong></p><p>剩余n-r个称为<strong>次特征</strong></p><script type="math/tex; mode=display">R_x=[S,G]\begin{bmatrix}\Sigma_s&0\\0&\Sigma_n\end{bmatrix}\begin{bmatrix}S^H\\G^H\end{bmatrix}</script><script type="math/tex; mode=display">=S\Sigma_sS^H+G\Sigma_nG^H</script><p>此时</p><script type="math/tex; mode=display">S=[s_1..s_2...s_r]=[u_1,u_2...u_r]\\G=[g_1,g_2...g_{n-r}]=[u_{r+1}...u_n]\\\Sigma_s=diag(\sigma_1^2+\sigma_w^2...\sigma_r^2+\sigma_w^2)\\\Sigma_n=diag(\sigma_w^2...\sigma_w^2)</script><ul><li><p>$Range(S)$称为信号子空间</p></li><li><p>$Range(G)$称为噪声子空间</p></li></ul><h2 id="重要性质"><a href="#重要性质" class="headerlink" title="重要性质"></a>重要性质</h2><p>噪声子空间和信号子空间正交</p><script type="math/tex; mode=display">UU^H=SS^H+GG^H=I</script>]]></content>
      
      
      <categories>
          
          <category> 《柯西的传世绝学》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 矩阵论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>特殊矩阵</title>
      <link href="2020/11/15/spacial-matrix/"/>
      <url>2020/11/15/spacial-matrix/</url>
      
        <content type="html"><![CDATA[<h1 id="Hermitian矩阵"><a href="#Hermitian矩阵" class="headerlink" title="Hermitian矩阵"></a>Hermitian矩阵</h1><p>复共轭对称矩阵<br><span id="more"></span></p><script type="math/tex; mode=display">R=R^H</script><h1 id="置换矩阵"><a href="#置换矩阵" class="headerlink" title="置换矩阵"></a>置换矩阵</h1><p>一个正方矩阵称为置换矩阵(permutation matrix)，若它的每一行和每一列有一个且仅有一个非零元素1。</p><script type="math/tex; mode=display">\begin{array}{l}P_{m\times n}^T=P_{n\times m}\\P^TP=PP^T=I\\P^T=P^{-1}\end{array}</script><blockquote><p>置换矩阵是正交矩阵。</p></blockquote><h1 id="互换矩阵"><a href="#互换矩阵" class="headerlink" title="互换矩阵"></a>互换矩阵</h1><script type="math/tex; mode=display">\boldsymbol{J}=\left[\begin{array}{ccc}0 & & &1 \\& & 1 \\& \cdot \cdot & \\1 & & & 0\end{array}\right]</script><ul><li>左乘：矩阵的行顺序反转</li><li>右乘：矩阵的列顺序反转</li></ul><h1 id="正交矩阵和酉矩阵"><a href="#正交矩阵和酉矩阵" class="headerlink" title="正交矩阵和酉矩阵"></a>正交矩阵和酉矩阵</h1><ul><li>U非奇异且，$U^H=U^{-1}$</li><li>$UU^H=U^HU=I$</li><li>U的行列向量组成标准正交组</li></ul><h2 id="酉不变性"><a href="#酉不变性" class="headerlink" title="酉不变性"></a>酉不变性</h2><ul><li><x,y>=<Ax,Ay></li><li>||Ax||^2 =||x||^2<script type="math/tex; mode=display">cos\theta=\frac{<Ax,Ay>}{||Ax||||Ay||}=\frac{<x,y>}{||x|| ||y||}</script><h1 id="三角矩阵"><a href="#三角矩阵" class="headerlink" title="三角矩阵"></a>三角矩阵</h1></li><li>上三角</li><li>下三角</li></ul><h1 id="相似矩阵"><a href="#相似矩阵" class="headerlink" title="相似矩阵"></a>相似矩阵</h1><p>S为非奇异矩阵</p><script type="math/tex; mode=display">B=S^{-1}AS</script><ul><li>特征值相同</li><li>特征向量存在线性变换关系</li></ul><script type="math/tex; mode=display">det(B)=det(A)tr(B)=tr(A)</script><h1 id="Vandermonde矩阵"><a href="#Vandermonde矩阵" class="headerlink" title="Vandermonde矩阵"></a>Vandermonde矩阵</h1><script type="math/tex; mode=display">\boldsymbol{A}=\left[\begin{array}{cccc}1 & 1 & \cdots & 1 \\x_{1} & x_{2} & \cdots & x_{n} \\x_{1}^{2} & x_{2}^{2} & \cdots & x_{n}^{2} \\\vdots & \vdots & \vdots & \vdots \\x_{1}^{n-1} & x_{2}^{n-1} & \cdots & x_{n}^{n-1}\end{array}\right] \quad \boldsymbol{A}=\left[\begin{array}{ccccc}1 & x_{1} & x_{1}^{2} & \cdots & x_{1}^{n-1} \\1 & x_{2} & x_{2}^{2} & \cdots & x_{2}^{n-1} \\\vdots & \vdots & \vdots & \vdots & \vdots \\1 & x_{n} & x_{n}^{2} & \cdots & x_{n}^{n-1}\end{array}\right]</script><h1 id="Fourier-矩阵"><a href="#Fourier-矩阵" class="headerlink" title="Fourier 矩阵"></a>Fourier 矩阵</h1><p>Fourier矩阵是一种特殊结构的Vandermonde</p><script type="math/tex; mode=display">\boldsymbol{F}=\left[\begin{array}{cccc}1 & 1 & \cdots & 1 \\1 & w & \cdots & w^{N-1} \\\vdots & \vdots & \vdots & \vdots \\1 & w^{N-1} & \cdots & w^{(N-1)(N-1)}\end{array}\right],</script><h1 id="Hadamard矩阵"><a href="#Hadamard矩阵" class="headerlink" title="Hadamard矩阵"></a>Hadamard矩阵</h1><p>所有元素取1，-1，且</p><script type="math/tex; mode=display">H_nH_n^T=H_n^TH_n=nI</script><ul><li>矩阵维度只能是2的倍数</li></ul><h1 id="Toeplitz矩阵"><a href="#Toeplitz矩阵" class="headerlink" title="Toeplitz矩阵"></a>Toeplitz矩阵</h1><p>任意一条对角线元素取相同值</p><script type="math/tex; mode=display">A=\begin{bmatrix}a_0&a_{-1}&a_{-2}&...&a_{-n}\\a_1&a_0&a_{-1}&...&a_{-n+1}\\a_2&a_1&a_0&...&...\\...&...&...&...&...\\a_n&a_{n-1}&...&a_1&a_0\end{bmatrix}=[a_{i-j}]^n_{i,j=0}</script><p>若元素满足复共轭关系</p><script type="math/tex; mode=display">a_{-i}=a_i^*</script><p>则称为Hermitian Toeplitz 矩阵</p><h1 id="Hankle矩阵"><a href="#Hankle矩阵" class="headerlink" title="Hankle矩阵"></a>Hankle矩阵</h1><script type="math/tex; mode=display">\boldsymbol{A}=\left[\begin{array}{ccccc}a_{0} & a_{1} & a_{2} & \cdots & a_{n} \\a_{1} & a_{2} & a_{3} & \cdots & a_{n+1} \\a_{2} & a_{3} & a_{4} & \cdots & a_{n+2} \\\vdots & \vdots & \vdots & \vdots & \vdots \\a_{n} & a_{n+1} & a_{n+2} & \cdots & a_{2 n}\end{array}\right]</script>]]></content>
      
      
      <categories>
          
          <category> 《柯西的传世绝学》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 矩阵论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>矩阵代数基础</title>
      <link href="2020/11/15/matrix-basic/"/>
      <url>2020/11/15/matrix-basic/</url>
      
        <content type="html"><![CDATA[<h1 id="典型物理模型"><a href="#典型物理模型" class="headerlink" title="典型物理模型"></a>典型物理模型</h1><h3 id="多个信号观测模型"><a href="#多个信号观测模型" class="headerlink" title="多个信号观测模型"></a>多个信号观测模型</h3><script type="math/tex; mode=display">x(n)=As(n)+v(n)</script><span id="more"></span><h1 id="书写规范"><a href="#书写规范" class="headerlink" title="书写规范"></a>书写规范</h1><ul><li>大写字母下加两线：矩阵</li><li>加一线：向量</li><li>不加：变量</li></ul><h1 id="矩阵代数基本性质"><a href="#矩阵代数基本性质" class="headerlink" title="矩阵代数基本性质"></a>矩阵代数基本性质</h1><script type="math/tex; mode=display">\begin{array}{l}(A+B)^*=A^*+B^*\\(A+B)^T=A^T+B^T\\(A+B)^H=A^H+B^H\\(AB)^T=B^TA^T\\(AB)^H=B^HA^H\\(AB)^{-1}=B^{-1}A^{-1}\\(A^*)^{-1}=(A^{-1})^T\\(A^H)^{-1}=(A^{-1})^H\end{array}</script><p>对于任意矩阵A，矩阵$B=A^HA$都是Hermitndian</p><h2 id="导数积分"><a href="#导数积分" class="headerlink" title="导数积分"></a>导数积分</h2><script type="math/tex; mode=display">\frac{dA}{dt}=\begin{bmatrix}\frac{da_{11}}{dt}&...&\frac{da_{1n}}{dt}\\...&...&...\\\frac{da_{m1}}{dt}&...&\frac{da_{mn}}{dt}\end{bmatrix}</script><ul><li>指数矩阵函数<script type="math/tex; mode=display">exp(At)=I+At+\frac{A^2t^2}{2!}...\frac{A^nt^n}{n!}</script></li><li>指数矩阵导数<script type="math/tex; mode=display">\frac{de^{At}}{dt}=Ae^{At}</script></li><li>矩阵乘积的导数<script type="math/tex; mode=display">\frac{d}{dt}(AB)=\frac{dA}{dt}B+A\frac{dB}{dt}</script></li><li>对数函数<script type="math/tex; mode=display">In(I+A)=\sum_{n=1}^\infty\frac{(-1)^{n-1}}{n}A^n</script><h2 id="特殊矩阵"><a href="#特殊矩阵" class="headerlink" title="特殊矩阵"></a>特殊矩阵</h2></li><li>幂等矩阵<script type="math/tex; mode=display">A^2=A</script></li><li>对合矩阵</li></ul><script type="math/tex; mode=display">A^2=I</script><ul><li>正交矩阵</li></ul><script type="math/tex; mode=display">AA^T=I</script><h2 id="线性无关与奇异性"><a href="#线性无关与奇异性" class="headerlink" title="线性无关与奇异性"></a>线性无关与奇异性</h2><h3 id="线性方程组"><a href="#线性方程组" class="headerlink" title="线性方程组"></a>线性方程组</h3><script type="math/tex; mode=display">c_1u_1+...c_nu_n=0</script><ul><li>向量组线性无关：只有零解</li><li>线性相关：有非零解<h3 id="矩阵方程"><a href="#矩阵方程" class="headerlink" title="矩阵方程"></a>矩阵方程</h3><script type="math/tex; mode=display">Ax=0</script></li><li>只有零解：A非奇异</li><li>存在非零解：A是奇异的</li></ul><h2 id="复矩阵方程"><a href="#复矩阵方程" class="headerlink" title="复矩阵方程"></a>复矩阵方程</h2><script type="math/tex; mode=display">(A_r+jA_i)(x_r+jx_i)=b_r+jb_i\begin{bmatrix}A_r&-A_i&b_r\\A_i&A_r&b_i\end{bmatrix}\to\begin{bmatrix}I_n&O_n&x_r\\O_n&I_n&x_i\end{bmatrix}</script><h2 id="向量空间和线性映射"><a href="#向量空间和线性映射" class="headerlink" title="向量空间和线性映射"></a>向量空间和线性映射</h2><p>向量空间是以向量为元素的集合</p><ul><li>子空间V到子空间W的映射<script type="math/tex; mode=display">T:V\to W</script><script type="math/tex; mode=display">T(c_1u_1...+c_nu_n)=c_1T(u_1)+...+c_nT(u_n)</script><blockquote><p>满足线性性质</p></blockquote></li></ul><h3 id="正交投影算子"><a href="#正交投影算子" class="headerlink" title="正交投影算子"></a>正交投影算子</h3><script type="math/tex; mode=display">w=T(x)</script><script type="math/tex; mode=display">\begin{bmatrix}w_1\\w_2\end{bmatrix}=\begin{bmatrix}0&0\\0&1\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}</script><h1 id="向量内积与范数"><a href="#向量内积与范数" class="headerlink" title="向量内积与范数"></a>向量内积与范数</h1><h3 id="典范内积"><a href="#典范内积" class="headerlink" title="典范内积"></a>典范内积</h3><script type="math/tex; mode=display"><x,y>=x^Hy=\sum_{i=1}^nx_{i}^*y_i</script><h3 id="加权内积"><a href="#加权内积" class="headerlink" title="加权内积"></a>加权内积</h3><script type="math/tex; mode=display"><x,y>=x^HGy</script><p>G为正定Hermitian矩阵</p><h3 id="连续函数内积"><a href="#连续函数内积" class="headerlink" title="连续函数内积"></a>连续函数内积</h3><script type="math/tex; mode=display"><x(t),y(t)>=\int_a^bx(t)y(t)dt</script><h2 id="向量范数"><a href="#向量范数" class="headerlink" title="向量范数"></a>向量范数</h2><ul><li><p>L0范数</p><script type="math/tex; mode=display">||x||_0=^{def}非零元素的个数</script><blockquote><p>表示矩阵稀疏程度</p></blockquote></li><li><p>L1范数</p><script type="math/tex; mode=display">||x||_1=^{def}\sum_{i=1}^m|x_i|</script></li><li>L2范数<script type="math/tex; mode=display">||x||_2=(|x_1|^2...+|x_n|^2)^{1/2}=\sqrt{x^Tx}</script></li><li>$L_{\infty}$<script type="math/tex; mode=display">L_{\infty}=max\{x_i\}</script></li><li>$L_P$<script type="math/tex; mode=display">||x_p||=(\sum_{i=1}^m|x_i|^p)^{1/p}</script></li></ul><h2 id="模式识别和机器学习中的向量相似比较"><a href="#模式识别和机器学习中的向量相似比较" class="headerlink" title="模式识别和机器学习中的向量相似比较"></a>模式识别和机器学习中的向量相似比较</h2><p>距离测度$D(p||g)$衡量向量相似度</p><p>比如k邻近算法中</p><script type="math/tex; mode=display">D_E(x,s_i)=||x-s_i||_2=\sqrt{(x-s_i)^T(x-s_i)}</script><h2 id="矩阵内积与范数"><a href="#矩阵内积与范数" class="headerlink" title="矩阵内积与范数"></a>矩阵内积与范数</h2><p>矩阵列向量化</p><script type="math/tex; mode=display">a=vec(A)=\begin{bmatrix}a_1\\a_2\\...\\a_n\end{bmatrix}</script><script type="math/tex; mode=display">mn\times 1 向量</script><h3 id="矩阵内积"><a href="#矩阵内积" class="headerlink" title="矩阵内积"></a>矩阵内积</h3><script type="math/tex; mode=display"><A,B>=<vec(A),vec(B)>=\sum_{i=1}^na_i^Hb_i=\sum_i^n<a_i,b_i></script><script type="math/tex; mode=display">=vec(A)^Hvec(B)=tr<A^HB></script><h3 id="诱导范数"><a href="#诱导范数" class="headerlink" title="诱导范数"></a>诱导范数</h3><script type="math/tex; mode=display">||A||=\max\{||Ax||:x\in K^n,||x||=1\}</script><script type="math/tex; mode=display">=\max\{\frac{||Ax||}{||X||}:x\in K^n,||x||=\not 0\}</script><h4 id="诱导p范数"><a href="#诱导p范数" class="headerlink" title="诱导p范数"></a>诱导p范数</h4><p>诱导$L$和$L_\infty$范数分别直接是该矩阵的各列元素绝对值之和的最大值(最大绝对列和)及最大绝对行和;而诱导L2范数则是矩阵A的最大奇异值。</p><script type="math/tex; mode=display">\begin{array}{l}\|\boldsymbol{A}\|_{1}=\max _{1 \leqslant j \leqslant n} \sum_{i=1}^{m}\left|a_{i j}\right| \\\|\boldsymbol{A}\|_{\mathrm{spec}}=\|\boldsymbol{A}\|_{2} \\\|\boldsymbol{A}\|_{\infty}=\max _{1 \leqslant i \leqslant m} \sum_{j=1}^{n}\left|a_{i j}\right|\end{array}</script><h3 id="元素函数"><a href="#元素函数" class="headerlink" title="元素函数"></a>元素函数</h3><p>将m×n矩阵先按照列堆栈的形式，排列成一个mn ×1向量，然后采用向量的范数定义，即得到矩阵的范数。由于这类范数是使用矩阵的元素表示的，故称为元素形式范数。元素形式范数是下面的p矩阵范数</p><script type="math/tex; mode=display">\|\boldsymbol{A}\|_{p} \stackrel{\text { def }}{=}\left(\sum_{i=1}^{m} \sum_{j=1}^{n}\left|a_{i j}\right|^{p}\right)^{1 / p}</script><ul><li>L1函数（和范数）（p=1）<script type="math/tex; mode=display">\|\boldsymbol{A}\|_{1} \stackrel{\text { def }}{=} \sum_{i=1}^{m} \sum_{j=1}^{n}\left|a_{i j}\right|</script></li><li>Frobenius范数（p=2）<script type="math/tex; mode=display">\|\boldsymbol{A}\|_{\mathrm{F}} \stackrel{\text { def }}{=}\left(\sum_{i=1}^{m} \sum_{j=1}^{n}\left|a_{i j}\right|^{2}\right)^{1 / 2}</script>Frobenius函数又可以写作迹函数的形式</li></ul><script type="math/tex; mode=display">||A||_F=\sqrt{tr(A^HA)}</script><ul><li>最大范数<script type="math/tex; mode=display">\|\boldsymbol{A}\|_{\infty}=\max _{i=1, \cdots, m ; j=1, \cdots, n}\left\{\left|a_{i j}\right|\right\}</script><h1 id="随机向量"><a href="#随机向量" class="headerlink" title="随机向量"></a>随机向量</h1></li></ul><h2 id="均值向量"><a href="#均值向量" class="headerlink" title="均值向量"></a>均值向量</h2><script type="math/tex; mode=display">\boldsymbol{\mu}_{x}=\mathrm{E}\{\boldsymbol{x}(\xi)\}=\left[\begin{array}{c}\mathrm{E}\left\{x_{1}(\xi)\right\} \\\vdots \\\mathrm{E}\left\{x_{m}(\xi)\right\}\end{array}\right]=\left[\begin{array}{c}\mu_{1} \\\vdots \\\mu_{m}\end{array}\right]</script><h2 id="相关矩阵"><a href="#相关矩阵" class="headerlink" title="相关矩阵"></a>相关矩阵</h2><script type="math/tex; mode=display">\boldsymbol{R}_{x} \stackrel{\text { def }}{=} \mathrm{E}\left\{\boldsymbol{x}(\xi) \boldsymbol{x}^{\mathrm{H}}(\xi)\right\}=\left[\begin{array}{ccc}r_{11} & \cdots & r_{1 m} \\\vdots & \ddots & \vdots \\r_{m 1} & \cdots & r_{m m}\end{array}\right]</script><h2 id="自协方差矩阵"><a href="#自协方差矩阵" class="headerlink" title="自协方差矩阵"></a>自协方差矩阵</h2><script type="math/tex; mode=display">C_{x}=\operatorname{Cov}(x, x) \stackrel{\text { def }}{=} \mathrm{E}\left\{\left[x(\xi)-\mu_{x}\right]\left[x(\xi)-\mu_{x}\right]^{\mathrm{H}}\right\}=\left[\begin{array}{ccc}c_{11} & \cdots & c_{1 m} \\\vdots & \ddots & \vdots \\c_{m 1} & \cdots & c_{m m}\end{array}\right]</script><h3 id="自相关矩阵与自协方差矩阵之间的关系"><a href="#自相关矩阵与自协方差矩阵之间的关系" class="headerlink" title="自相关矩阵与自协方差矩阵之间的关系"></a>自相关矩阵与自协方差矩阵之间的关系</h3><script type="math/tex; mode=display">C_x=R_x-\mu_x\mu_x^H</script><h2 id="互协方差矩阵"><a href="#互协方差矩阵" class="headerlink" title="互协方差矩阵"></a>互协方差矩阵</h2><script type="math/tex; mode=display">C_{xy} \stackrel{\text { def }}{=} \mathrm{E}\left\{\left[x(\xi)-\mu_{u}\right]\left[y(\xi)-\mu_{y}\right]^{\mathrm{H}}\right\}=\left[\begin{array}{ccc}c_{x_1,y_1} & \cdots & c_{x_1,y_m} \\\vdots & \ddots & \vdots \\c_{x_m,y_1} & \cdots & c_{x_m,x_m}\end{array}\right]</script><script type="math/tex; mode=display">C_x=R_{xy}-\mu_x\mu_y^H</script><h2 id="相关系数"><a href="#相关系数" class="headerlink" title="相关系数"></a>相关系数</h2><script type="math/tex; mode=display">\rho_{x y} \stackrel{\text { def }}{=} \frac{c_{x y}}{\sqrt{\mathrm{E}\left\{|x(\xi)|^{2}\right\} \mathrm{E}\left\{|y(\xi)|^{2}\right\}}}=\frac{c_{x y}}{\sigma_{x} \sigma_{y}}</script><h2 id="高斯随机向量"><a href="#高斯随机向量" class="headerlink" title="高斯随机向量"></a>高斯随机向量</h2><script type="math/tex; mode=display">E\{x(t)\}=0</script><script type="math/tex; mode=display">E\{x(t)x^T(t)\}=\sigma I</script><h1 id="矩阵的性能指标"><a href="#矩阵的性能指标" class="headerlink" title="矩阵的性能指标"></a>矩阵的性能指标</h1><h2 id="矩阵的二次型"><a href="#矩阵的二次型" class="headerlink" title="矩阵的二次型"></a>矩阵的二次型</h2><p>对于任意方阵A的二次型定义为$x^HAx$</p><script type="math/tex; mode=display">f\left(x_{1}, \cdots, x_{n}\right)=\sum_{i=1}^{n} \alpha_{i i} x_{i}^{2}+\sum_{i=1, i \neq j}^{n} \sum_{j=1}^{n} \alpha_{i j} x_{i} x_{j}</script><p>为保证唯一性，在讨论矩阵的二次型时，有必要假定矩阵为实对称矩阵或复共轭对称矩阵</p><ul><li>正定矩阵，<ul><li>二次型$x^HAx &gt; 0$</li></ul></li><li>半正定矩阵<ul><li>$x^HAx &gt; 0$ </li></ul></li><li>负定矩阵<ul><li>二次型$x^HAx &gt; 0$</li></ul></li><li>半负定矩阵<ul><li>若二次型$x^HAx &gt; 0$</li></ul></li><li>不定矩阵，若二次型$x^HAx$ 既可能取正值，也可能取负值。</li></ul><h3 id="用特征值描述矩阵的正定性与非正定性"><a href="#用特征值描述矩阵的正定性与非正定性" class="headerlink" title="用特征值描述矩阵的正定性与非正定性"></a>用特征值描述矩阵的正定性与非正定性</h3><ul><li>正定矩阵:所有特征值取正实数的矩阵。</li><li>半正定矩阵:各个特征值取非负实数的矩阵。</li><li>负定矩阵:全部特征值为负实数的矩阵。</li><li>半负定矩阵:每个特征值取非正实数的矩阵。</li><li>不定矩阵:特征值有些取正实数,另一些取负实数的矩阵。</li></ul><blockquote><p>矩阵的特征值可描述正定性、奇异性及对角元素的特殊结构</p></blockquote><h2 id="矩阵的迹"><a href="#矩阵的迹" class="headerlink" title="矩阵的迹"></a>矩阵的迹</h2><p>矩阵的迹反映所有特征值之和</p><h2 id="矩阵的秩"><a href="#矩阵的秩" class="headerlink" title="矩阵的秩"></a>矩阵的秩</h2><p>矩阵中线性无关的行或列的数目</p><ul><li>适定方程:若m = n，并且 rank(A) = n，即矩阵A非奇异，则称矩阵方程Aa = b为适定(well-determined)方程。</li><li>欠定方程:若独立的方程个数小于独立的未知参数个数，则称矩阵方程 Aa = b为欠定(under-determined)方程。</li><li>超定方程:若独立的方程个数大于独立的未知参数个数，则称矩阵方程Aa = b为超定(over-determined)方程。</li></ul><h1 id="逆矩阵和伪逆矩阵"><a href="#逆矩阵和伪逆矩阵" class="headerlink" title="逆矩阵和伪逆矩阵"></a>逆矩阵和伪逆矩阵</h1><p>矩阵的可逆和非奇异的叙述中，下列叙述等价</p><ul><li>A非奇异</li><li>A-1存在</li><li>rank(A) = n</li><li>A的行线性无关</li><li>A的列线性无关;</li><li>det(A)≠0</li><li>A 的值域的维数是n</li><li>A 的零空间的维数是0</li><li>Ax = b对每一个$b\in C^n$都是一致方程</li><li>Ax = b对每一个b有唯一的解</li><li>Ax = 0只有平凡解x = 0。</li></ul><p>n x n矩阵A的逆矩阵A-1具有以下性质</p><ul><li>A-1A= AA-1 =I。</li><li>A-1是唯一的。</li><li>逆矩阵的行列式等于原矩阵行列式的倒数，即$|A^{-1}|=\frac{1}{|A|}$</li><li>逆矩阵是非奇异的。</li><li>$(A^{-1})^{-1}=A$</li></ul><h2 id="矩阵求逆定理-Sherman-Morrison"><a href="#矩阵求逆定理-Sherman-Morrison" class="headerlink" title="矩阵求逆定理(Sherman-Morrison)"></a>矩阵求逆定理(Sherman-Morrison)</h2><script type="math/tex; mode=display">(A+xy^H)^{-1}=A^{-1}-\frac{A^{-1}xy^HA^{-1}}{1+y^HA^{-1}x}</script><h2 id="广义逆矩阵"><a href="#广义逆矩阵" class="headerlink" title="广义逆矩阵"></a>广义逆矩阵</h2><p>A的广义逆矩阵满足以下四个条件</p><script type="math/tex; mode=display">\begin{array}{l}A A^{\dagger} A=A\\A^{\dagger} AA^{\dagger}=A^{\dagger}\\AA^{\dagger}=(AA^{\dagger})^H\\A^{\dagger}A=(A^{\dagger}A)^H\end{array}</script><h1 id="矩阵的直和"><a href="#矩阵的直和" class="headerlink" title="矩阵的直和"></a>矩阵的直和</h1><script type="math/tex; mode=display">\boldsymbol{A} \oplus \boldsymbol{B}=\left[\begin{array}{cc}\boldsymbol{A} & \boldsymbol{O}_{m \times n} \\\boldsymbol{O}_{n \times m} & \boldsymbol{B}\end{array}\right]</script><h1 id="Hadamard积"><a href="#Hadamard积" class="headerlink" title="Hadamard积"></a>Hadamard积</h1><p>两个同维度矩阵对应元素直接相乘</p><script type="math/tex; mode=display">(A*B)_{ij}=a_{ij}b_{ij}</script><h1 id="Kronecker积"><a href="#Kronecker积" class="headerlink" title="Kronecker积"></a>Kronecker积</h1><h2 id="左Kronecker积"><a href="#左Kronecker积" class="headerlink" title="左Kronecker积"></a>左Kronecker积</h2><script type="math/tex; mode=display">\boldsymbol{A} \otimes \boldsymbol{B}=\left[\boldsymbol{a}_{1} \boldsymbol{B}, \cdots, \boldsymbol{a}_{n} \boldsymbol{B}\right]=\left[a_{i j} \boldsymbol{B}\right]_{i=1, j=1}^{m, n}=\left[\begin{array}{cccc}a_{11} \boldsymbol{B} & a_{12} \boldsymbol{B} & \cdots & a_{1 n} \boldsymbol{B} \\a_{21} \boldsymbol{B} & a_{22} \boldsymbol{B} & \cdots & a_{2 n} \boldsymbol{B} \\\vdots & \vdots & \ddots & \vdots \\a_{m 1} \boldsymbol{B} & a_{m 2} \boldsymbol{B} & \cdots & a_{m n} \boldsymbol{B}\end{array}\right]</script><h2 id="右Kronecker积"><a href="#右Kronecker积" class="headerlink" title="右Kronecker积"></a>右Kronecker积</h2><script type="math/tex; mode=display">[\boldsymbol{A} \otimes \boldsymbol{B}]_{\text {left }}=\left[\boldsymbol{A} \boldsymbol{b}_{1}, \cdots, \boldsymbol{A} \boldsymbol{b}_{\boldsymbol{q}}\right]=\left[b_{i j} \boldsymbol{A}\right]_{i=1, j=1}^{p, \boldsymbol{q}}=\left[\begin{array}{cccc}\boldsymbol{A} b_{11} & \boldsymbol{A} b_{12} & \cdots & \boldsymbol{A} b_{1 \boldsymbol{q}} \\\boldsymbol{A} b_{21} & \boldsymbol{A} b_{22} & \cdots & \boldsymbol{A} b_{2 q} \\\vdots & \vdots & \ddots & \vdots \\\boldsymbol{A} b_{\boldsymbol{p} 1} & \boldsymbol{A} b_{p 2} & \cdots & \boldsymbol{A} b_{p q}\end{array}\right]</script><h1 id="矩阵向量化"><a href="#矩阵向量化" class="headerlink" title="矩阵向量化"></a>矩阵向量化</h1><h2 id="按列堆栈"><a href="#按列堆栈" class="headerlink" title="按列堆栈"></a>按列堆栈</h2><script type="math/tex; mode=display">vec (\boldsymbol{A})=\left[a_{11}, \cdots, a_{m 1}, \cdots, a_{1 n}, \cdots, a_{m n}\right]^{\mathrm{T}}</script><h2 id="按行堆栈"><a href="#按行堆栈" class="headerlink" title="按行堆栈"></a>按行堆栈</h2><script type="math/tex; mode=display">rvec (\boldsymbol{A})=\left[a_{11}, \cdots, a_{1 n}, \cdots, a_{m 1}, \cdots, a_{m n}\right]</script>]]></content>
      
      
      <categories>
          
          <category> 《柯西的传世绝学》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 矩阵论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>矩阵方程求解&amp;最小二乘法</title>
      <link href="2020/10/21/matrix-equation/"/>
      <url>2020/10/21/matrix-equation/</url>
      
        <content type="html"><![CDATA[<p>矩阵方程的求解主要分为三类</p><ul><li>超定矩阵方程，m&gt;n,A,b已知</li><li>盲矩阵方程：b已知，A未知</li><li>欠定稀疏矩阵方程:m&lt;n，A，b已知但x未知且稀疏<span id="more"></span></li></ul><h1 id="最小二乘法（LS）"><a href="#最小二乘法（LS）" class="headerlink" title="最小二乘法（LS）"></a>最小二乘法（LS）</h1><h2 id="普通最小二乘"><a href="#普通最小二乘" class="headerlink" title="普通最小二乘"></a>普通最小二乘</h2><p>考虑超定矩阵方程Ax=b,m&gt;n，为了抵制误差对矩阵方程求解的影响，引入一校正向量△b，并用它去“扰动”有误差的数据向量b。我们的目标是，使校正项△b“尽可能小”</p><script type="math/tex; mode=display">Ax=b</script><p>m&gt;n时为超定方程</p><script type="math/tex; mode=display">b=b_0+e</script><script type="math/tex; mode=display">b+\Delta b\to b_0</script><p>问题可以理解为使</p><script type="math/tex; mode=display">||\Delta b||^2\to min</script><p>的优化问题</p><script type="math/tex; mode=display">Ax=b+\Delta b</script><script type="math/tex; mode=display">L(x)=||Ax-b||_2^2</script><script type="math/tex; mode=display">=(Ax-b)^H(Ax-b)</script><script type="math/tex; mode=display">\frac{\partial L(x)}{\partial x^*}=A^HAx-A^Hb=0</script><script type="math/tex; mode=display">\to x_{LS}=(A^HA)^{-1}A^Hb</script><p>对于秩亏缺$（rank（A）&lt;n）$的超定方程，最小二乘解为</p><script type="math/tex; mode=display">\to x_{LS}=(A^HA)^{\dagger}A^Hb</script><h2 id="高斯马尔可夫定理"><a href="#高斯马尔可夫定理" class="headerlink" title="高斯马尔可夫定理"></a>高斯马尔可夫定理</h2><p>在参数估计理论中,称参数向量$\theta$的估计$\hat{\theta}$为无偏估计，若它的数学期望值等于真实的未知参数向量，即$E{\hat{\theta}}=\theta$。进一步地，如果一个无偏估计还具有最小方差，则称这一无偏估计为最优无偏估计。</p><p>类似地，对于数据向量b含有加性噪声或者扰动的超定方程A$\theta$= b+e，若最小二乘解$\theta_{LS}$的数学期望等于真实参数向量$\theta$，便称最小二乘解是无偏的。如果它还具有最小方差，则称最小二乘解是最优无偏的。</p><p>高斯马尔科夫定理指出了最优无偏解的条件，对于线性方程组</p><script type="math/tex; mode=display">Ax=b_0+e</script><p>满足</p><script type="math/tex; mode=display">\begin{cases}E(e)=0\\cov\{e\}=E\{ee^H\}=\sigma^2I\end{cases}</script><p>当且仅当$rank(A)=n$时，最优无偏解为最小二乘解</p><script type="math/tex; mode=display">x_{LS}=(A^HA)^{-1}A^Hb</script><h2 id="LS解与最大似然解的等价性"><a href="#LS解与最大似然解的等价性" class="headerlink" title="LS解与最大似然解的等价性"></a>LS解与最大似然解的等价性</h2><p>若加性误差向量$e=[e_1,… ,e_m]^T$为独立同分布的复高斯随机向量,其概率密度函数为</p><script type="math/tex; mode=display">f(e)=\frac{1}{\pi^m||_e}exp[-(e-ue)^H(e-Ue)]</script><script type="math/tex; mode=display">max \log f(e) ==\frac{1}{\sigma^2}e^He</script><script type="math/tex; mode=display">max f(e) ==min||Ax-b||_2^2</script><p>所以在高斯马尔科夫条件下，矩阵方程的最大似然解等价于最小二乘解，但误差向量方程不同时，两者不相等</p><script type="math/tex; mode=display">\hat{x}_{ML}=\hat{x}_{LS}</script><h1 id="数据最小二乘（DLS）"><a href="#数据最小二乘（DLS）" class="headerlink" title="数据最小二乘（DLS）"></a>数据最小二乘（DLS）</h1><p>与普通最小二乘不同，我们假定b无噪声，数据矩阵$A=A_0+E$有噪声,我们加入校正矩阵$\Delta A$对A进行校准</p><script type="math/tex; mode=display">\begin{array}{l}A=A_0+E\\(A+\Delta A)x=b\end{array}</script><p>我们的问题变为</p><script type="math/tex; mode=display">min||\Delta A||_F^2,s.t.(A+\Delta A)x=b</script><p>我们采用拉格朗日乘子法进行求解</p><script type="math/tex; mode=display">L(x)=||\Delta A||_F^2+\lambda^H(Ax+\Delta Ax-b)</script><script type="math/tex; mode=display">=tr(\Delta A\Delta A^H)+\lambda^H(Ax+\Delta A x-b)</script><script type="math/tex; mode=display">\frac{\partial L(x)}{\partial \Delta A^T}=(\Delta A^H)+x\lambda^H=0</script><script type="math/tex; mode=display">\to \Delta A=-\lambda x^H</script><p>代入约束条件</p><script type="math/tex; mode=display">(A+\Delta A)x=b</script><script type="math/tex; mode=display">-\lambda x^Hx+Ax=b</script><script type="math/tex; mode=display">\lambda x^Hx=Ax-b</script><script type="math/tex; mode=display">\lambda=\frac{Ax-b}{x^Hx}</script><script type="math/tex; mode=display">\Delta A=-\frac{Ax-b}{x^Hx}x^H</script><script type="math/tex; mode=display">J(x)=||\Delta A||_F^2</script><script type="math/tex; mode=display">=tr(\frac{(Ax-b)(Ax-b)^H}{x^Hx})=\frac{||Ax-b||_2^2}{||x||_2^2}</script><p>得到<strong>数据最小二乘</strong></p><script type="math/tex; mode=display">\hat{x}_{DLS}=\arg\min_x\frac{(AX-b)^H(Ax-b)}{x^Hx}</script><h1 id="求导问题"><a href="#求导问题" class="headerlink" title="求导问题"></a>求导问题</h1><p>x为一个列向量</p><ul><li>行向量求偏导<script type="math/tex; mode=display">\frac{\partial}{\partial x^T}=[\frac{\partial}{\partial x_1},...,\frac{\partial}{\partial x_n}]</script></li><li>列向量<script type="math/tex; mode=display">\frac{\partial}{\partial x^T}=[\frac{\partial}{\partial x_1},...,\frac{\partial}{\partial x_n}]^T</script></li><li>example<script type="math/tex; mode=display">\frac{\partial (a^Tx)}{\partial x^T}=a^T</script><script type="math/tex; mode=display">\frac{\partial (a^Tx)}{\partial x}=a</script></li><li>矩阵求导<script type="math/tex; mode=display">L=tr(\Delta A\Delta A^H)+\lambda^H(Ax+\Delta A-b)</script><script type="math/tex; mode=display">\frac{\partial L}{\partial \Delta A^T}=\Delta A^H+x\lambda^H=0</script></li></ul><h1 id="Tikhonov-正则化方法"><a href="#Tikhonov-正则化方法" class="headerlink" title="Tikhonov 正则化方法"></a>Tikhonov 正则化方法</h1><p>因为在真实问题中往往存在秩亏缺，所以在最小二乘的代价函数的基础上我们进行改进，加入了正则化参数，在神经网络的代价函数经常用到</p><script type="math/tex; mode=display">J(x)=min_{x}||Ax-b||_2^2+\lambda||x||_2^2</script><script type="math/tex; mode=display">J(x)=x^HA^HAx-x^HA^Hb-b^HAx+b^Hb+\lambda x^Hx</script><script type="math/tex; mode=display">\frac{\partial J(x)}{\partial x^*}=A^HAx-A^Hb+\lambda x=0</script><script type="math/tex; mode=display">\to (A^HA+\lambda I)x=A^Hb</script><script type="math/tex; mode=display">\hat{x}_{Tik}=(A^HA+\lambda I)^{-1}A^Hb</script><p>这种使用$(A^HA+\lambda I)^{-1}$代替协方差矩阵的直接求逆$(A^HA)^{-1}$的方法常称为Tikhonov正则化(Tikhonov regularization)，或简称正则化方法(regularized method)。</p><p>Tikhonov正则化方法的本质是:通过对秩亏缺的矩阵A 的协方差矩阵 $A^HA$ 的每一个对角元素加一个很小的扰动$\lambda$，使得奇异的协方差矩阵$A^HA$ 的求逆变成非奇异矩阵$(A^HA+\lambda I)$的求逆，从而大大改善求解秩亏缺矩阵方程 Ax = b 的数值稳定性。</p><h2 id="反正则化去燥"><a href="#反正则化去燥" class="headerlink" title="反正则化去燥"></a>反正则化去燥</h2><p>显然，若数据矩阵A 满列秩，但存在误差或者噪声时，就需要采用与Tikhonov正则化相反的做法，对被噪声污染的协方差矩阵$A^HA$ 加一个很小的负扰动矩阵$-\lambda I$，</p><script type="math/tex; mode=display">min_x ||Ax-b||_2^2-\lambda||x||_2^2</script><script type="math/tex; mode=display">\to \hat{x}=(A^HA-\lambda I)^{-1}A^Hb</script><h1 id="总体最小二乘-TLS"><a href="#总体最小二乘-TLS" class="headerlink" title="总体最小二乘(TLS)"></a>总体最小二乘(TLS)</h1><p>在A，b同时存在误差的最小二乘方法</p><script type="math/tex; mode=display">A=A_0+E</script><script type="math/tex; mode=display">b=b_0+e</script><script type="math/tex; mode=display">min||\Delta A,\Delta b||_F^2</script><p>我们的约束优化问题转化为了</p><script type="math/tex; mode=display">min||\Delta A,\Delta b||_2^2=||\Delta A||_2^2+||\Delta b||_2^2,s.t.(A+\Delta A)x=b+\Delta b</script><script type="math/tex; mode=display">([A,b]+[\Delta A,\Delta b])\begin{bmatrix}x\\1\end{bmatrix}=0</script><p>令</p><script type="math/tex; mode=display">A'=[A,b]</script><script type="math/tex; mode=display">\Delta A'=[\Delta A,\Delta b])</script><script type="math/tex; mode=display">x'=\begin{bmatrix}x\\1\end{bmatrix}</script><p>原方程转化为数据最下二乘问题</p><script type="math/tex; mode=display">(A'+\Delta A')x'=0</script><script type="math/tex; mode=display">\to min_{x'}\frac{(A'x'-b')(A'x'-b')^H}{x'^Hx'}</script><script type="math/tex; mode=display">=min\frac{x^{'H}A'^HA'x'}{x'^Hx'}</script><script type="math/tex; mode=display">=min\frac{||Ax-b||_2^2}{||x||_2^2+1}</script><p>令</p><script type="math/tex; mode=display">B=[A,b]</script><p>其奇异值分解为</p><script type="math/tex; mode=display">B=U\Sigma V^H</script><p>总体最小二乘的解为</p><script type="math/tex; mode=display">x'=-\frac{1}{v(n+1,n+1)}V_{min}</script><h2 id="总体最小二乘的几何意义"><a href="#总体最小二乘的几何意义" class="headerlink" title="总体最小二乘的几何意义"></a>总体最小二乘的几何意义</h2><p>和点到线的距离相似$a<em>i^T$是矩阵的第i行，$b_i$是第i个元素，总体最小二乘的解$x</em>{TLS}$是使</p><script type="math/tex; mode=display">min_x\frac{||Ax-b||_2^2}{||x||_2^2+1}=\sum_{i=1}^n\frac{|a_i^Tx-b_i|^2}{x^Tx+1}</script><p>问题转化为了点集$(a_i,b_i)$到直线$b=ax$的最小距离平方和<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/21/3b2eb4fb4bd4c7c4843067e1c0447c01.png" alt=""></p><p>的极小化变量</p><h2 id="总体最小二乘闭式解"><a href="#总体最小二乘闭式解" class="headerlink" title="总体最小二乘闭式解"></a>总体最小二乘闭式解</h2><p>若增广矩阵$B=[A ,b]$的奇异值为$\sigma<em>1≥…\sigma</em>{n+1}$，则总体最小二乘解可表示成</p><script type="math/tex; mode=display">x_{TLS}=(A^HA-\sigma^2_{n+1}I)^{-1}A^Hb</script><p>与Tikhonov 正则化比较知，总体最小二乘是一种反正则化方法，可以解释为一种具有噪声清除作用的最小二乘方法:先从协方差矩阵 $A^TA$中减去噪声影响项$\sigma_{n+1}^2I$，然后再矩阵求逆，得到最小二乘解。</p><h1 id="四种最小二乘的比较"><a href="#四种最小二乘的比较" class="headerlink" title="四种最小二乘的比较"></a>四种最小二乘的比较</h1><h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/21/35c2e184a7f595ebf5643f63e0a4277e.png" alt=""></p><h2 id="对应解"><a href="#对应解" class="headerlink" title="对应解"></a>对应解</h2><script type="math/tex; mode=display">\begin{array}{l}\hat{x}_{LS}=(A^HA)^{-1}A^Hb\\\hat{x}_{Tik}=(A^HA+\lambda I)^{-1}A^Hb\\\hat{x}_{TLS}=(A^HA-\sigma^2_{n+1}I)^{-1}A^Hb\end{array}</script><h2 id="总体最小二乘拟合直线"><a href="#总体最小二乘拟合直线" class="headerlink" title="总体最小二乘拟合直线"></a>总体最小二乘拟合直线</h2><p>普通最小二乘考虑拟合误差平方和最小化,对于直线$m(x-\bar x)+(y-\bar y)=0$，代价函数为</p><script type="math/tex; mode=display">\begin{array}{l}D_{LS}^{(1)}=\sum_{i=1}^n[(x_i-\bar{x})+m(y_i-\bar{y})]^2\\D_{LS}^{(2)}=\sum_{i=1}^n[m(x_i-\bar{x})-(y_i-\bar{y})]^2\end{array}</script><p>对于若干数据点集，我们用直线ax+by+c=0拟合，，总体最小二乘考虑数据点距离平方和的最小化</p><script type="math/tex; mode=display">D_{TLS}=\frac{\sum_{i=1}^n[a(x_i-\hat{x})+b(y_i-\hat{y})]^2}{a^2+b^2}</script><p>我们把D转化为单位向量t与M的乘积<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/21/844232223ed1a6dd8098795480f2b97d.png" alt=""></p><p>对此，我们有定理，单位法向量$t=(a^2+b^2)^{-1/2}[a,b]^T$取矩阵$M^tM$的最小特征值$\lambda<em>{min}$对应的特征向量，距离平方和取最小值$\lambda</em>{min}$</p><script type="math/tex; mode=display">D_{TSL}=\frac{(Mt)^TMt}{||t||_2^2}=\frac{t^TM^TMt}{||t||_2^2}</script><h3 id="example"><a href="#example" class="headerlink" title="example"></a>example</h3><p>对于三个数据点(2,1),(2,4),(5,1)</p><script type="math/tex; mode=display">\begin{array}{l}\bar{x}=\frac{1}{3}(2+2+5)=3\\\bar{y}=\frac{1}{3}(1+4+1)=2\end{array}</script><script type="math/tex; mode=display">M=\begin{bmatrix}2-3&1-2\\2-3&4-2\\5-3&1-2\end{bmatrix}=\begin{bmatrix}-1&-1\\-1&2\\2&-1\end{bmatrix}</script><script type="math/tex; mode=display">M^TM=\begin{bmatrix}6&-3\\-3&6\\\end{bmatrix}=\begin{bmatrix}\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\-\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\end{bmatrix}\begin{bmatrix}9&0\\0&3\end{bmatrix}\begin{bmatrix}\frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{2}}\\\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\end{bmatrix}</script><p>所以法向量$t=[1/\sqrt{2},1/\sqrt{2}]^T$<br>得到拟合直线为</p><script type="math/tex; mode=display">\frac{1}{\sqrt{2}}(x-3)+\frac{1}{\sqrt{2}}(y-2)=0</script><p>距离平方和为</p><script type="math/tex; mode=display">D_{TLS}=3</script><p>如果采用普通最小二乘</p><script type="math/tex; mode=display">D_{LS}^{(1)}=\frac{1}{m^2+1}\sum_{i=1}^n[(x_i-\bar{x})+m(y_i-\bar{y})]^2</script><script type="math/tex; mode=display">\frac{\partial D_{LS}^{(1)}}{\partial m}=6m-3=0</script><h1 id="平均约束的总体最小二乘"><a href="#平均约束的总体最小二乘" class="headerlink" title="平均约束的总体最小二乘"></a>平均约束的总体最小二乘</h1><script type="math/tex; mode=display">[A,b]\begin{bmatrix}x\\-1\end{bmatrix}=0\to Cx=0</script><p>考虑存在于增广矩阵中的噪声矩阵</p><script type="math/tex; mode=display">D=[E,e]</script><p>引入校正矩阵,矩阵中每个列向量都与向量u相关</p><script type="math/tex; mode=display">\Delta C=[G_1u,G_2u...G_{n+1}u]\in R^{m\times(n+1)}</script><p>我们的问题转化为了约束优化问题</p><script type="math/tex; mode=display">\min_{u,x}u^TWu</script><script type="math/tex; mode=display">s.t. (A+\Delta A)x=b+\Delta b</script><script type="math/tex; mode=display">[\Delta A,\Delta b]=[G_1u,G_2u...G_{n+1}u]</script><p>W为加权矩阵，通常为对角矩阵，或者单位矩阵</p><script type="math/tex; mode=display">min||u||_2^2</script>]]></content>
      
      
      <categories>
          
          <category> 《柯西的传世绝学》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 矩阵论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>矩阵特征分析</title>
      <link href="2020/10/21/feature-analysis/"/>
      <url>2020/10/21/feature-analysis/</url>
      
        <content type="html"><![CDATA[<h1 id="特征值求解方法"><a href="#特征值求解方法" class="headerlink" title="特征值求解方法"></a>特征值求解方法</h1><p>如果</p><script type="math/tex; mode=display">L[u]=\lambda u</script><script type="math/tex; mode=display">u =\not 0</script><p>则$\lambda$称为线性算子L的特征值，u为特征向量<br><span id="more"></span></p><p>特征值求解</p><script type="math/tex; mode=display">Au=\lambda u</script><script type="math/tex; mode=display">\to det(A-\lambda I)=0</script><p>如果A为Hermitian矩阵，则</p><script type="math/tex; mode=display">A=U\Sigma U^H</script><script type="math/tex; mode=display">U=[u_1...u_n]^T</script><script type="math/tex; mode=display">\Sigma=diag(\lambda_1...\lambda_2)</script><h2 id="特征值的基本概念"><a href="#特征值的基本概念" class="headerlink" title="特征值的基本概念"></a>特征值的基本概念</h2><ul><li>称A的特征值入具有代数多重度(algebraic multiplicity) u，若$\lambda$是特征多项式<br>det(A - zI)=0的u重根。</li><li>若特征值$\lambda$的代数多重度为1，则称该特征值为单特征值(simple eigenvalue)。非<br>单的特征值称为多重特征值(multiple eigenvalue)。</li><li>称A的特征值入具有几何多重度(geometric multiplicity)~，若与入对应的线性<br>无关特征向量的个数为$\gamma$。换言之，几何多重度，是特征空间Null(A -入I)的维数。</li><li>矩阵A称为减次矩阵(derogatory matrix)，若至少有一个特征值的几何多重度<br>大于1。</li><li>一特征值称为半单特征值(semi-simple eigenvalue)，若它的代数多重度等于它的<br>几何多重度。不是半单的特征值称为亏损特征值(defective eigenvalue)。<h2 id="特征多项式"><a href="#特征多项式" class="headerlink" title="特征多项式"></a>特征多项式</h2><script type="math/tex; mode=display">P(x)=det(A-xI)=P_nx^n...P_1x+P_0</script></li></ul><h3 id="特征方程"><a href="#特征方程" class="headerlink" title="特征方程"></a>特征方程</h3><p>P(x)=0为特征方程</p><h2 id="特征值性质"><a href="#特征值性质" class="headerlink" title="特征值性质"></a>特征值性质</h2><ul><li>矩阵A奇异，当且仅当至少有一个特征值$\lambda$=0。</li><li>矩阵A和$A^T$具有相同的特征值。</li></ul><script type="math/tex; mode=display">\begin{array}{l}A\to \lambda\\A^T\to \lambda\\A^H\to \lambda^*\\A^k\to \lambda^k\\A^{-1}\to \lambda^{-1}\\A+\sigma^2 I\to\lambda+\sigma^2\end{array}</script><ul><li>共轭特征值和共轭特征向量成对出现</li><li>对角矩阵和三角矩阵特征值为对角线元素值</li><li>幂等矩阵$A^2=A$特征值全部为0、1</li><li>实正交矩阵特征值全部在单位圆上</li><li>奇异矩阵至少一个特征值为0</li><li>非奇异矩阵所有特征值非0</li><li>特征值之和=迹<script type="math/tex; mode=display">\sum_{i=1}^n\lambda_i=tr(A)</script></li><li>一个Hermitian矩阵A是正定(或半正定)的，当且仅当它的特征值是正(或者<br>非负)的。</li><li>特征值之积等于行列式<script type="math/tex; mode=display">\prod \lambda=det(A)=|A|</script></li><li><p>特征值与秩的关系:</p><ul><li>若n×n矩阵A有r个非零特征值，则rank(A) ≥T。</li><li>若0是n×n矩阵A的无多重的特征值，则rank(A)= n - 1。</li><li>若rank(A -$\lambda$I)≤n -1，则$\lambda$是矩阵A的特征值。</li></ul></li><li><p>若A的特征值不相同，则一定可以找到一个相似矩阵$S^{-1}AS= D$(对角矩阵)，<br>其对角元素即是矩阵A的特征值。</p></li><li>n xn矩阵A的任何一个特征值$\lambda$的几何多重度都不可能大于$\lambda$的代数多<br>重度。</li><li>Caley-harmilton定理<script type="math/tex; mode=display">\prod(A-\lambda_iI)=0</script></li></ul><ul><li>特征值和相似矩阵<ul><li>若$\lambda$是nxn矩阵A的一个特征值，并且nxn矩阵B非奇异，则入也是矩阵B-1AB的一个特征值，但对应的特征向量一般不相同。</li><li>若$\lambda$是n×n矩阵A的一个特征值，并且n×n矩阵B是酉矩阵，则$\lambda$也是矩阵$B^HAB$的一个特征值，但对应的特征向量一般不相同。</li><li>若$\lambda$是nxn矩阵A的一个特征值，并且nxn矩阵B是正交矩阵，则$\lambda$也是<br>矩阵$B^TAB$的一个特征值，但对应的特征向量一般不相同。</li></ul></li><li>一个n xn矩阵A的最大特征值以该矩阵的列元素之和的最大值为界，<script type="math/tex; mode=display">\lambda_{max}<max\sum_{j=1}^n a_{ij}</script></li><li>随机向量$x(t)= [x_1(t),x_2(t),… ,x_n(t)]^T$的相关矩阵$R=E{x(t)x^H(t)}$的特征值以最大和最小功率为界<script type="math/tex; mode=display">P_{min}<=\lambda<=P_{max}</script><h2 id="矩阵的谱"><a href="#矩阵的谱" class="headerlink" title="矩阵的谱"></a>矩阵的谱</h2>矩阵所有特征值的集合<h3 id="谱半径"><a href="#谱半径" class="headerlink" title="谱半径"></a>谱半径</h3><script type="math/tex; mode=display">\rho=max(\lambda_i)</script><h2 id="矩阵对角化"><a href="#矩阵对角化" class="headerlink" title="矩阵对角化"></a>矩阵对角化</h2></li><li><p>定义： 一个n×n实矩阵A若与一个对角矩阵相似，则称矩阵A是可对角化<br>的(diagonalizable)。</p></li><li><p>定理：一个nxn实矩阵A是可对角化的，当且仅当A具有n个线性无关的<br>特征向量。</p><script type="math/tex; mode=display">U^{-1}AU=\Sigma</script><script type="math/tex; mode=display">U=[u_1...u_n]^T</script><script type="math/tex; mode=display">\Sigma=diag(\lambda_1...\lambda_2)</script></li></ul><h1 id="Caley-harmlton定理及应用"><a href="#Caley-harmlton定理及应用" class="headerlink" title="Caley-harmlton定理及应用"></a>Caley-harmlton定理及应用</h1><p>若</p><script type="math/tex; mode=display">P(x)=P_nA^n+...P_1A+P_0I=O</script><p>则</p><script type="math/tex; mode=display">P(x)=P_nx^n+...P_1x+P_0</script><p>是使A零化的多项式，则</p><script type="math/tex; mode=display">P(A)=0</script><h2 id="可以利用该定理求逆"><a href="#可以利用该定理求逆" class="headerlink" title="可以利用该定理求逆"></a>可以利用该定理求逆</h2><p>左右同乘A的逆</p><script type="math/tex; mode=display">A^{-1}=-\frac{1}{P_0}(P_nA^{n-1}+P_{n-1}A^{n-2}+...+P_1I)</script><ul><li>example<script type="math/tex; mode=display">A=\begin{bmatrix}1&5\\4&6\end{bmatrix}</script><script type="math/tex; mode=display">\operatorname{det}(\boldsymbol{A}-x \boldsymbol{I})=\left|\begin{array}{cc}1-x & 5 \\4 & 6-x\end{array}\right|=(1-x)(6-x)-5 \times 4=x^{2}-7 x-14</script><script type="math/tex; mode=display">\boldsymbol{A}^{-1}=\frac{1}{14}(\boldsymbol{A}-7 \boldsymbol{I})=\frac{1}{14}\left(\left[\begin{array}{ll}1 & 5 \\4 & 6\end{array}\right]-7\left[\begin{array}{ll}1 & 0 \\0 & 1\end{array}\right]\right)=\left[\begin{array}{rr}-\frac{3}{7} & \frac{5}{14} \\\frac{2}{7} & -\frac{1}{14}\end{array}\right]</script></li></ul><h2 id="矩阵指数函数的计算"><a href="#矩阵指数函数的计算" class="headerlink" title="矩阵指数函数的计算"></a>矩阵指数函数的计算</h2><script type="math/tex; mode=display">e^{At}=I+At...\frac{1}{k!}A^kt^k...</script><p>对于常见的一阶微分方程</p><script type="math/tex; mode=display">x'(t)=Ax(t)</script><script type="math/tex; mode=display">x(0)=x_0</script><p>解为</p><script type="math/tex; mode=display">x(t)=e^{At}x_0</script><h3 id="多阶矩阵微分方程"><a href="#多阶矩阵微分方程" class="headerlink" title="多阶矩阵微分方程"></a>多阶矩阵微分方程</h3><p>若A的特征多项式为</p><script type="math/tex; mode=display">p(\lambda)=\lambda^n+c_{n-1}\lambda^{n-1}...+c_1\lambda+c_0</script><p>对于矩阵微分方程</p><script type="math/tex; mode=display">Y^{(n)}(t)+c_{n-1}Y^{(n-1)}(t)...Y(t)=O</script><p>且满足初始条件</p><script type="math/tex; mode=display">\begin{array}{l}Y(0)=I\\Y'(0)=A\\...\\Y^{(n-1)}(t)=A^{n-1}\end{array}</script><p>则$Y(t)=e^{At}$为矩阵方程唯一解</p><p>那么如何求解这个唯一解？</p><script type="math/tex; mode=display">Y(t)=x_1(t)I+x_2(t)A+...x_nA^{n-1}</script><p>其中$x_k(t)$满足微分方程</p><script type="math/tex; mode=display">x^{(n)}(t)+c_{n-1}x^{(n-1)}(t)...c_1x'(t)+c_0x(t)=0</script><p>且满足初始条件</p><script type="math/tex; mode=display">x_i^{(i-1)}=1</script><p>其他所有为0</p><hr><h1 id="特征分解的应用"><a href="#特征分解的应用" class="headerlink" title="特征分解的应用"></a>特征分解的应用</h1><p>给定一组彼此相关的随机变量，常常希望通过线性变换，把它变换成另外一组统计不相关的随机变量。甚至更进一步，希望变换后的一组统计不相关随机变量各个分量还具有单位方差。这两个任务可以通过标准正交变换和迷向圆变换分别完成。</p><h2 id="标准正交变换"><a href="#标准正交变换" class="headerlink" title="标准正交变换"></a>标准正交变换</h2><p>x 为m维随机向量，将x转化为0均值的随机向量，此时自相关矩阵和协方差矩阵相同，$m_x$为其均值向量</p><script type="math/tex; mode=display">x_0=x-m_x</script><script type="math/tex; mode=display">R_{x_0}=C_x</script><p>其特征分解为</p><script type="math/tex; mode=display">C_x=U_x\Sigma_xU_x^H</script><p>令</p><script type="math/tex; mode=display">w=U_x^Hx_0=U_x^H(x-m_x)</script><script type="math/tex; mode=display">m_w=0</script><script type="math/tex; mode=display">C_w=R_w=U_x^HU_x\Sigma_x U_x^HU_x=\Sigma_x</script><blockquote><p>该变化用于有色噪声白化</p></blockquote><h2 id="迷向圆变化"><a href="#迷向圆变化" class="headerlink" title="迷向圆变化"></a>迷向圆变化</h2><p>在上面的标准正交变换中，线性变换$w= U^H_x(x-m_x)$的自相关矩阵(与协方差矩阵相等)<br>$R_x$。为对角矩阵，但不是单位矩阵Ⅰ。要使u的自相关矩阵为单位矩阵，就需要对w再<br>作另一个线性变换</p><script type="math/tex; mode=display">y=\Sigma_x^{-1/2}w=\Sigma_x^{-1/2}U_x^H(x-m_x)</script><p>则</p><script type="math/tex; mode=display">R_y=\Sigma_x^{-1/2}\Sigma\Sigma_x^{-1/2}=I</script><h2 id="离散K-L变换"><a href="#离散K-L变换" class="headerlink" title="离散K-L变换"></a>离散K-L变换</h2><p>在许多信号处理和模式识别应用中，常常需要将随机信号的观测样本用另外一组数(或系数)表示，同时使这种新的表示具有某些所希望的性质。例如，对于编码而言，希望信号可以用少数系数表示，同时这些系数集中了原信号的功率。又如，对于最优滤波，则希望变换后的样本统计不相关，这样就可以降低滤波器的复杂度，或者提高信噪比。实现上述目标的通用做法是将信号展开成正交基函数的线性组合，使得信号相对于基函数的各个分量不会相互干扰。</p><p>如果正交基函数根据信号观测样本的协方差矩阵适当选择，就有可能在所有正交基函数中，获得具有最小均方误差的信号表示。在均方误差最小的意义上，这样一种信号表示是最优的信号表示，它在随机信号的分析与编码中具有重要的意义和应用。这种信号变换是Karhunen和 Loeve 针对连续随机信号提出的，称为Kauhunen-Loeve变换。</p><p>对于M维向量</p><script type="math/tex; mode=display">x=[x_1,x_2...x_M]^T</script><p>是一零均值向量</p><p>我们用小维度的w表示x,$Q$为一个酉矩阵</p><script type="math/tex; mode=display">W=Q^Hx</script><script type="math/tex; mode=display">x=QW=\sum_{i=1}^Mq_iw_i</script><p>为了减小变换后的系数u;的个数,假定在上式中只使用w的前m个系数$w_1,…, w_m(m = 1,… M)$逼近随机信号向量c，即</p><script type="math/tex; mode=display">\hat{x}=\sum_{i=1}^mq_iw_i ,( 1=<m<=M)</script><h2 id="主分量分析"><a href="#主分量分析" class="headerlink" title="主分量分析"></a>主分量分析</h2><p>组分量分析的核心思想是吧，存在信息冗余的特征向量空间，通过正交变换进行降维，变成无信息冗余的向量空间</p><p>令Rx是数据向量x 的自相关矩阵，它有K个主特征值，与这些主特征值对应的K个特征向量称为数据向量x的主分量。</p><h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><ul><li>降维</li><li>正交化</li><li>功率最大化</li></ul><script type="math/tex; mode=display">\hat{x}=Q^Hx=\sum_{i=1}^p q_ix_i\approx \sum_{i=1}^kq_ix_i(k<p)</script><h1 id="广义特征分解"><a href="#广义特征分解" class="headerlink" title="广义特征分解"></a>广义特征分解</h1><script type="math/tex; mode=display">Au=\lambda Bu</script><script type="math/tex; mode=display">det(A-\lambda B)=0</script><h1 id="Rayleigh商"><a href="#Rayleigh商" class="headerlink" title="Rayleigh商"></a>Rayleigh商</h1><h2 id="Rayleigh商定义及其性质"><a href="#Rayleigh商定义及其性质" class="headerlink" title="Rayleigh商定义及其性质"></a>Rayleigh商定义及其性质</h2><p>Hermitian矩阵 $A\in C^{n\times n}$的 Rayleigh商或Rayleigh-Ritz 比 R(z)是一个标量，定义为</p><script type="math/tex; mode=display">R(x)=\frac{x^HAx}{x^Hx}</script><blockquote><p>瑞利商的目的是找到x让瑞利商最大或者最小,常在滤波中使用</p></blockquote><p>假设信号如下，如果要对信号进行滤波，利用滤波器x，让滤波后信噪比最大</p><script type="math/tex; mode=display">r(t)=B\alpha S(t)+n(t)</script><ul><li>滤波器$x^H$进行滤波后，SNR为<script type="math/tex; mode=display">\frac{|x^HB\alpha S(t)|^2}{|x^Hn|^2}</script><script type="math/tex; mode=display">=\frac{x^HAx}{x^Hn^Hnx}(噪声为白噪声时)</script><script type="math/tex; mode=display">=\frac{x^HAx}{\sigma^2 x^Hx}</script></li></ul><h2 id="Rayleigh-Ritz定理"><a href="#Rayleigh-Ritz定理" class="headerlink" title="Rayleigh-Ritz定理"></a>Rayleigh-Ritz定理</h2><p>取x取最大最小特征值对应的特征向量时可以得到极值，极值为最大或最小特征值</p><script type="math/tex; mode=display">Ax=\lambda x</script><script type="math/tex; mode=display">max \frac{x^HAx}{x^Hx}=\lambda_{max}</script><script type="math/tex; mode=display">min \frac{x^HAx}{x^Hx}=\lambda_{min}</script><h2 id="广义瑞利商"><a href="#广义瑞利商" class="headerlink" title="广义瑞利商"></a>广义瑞利商</h2><p>当噪声不为白噪声时，噪声相关矩阵为B</p><script type="math/tex; mode=display">R(x)=\frac{x^HAx}{x^HBx}</script><p>应用变量代换</p><script type="math/tex; mode=display">\hat{x}=B^{1/2}x</script><script type="math/tex; mode=display">R(\hat{x})=\frac{\hat{x}^H(B^{-1/2})^HAB^{1/2}\hat{x}}{\hat{x}^H\hat{x}}</script><p>最大最小值为矩阵</p><script type="math/tex; mode=display">(B^{-1/2})^HAB^{1/2}</script><p>的最大或最小特征值</p><p>变量代换后求广义特征值</p><script type="math/tex; mode=display">Ax=\lambda Bx</script>]]></content>
      
      
      <categories>
          
          <category> 《柯西的传世绝学》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 矩阵论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>奇异值SVD</title>
      <link href="2020/10/21/SVD/"/>
      <url>2020/10/21/SVD/</url>
      
        <content type="html"><![CDATA[<h1 id="数值的稳定性与条件数"><a href="#数值的稳定性与条件数" class="headerlink" title="数值的稳定性与条件数"></a>数值的稳定性与条件数</h1><p>当输入d受到扰动时，输出f(d) 的扰动情况<br><span id="more"></span><br>目的：解决病态问题</p><h2 id="条件数"><a href="#条件数" class="headerlink" title="条件数"></a>条件数</h2><ul><li>A 数据矩阵 n*n</li><li>b 数据向量 n*1</li><li>x 未知向量 n*1</li></ul><p>在这里我们研究x受A，或者b扰动的影响</p><script type="math/tex; mode=display">A(x+\delta x)=b+\delta b</script><script type="math/tex; mode=display">A\delta x=\delta b</script><script type="math/tex; mode=display">\delta x=A^{-1}\delta b</script><p>由</p><script type="math/tex; mode=display">||AB||<=||A|| ||B||</script><script type="math/tex; mode=display">\to\begin{cases}||\delta x||_2<=||A^{-1}||_2 ||\delta b||_2\\||b||_2<=||A||_2 ||x||_2\to \frac{1}{||x||_2}<=\frac{||A||_2}{||b||_2}\end{cases}</script><p>两边相乘</p><script type="math/tex; mode=display">\to \frac{||\delta x||_2}{||x||_2}<=||A^{-1}||_2 ||A||_2 \frac{||\delta b||_2}{||b||_2}</script><p>一起考虑A扰动的影响，可以得到</p><script type="math/tex; mode=display">(A+\delta A)(x+\delta x)=b</script><script type="math/tex; mode=display">\begin{array}{l}\delta x=(A+\delta A)^{-1}b-A^{-1}b\\=A^{-1}[A-(A+\delta A)](A+\delta A)^{-1}b\\=A^{-1}(-\delta A)(A+\delta A)^{-1}b\\\to \frac{||\delta x||_2}{||x+\delta x||_2}<=||A^{-1}||||A||_2\frac{||\delta A||_2}{||A||_2}\end{array}</script><p>可以看出噪声（解向量x的误差）与某个数成正比，这个数被称为<strong>条件数</strong></p><script type="math/tex; mode=display">cond(A)=||A^{-1}||_2||A||_2</script><p>有性质</p><script type="math/tex; mode=display">cond(A^HA)=[cond(A)]^2</script><p>如果矩阵A不是方阵，可以求出最小二乘解</p><script type="math/tex; mode=display">Ax=b\to x=(A^HA)^{-1}A^{H}b</script><p>求此时的条件数</p><script type="math/tex; mode=display">cond(A^HA)=||A^HA||_2||(A^HA)^{-1}||_2=cond(A)^2</script><p>条件数平方后变大，所以此时的最小二乘解不稳定 </p><h1 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h1><p>奇异值分解定理对于矩阵$A:m\times n$,存在正交或者酉矩阵$U:m\times m,V:n\times n$使得</p><script type="math/tex; mode=display">A=U\Sigma V^H</script><script type="math/tex; mode=display">\Sigma=\begin{bmatrix}\Sigma_r &0\\0 &0\end{bmatrix}</script><script type="math/tex; mode=display">diag(\Sigma_r)=[\sigma_1,\sigma_2...\sigma_r]</script><script type="math/tex; mode=display">r=rank(A)</script><p>$\sigma<em>1,\sigma_2…\sigma_r，\sigma</em>{r+1}=…\sigma_n=0$被称为A的奇异值</p><script type="math/tex; mode=display">AV=U\Sigma</script><script type="math/tex; mode=display">\to Av_i=\begin{cases}u_i\sigma_i &i<=r\\0&i>r\end{cases}</script><script type="math/tex; mode=display">U^HA=\Sigma V^H</script><script type="math/tex; mode=display">u_i^HA=\sigma_iv_i^H</script><p>$v_i$为右奇异向量,$u_i$为左奇异向量</p><ul><li>矩阵A的奇异值分解式可以改写成向量表达式<script type="math/tex; mode=display">A=\Sigma_{i=1}^r\sigma_i u_i v_i^H</script></li><li>当矩阵的秩$r=rank(A)&lt;min(m,n)$<br>奇异值分解可以被简化为</li></ul><script type="math/tex; mode=display">A=U_r\Sigma_rV_r^H</script><p>被称为截断奇异值分解，<br>只取前r个非0度奇异值</p><h2 id="奇异值分解和特征分解的关系"><a href="#奇异值分解和特征分解的关系" class="headerlink" title="奇异值分解和特征分解的关系"></a>奇异值分解和特征分解的关系</h2><script type="math/tex; mode=display">A^HA=V\Sigma U^HU\Sigma V^H</script><script type="math/tex; mode=display">=V\Sigma^2V^H</script><ul><li>$A^HA$的特征值是A奇异值的平方，是右奇异向量</li><li>$AA^H$的特征值是A奇异值的平方，是左奇异向量<script type="math/tex; mode=display">AA^H=U\sum^2U^H</script></li><li>广义逆<script type="math/tex; mode=display">A^\dagger=V\Sigma^\dagger U^H</script><h2 id="奇异值的性质"><a href="#奇异值的性质" class="headerlink" title="奇异值的性质"></a>奇异值的性质</h2><h3 id="范数性质"><a href="#范数性质" class="headerlink" title="范数性质"></a>范数性质</h3></li><li>诱导谱范数<script type="math/tex; mode=display">||A||_{spec}=||A||_2=\sigma_i(最大奇异值)</script></li><li>f范数<script type="math/tex; mode=display">||A||_F=sqrt{<A,A>}=\sqrt{tr(A^H,A)}=\sqrt{\sum \sigma_i}</script>所以特征值之和=奇异值平方和<script type="math/tex; mode=display">\lambda_1+\lambda_2...+\lambda_r=\sigma_1^2+\sigma_2^2...+\sigma_r^2</script></li></ul><blockquote><p>根据矩阵A的谱范数可以确定最大奇异值</p></blockquote><h3 id="酉不变性"><a href="#酉不变性" class="headerlink" title="酉不变性"></a>酉不变性</h3><script type="math/tex; mode=display">A=U\Sigma V^H</script><p>在矩阵A的基础上再左乘或者右乘一个酉矩阵</p><script type="math/tex; mode=display">B=PU\Sigma V^HQ^H</script><p>B的奇异值不变</p><h3 id="奇异值与行列式"><a href="#奇异值与行列式" class="headerlink" title="奇异值与行列式"></a>奇异值与行列式</h3><p>因为酉矩阵行列式绝对值为1</p><p>所以</p><script type="math/tex; mode=display">|detA|=|det\sum|=\sigma_1\sigma_2...\sigma_n</script><p>如果$detA\not=0$表明A是非奇异的，若存在$\sigma_i=0,datA=0$,则A奇异，这是把$\sigma$称为奇异值的原因</p><h3 id="奇异值与条件数"><a href="#奇异值与条件数" class="headerlink" title="奇异值与条件数"></a>奇异值与条件数</h3><script type="math/tex; mode=display">cond(A)=\sigma_1/\sigma_p(p=\{m,n\})</script><p>所以条件数总大于1</p><blockquote><p>奇异值的内在含义，如果一个矩阵有一个奇异值为0，对于正方矩阵，代表，矩阵奇异，行列式为0，非正方矩阵代表秩亏缺</p></blockquote><h2 id="秩亏缺矩阵（LS）"><a href="#秩亏缺矩阵（LS）" class="headerlink" title="秩亏缺矩阵（LS）"></a>秩亏缺矩阵（LS）</h2><p>应用奇异值分解求解最小二乘问题的方法常简称为奇异值分解方法。</p><script type="math/tex; mode=display">\begin{array}{l}Ax=b\\U\Sigma V^Hx=b\\\Sigma V^Hx=U^Hb\end{array}</script><p>令</p><script type="math/tex; mode=display">x'=V^Hx,y=U^Hb</script><p>得到</p><script type="math/tex; mode=display">\to \Sigma x'=y</script><p>我们的目标变为优化问题</p><script type="math/tex; mode=display">min||\Sigma x'-y||_2^2=\sum_{i=1}^r|\sigma_ix_i-y_i|^2+\sum_{i=r+1}^N|y_i|^2=0</script><p>得到最小范数解</p><script type="math/tex; mode=display">x_{LS}=\sum_{i=1}^r(u_i^Hb/\sigma_i)v_i</script><p>相应的最小残差为</p><script type="math/tex; mode=display">\rho_{LS}=||Ax_{LS}-b||_2=||[u_{r+1},..,u_m]^Hb||_2</script><h3 id="有效秩的确定"><a href="#有效秩的确定" class="headerlink" title="有效秩的确定"></a>有效秩的确定</h3><p>如果要衡量衡量A，B的逼近程度，P,K分别为A、B的秩</p><script type="math/tex; mode=display">A=\sum_{i=1}^P\sigma_i u_i v_i^H</script><script type="math/tex; mode=display">B=\sum_{i=1}^K\sigma_i u_i v_i^H</script><ul><li>谱范数法<script type="math/tex; mode=display">||A-B||_{spec}=\sigma_{k+1}</script></li><li>F范数法<script type="math/tex; mode=display">||A-B||_F=\sqrt{\sum_{i=k+1}^P\sigma_i^2}</script><h4 id="衡量标准"><a href="#衡量标准" class="headerlink" title="衡量标准"></a>衡量标准</h4></li></ul><p>对应谱范数</p><script type="math/tex; mode=display">\epsilon=\frac{\sigma_k}{\sigma_1}>=5\%</script><p>对应F范数（范数比方法）</p><script type="math/tex; mode=display">V(k)=\frac{\sqrt{\sum^k\sigma^2}}{\sqrt{\sum^P\sigma^2}}</script><script type="math/tex; mode=display">=\frac{||A_k||_F}{||A||_F}>=\alpha</script><h3 id="奇异值分解在图像视频压缩中的作用"><a href="#奇异值分解在图像视频压缩中的作用" class="headerlink" title="奇异值分解在图像视频压缩中的作用"></a>奇异值分解在图像视频压缩中的作用</h3><p>在矩阵A稀疏的情况下</p><script type="math/tex; mode=display">A \to n\times n</script><script type="math/tex; mode=display">A_k=U_k\sum_kV_k^H</script>]]></content>
      
      
      <categories>
          
          <category> 《柯西的传世绝学》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 矩阵论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Beam Alignment and Tracking for Millimeter Wave Communications via Bandit Learning</title>
      <link href="2020/10/19/Bandit/"/>
      <url>2020/10/19/Bandit/</url>
      
        <content type="html"><![CDATA[<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Millimeter wave (mmwave) communications have attracted increasing attention thanks to the abundant spectrum resource. The short wave-length of mmwave signals facilitates exploiting large antenna arrays to achieve large array gains and combat the large path-loss. However, the use of large antenna arrays along with narrow beams leads to a large overhead in beam training for obtaining channel state information, especially in dynamic environments. To reduce the overhead of beam training, in this paper we formulate the problem of <strong>beam alignment and tracking</strong> (BA/T) as a <strong>stochastic bandit</strong> problem. In particular, to sense the change of the environments, the actions<br>are designed based on the offset of successive beam indexes，beam index difference), which measures the rate of change of the envir-onments. Then, we propose two efficient BA/T algorithms based on the stochastic bandit learning. To reveal useful insights,the performance of effective achievable rate is further analyzed for the proposed BA/T algorithms. The analytical results show that the algorithms can sense the change of the environments and adjust beam training strategies intelligently. In addition, they do not require any priori knowledge of dynamic channel modeling, and thus are applicable to a variety of complicated scenarios. Simulation results demonstrate the effectiveness and superiority of the proposed algorithms.<br><span id="more"></span></p><p><strong>Index Terms— Beam alignment, beam tracking, beam training,<br>stochastic bandit learning, millimeter wave communication.</strong></p><p>简单来说，就是毫米波通信比较严重的路径损耗，在动态环境的波束对准和跟踪问题中需要新的算法，本文使用了多臂老虎机算法进行了优化</p><hr><h1 id="什么是多臂老虎机？"><a href="#什么是多臂老虎机？" class="headerlink" title="什么是多臂老虎机？"></a>什么是多臂老虎机？</h1><h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><p>赌场的老虎机有一个绰号叫单臂强盗（single-armed bandit），因为它即使只有一只胳膊，也会把你的钱拿走。而多臂老虎机（或多臂强盗）就从这个绰号引申而来。假设你进入一个赌场，面对一排老虎机（所以有多个臂），由于不同老虎机的期望收益和期望损失不同，你采取什么老虎机选择策略来保证你的总收益最高呢？这就是经典的多臂老虎机问题。</p><hr><p>这个经典问题集中体现了在线学习及更宽泛的强化学习中一个核心的权衡问题：我们是应该探索（exploration）去尝试新的可能性，还是应该守成（exploitation），坚持目前已知的最好选择?在多臂老虎机问题中，探索意味着去玩还没玩过的老虎机，但这有可能使你花太多时间和金钱在收益不好的机器上；而守成意味着只玩目前为止给你收益最好的机器，但这又可能使你失去找到更好机器的机会。而类似抉择在日常生活中随处可见：去一个餐厅，你是不是也纠结于是点熟悉的菜品，还是点个新菜？去一个地方，是走熟知的老路还是选一条新路？而探索和守成的权衡就是在线学习的核心。</p><hr><p>如果我们考虑吧这种算法应用在通信的波束对准场景，通过环境感知来减少训练开销</p><h1 id="System-Model"><a href="#System-Model" class="headerlink" title="System Model"></a>System Model</h1><p>我们只考虑一个点对点的毫米波通讯系统</p><ul><li>发射端 N个天线</li><li>接收端一个天线</li></ul><p>我们构造长度为M的波束码本</p><script type="math/tex; mode=display">C={f_i=a(-1+2i/M)|i=0,1,2,...M-1}</script><p>a表示响应向量</p><script type="math/tex; mode=display">a(x)=\frac{1}{N}[1,e^{j2\pi/\lambda dx},e^{j2\pi/\lambda 2dx}...e^{j2\pi/\lambda(N-1) dx}]</script><ul><li>λ:信号波长</li><li>d：天线之间的距离</li></ul><p>由于毫米波通道的稀疏性，我们采用了扩展的SalehValenzuela几何通道模型。 给出了BS与UE之间的信道向量</p><script type="math/tex; mode=display">h=\sqrt{N/\beta}\sum_{l=1}^L\alpha_l a(\phi_l)</script><ul><li>$\beta$平均路损</li><li>L:信道数量</li><li>$\alpha_l$第l个信道的路径复增益</li><li>$\phi_l=cos\theta_l,\theta_l$为第l个信道的发射角（AoD）</li></ul><p><strong>在UE处接受信号</strong></p><script type="math/tex; mode=display">y_i=\sqrt{P}h^Hf_is+n_i</script><ul><li>P：传输的功率</li><li>s:试点序列</li><li><p>$n_i==N(0,I)$噪声向量</p><p>每个时隙由两个阶段组成，即波束训练和数据传输，采用有效可达速率(EAR)作为度量BA/T算法吞吐量性能的度量，定义为</p></li></ul><script type="math/tex; mode=display">R_{eff}=(1-T_B/T_S)\log(1+P|h^Hf_i|^2)</script><ul><li>$T_B$训练时间</li><li>$T_S$总时间</li></ul><p>数据吞吐量的大小和增益之间存在矛盾</p><h1 id="Stochastic-Bandit-and-learning-policies"><a href="#Stochastic-Bandit-and-learning-policies" class="headerlink" title="Stochastic Bandit and learning policies"></a>Stochastic Bandit and learning policies</h1><p>随机老虎机可以定义为一个集合${P<em>a|a\in A}$和期望$\mu_a$,A是所有可行的操作，学习者和环境进行n轮互动，在每轮互动中，学习者从$A_t\in A$中选取一个操作，环境会给予反馈$R_t\in [0,1]$,学习者的目标是最大化反馈$S_n=\sum</em>{t=1}^n R_t$<br>。 由于Sn是一个随机数量，取决于所选择的动作和抽样的奖励，在实践中，我们试图最大化它的期望。</p><p>下一个动作将会被过去的动作的时间序列和反馈决定，$S_n$的在行动$\pi$期望定义为</p><script type="math/tex; mode=display">E_n(\pi,\varXi)=E_{\pi,\varXi}(\sum_{t=1}^nR_t)</script><p>在本文中我们把动作定义为集合</p><script type="math/tex; mode=display">A=K=\{1,2,...,K\}</script><p>在一个n轮互动中选择动作a的次数定义为</p><script type="math/tex; mode=display">T_a(n)=\sum_{t=1}^nII\{A_t=a\}</script><p>在前面讲到过随机老虎机的本质问题是探索未知选项和保留已有优质选择的矛盾，我们在这里提出两种算法$\epsilon$贪婪算法和UCB算法来平衡矛盾</p><h1 id="epsilon-greedy-and-UCB"><a href="#epsilon-greedy-and-UCB" class="headerlink" title="$\epsilon$greedy and UCB"></a>$\epsilon$greedy and UCB</h1><h2 id="epsilon-greedy"><a href="#epsilon-greedy" class="headerlink" title="$\epsilon$greedy"></a>$\epsilon$greedy</h2><p>$\bar{\mu}_a(t)$表示t轮后从arm a得到的平均反馈</p><script type="math/tex; mode=display">\bar{\mu}_a(t)=\frac{1}{T}\sum_{s=1}^t\{A_s=a\}R_s</script><p>$\epsilon$贪婪算法由参数$\epsilon_1,\epsilon_2…$决定，首先会选择所有arm一次，然后有$1-\epsilon_t$的概率选择$A_t=arg max_a \hat{\mu}_a(t-1)$,否则随机选择选择另一个arm</p><ul><li>input：action space[K] and exploration parameter $\epsilon_t$</li><li>initialize:choose each arm once and let t=K+1</li><li>repeat<ul><li>$A_t=argmax_a\hat{\mu}_a(t-1)$ has highest reward</li><li>choose with$1-\epsilon_t$possibility and otherwise choose other arm randomly</li><li>t=t+1</li></ul></li></ul><h2 id="UCB-upper-confidence-Bound"><a href="#UCB-upper-confidence-Bound" class="headerlink" title="UCB:upper confidence Bound"></a>UCB:upper confidence Bound</h2><script type="math/tex; mode=display">UCB_a(t-1)=\begin{cases}\infty&if T_a(t-1)=0\\\hat{\mu}_a(t-1)+\sqrt{\frac{c\log(t)}{T_a(t-1)}}&otherwise\end{cases}</script><p>在第t轮选择$A_t$为</p><script type="math/tex; mode=display">argmax_a UCB_a(t-1)</script><ul><li>input :action space $[K]={1,2,…K}$</li><li>initialize:t=1</li><li>repeat<ul><li>choose arm $A_t=argmax_aUCB_a(t-1)$</li><li>play arm $A_t$and receive reward $R_t$</li><li>update the UCB value $UCB_{A_t}(t)$</li><li>t=t+1</li></ul></li></ul><h1 id="Stochastic-bandit-based-beam-aligment-and-tracking-algogithms"><a href="#Stochastic-bandit-based-beam-aligment-and-tracking-algogithms" class="headerlink" title="Stochastic bandit based beam aligment and tracking algogithms"></a>Stochastic bandit based beam aligment and tracking algogithms</h1><p>现在我们有了通信场景模型和老虎机算法的基本概念，我们问题的核心转化为了如何设计动作和反馈，在这里我们不是把每个波束定义为一个arm，而是基于两个波束之差定义</p><h2 id="基于老虎机的通讯场景建模"><a href="#基于老虎机的通讯场景建模" class="headerlink" title="基于老虎机的通讯场景建模"></a>基于老虎机的通讯场景建模</h2><ul><li>波束编码差</li></ul><p>比如，当前波束是10，在环境缓慢变化时，下一波束可能为11</p><p>11-10=1</p><p>环境快速变化时下一波束可能是7</p><p>10-7=3</p><p>我们用（u，b）两个值来定义一个arm，u为波束差，b为扫描的波束数量，动作空间可以描述为</p><script type="math/tex; mode=display">B=\{(u_1,b_1)...(u_L,b_L)\}</script><p>L为空间大小，u为了测量环境的平均变化率，b测试变化方差</p><p>若当前时刻t的波束为$f_c$,t+1时刻的波束在以下集合选取</p><script type="math/tex; mode=display">F_{u,b}=\{f_{c+u},f_{c+u+1}...f_{c+u+b-1}\}</script><p>那么我们如何定义反馈，在上文通讯模型中我们定义过接受信号强度</p><script type="math/tex; mode=display">y_i=\sqrt{P}h^Hf_is+n_i</script><p>平均接受功率为</p><script type="math/tex; mode=display">Y_i=|1^Ty_i|^2/L_P</script><p>$L_P$为导频序列长度</p><p> 下一次时隙t1用于数据传输的光束应该是$f^i<em>{u，b}，i</em>{u,b}=argmax<em>i{Y_i|f_i \in F</em>{u,b}}$ 设T0表示发射一个/单位导频信号的持续时间。 对应于arm(u，b)的奖励，由$R_{u,b}$表示，定义为</p><script type="math/tex; mode=display">R_{u,b}=(1-bT_0/T_s)\log(1+P|h^Hf^i_{y,b}|^2)^6</script><p>$bT_0$为训练波束的时间开销</p><p>所以此时我们的目标是优化时间1到n的EAR</p><script type="math/tex; mode=display">max_{F_t}E(\sum_{t=1}^nR_{eff,t})</script><h2 id="波束对准和跟踪算法"><a href="#波束对准和跟踪算法" class="headerlink" title="波束对准和跟踪算法"></a>波束对准和跟踪算法</h2><ul><li>input <ul><li>codebook</li><li>action space</li></ul></li><li>find out the initial optimal beam from code book</li><li>初始化老虎机参数</li><li>repeat<ul><li>choose optimal arm$(u^<em>,b^</em>)$</li><li>choose optimal beam$f^*$</li><li>transmit data with beam $f^*$</li><li>update bandit information of the policy</li></ul></li></ul><h3 id="epsilon-greedy-1"><a href="#epsilon-greedy-1" class="headerlink" title="$\epsilon$greedy"></a>$\epsilon$greedy</h3><ul><li>input <ul><li>codebook</li><li>action space with size K</li></ul></li><li>initialize<ul><li>$\epsilon$sequence${\epsilon_n\in (0,1]n&gt;=1 }$</li><li>let $T<em>{u_i,b_i}=0$and$\hat{\mu}</em>{u_i,b_i}=0(i\in K)$</li></ul></li><li>find out an initial optimal beam from code book</li><li>choose each arm once,update bandit information,let n=K+1</li><li>repeat <ul><li>let$(u^<em>,b^</em>)$be the arm with highest current average reward</li><li>play $(u^<em>,b^</em>)$with probability $1-\epsilon_n$and playa random arm with probalility $\epsilon_n/K$</li><li>choose an optimal beam from $F_{u^<em>,b^</em>}$,to transmit data</li><li>update $T<em>{u^<em>,b^</em>}$,and $\hat{\mu}</em>{u^*,b}$</li><li>update counter:n=n+1</li></ul></li></ul><ul><li>$T_{u,b}$为arm（u，b）被选择的次数</li></ul><h3 id="UCB-Based-BA-T-Algorithm"><a href="#UCB-Based-BA-T-Algorithm" class="headerlink" title="UCB Based BA/T Algorithm"></a>UCB Based BA/T Algorithm</h3><ul><li>input <ul><li>codebook C</li><li>$B={(u_i,b_i)}$ of size K</li></ul></li><li>find an initial optimal beam from codebook C</li><li>play each arm once and update bandit information according to and n=K+1</li><li>repeat <ul><li>compute $UCB<em>{u_i,b_i}(n)$for each $(u_i,b_i)$and let $(u^<em>,b^</em>)=argmax</em>{(u<em>i,b_i)}UCB</em>{u_i,b_i}(n-1$</li><li>play chosen arm</li><li>choose $F_{u^*,b}$to transmit data</li><li>update $T<em>{u^*,b}$and$\hat{\mu}</em>{u,b}$</li><li>n=n+1</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 《搬砖狗的日常》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> Beam tracking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>信息熵</title>
      <link href="2020/10/06/xinxilun/"/>
      <url>2020/10/06/xinxilun/</url>
      
        <content type="html"><![CDATA[<h1 id="信息的基本分类"><a href="#信息的基本分类" class="headerlink" title="信息的基本分类"></a>信息的基本分类</h1><ul><li>语法信息</li><li>语义信息</li><li>语用信息<span id="more"></span><h1 id="符号系统"><a href="#符号系统" class="headerlink" title="符号系统"></a>符号系统</h1></li><li>X，Y：随机变量</li><li>$x_k,y_j$ 变量取值</li><li>$a_k,b_j$变量取值</li><li>$\chi={x_k;k=1,2…K},\gamma={y_j;j=1,2…,J}$</li><li>事件：$X=x_k$</li><li>$q_k=Pr{X=x_k}$</li></ul><h1 id="事件自信息"><a href="#事件自信息" class="headerlink" title="事件自信息"></a>事件自信息</h1><p>我们将特定事件$X=x_k$发生后给外界带来的信息量定义为事件的自信息</p><script type="math/tex; mode=display">I(X=x_k)=I(x_k)=-\log q(x_k)</script><p>a=2的单位为比特</p><p>a=e的单位为奈特</p><blockquote><p>事件自信息的本质既是事件对外界提供的信息，也是外界观察心信息付出的代价,通常认为概率越小的事件的信息量越大</p></blockquote><h2 id="条件自信息"><a href="#条件自信息" class="headerlink" title="条件自信息"></a>条件自信息</h2><script type="math/tex; mode=display">I(x_k|y_j)=-\log_ap(x_k|y_j)</script><blockquote><p>事件Y=yj发生后X=xk发生给外界带来的信息</p><h2 id="联合自信息"><a href="#联合自信息" class="headerlink" title="联合自信息"></a>联合自信息</h2></blockquote><script type="math/tex; mode=display">I(x_k,y_j)=-\log_ap(x_k,y_j)</script><blockquote><p>X=xk,Y=yj一起发生的信息量</p></blockquote><h2 id="事件互信息"><a href="#事件互信息" class="headerlink" title="事件互信息"></a>事件互信息</h2><script type="math/tex; mode=display">I(x_k;y_j)=\log_a\frac{p(x_k|y_j)}{q(x_k)}</script><blockquote><p>互信息的本质为事件Y=yj<br>中包含的有关事件X=xk信息量，即可以是事件X发生的信息量减去事件Y发生后事件X还能给外界提供的信息量</p></blockquote><script type="math/tex; mode=display">I(x_k;y_j)=-\log q(x_k)-[-\log p(x_k|y_j)]</script><h3 id="互信息的对称性"><a href="#互信息的对称性" class="headerlink" title="互信息的对称性"></a>互信息的对称性</h3><script type="math/tex; mode=display">I(x_k;y_j)=I(y_j;x_k)</script><h3 id="互信息的性质"><a href="#互信息的性质" class="headerlink" title="互信息的性质"></a>互信息的性质</h3><script type="math/tex; mode=display">I(x_k;y_j)=\begin{cases}>0 & p(x_k|y_j)>q(x_k)\\=0 & X,Y独立\\<0 & p(x_k|y_j)<q(x_k)\end{cases}</script><blockquote><p>事件Y中包含X的信息量</p></blockquote><h3 id="条件互信息"><a href="#条件互信息" class="headerlink" title="条件互信息"></a>条件互信息</h3><script type="math/tex; mode=display">I(x;y|z)=\log \frac{p(x|y,z)}{p(x|z)}</script><script type="math/tex; mode=display">=\log \frac{p(x,y|z)}{p(x|z)p(y|z)}</script><h3 id="联合互信息"><a href="#联合互信息" class="headerlink" title="联合互信息"></a>联合互信息</h3><script type="math/tex; mode=display">I(x;y,z)=\log\frac{p(x|y,z)}{p(x)}</script><h2 id="联合互信息动链式法则"><a href="#联合互信息动链式法则" class="headerlink" title="联合互信息动链式法则"></a>联合互信息动链式法则</h2><script type="math/tex; mode=display">I(x;y,z)=I(x;y)+I(x;z|y)</script><script type="math/tex; mode=display">=I(y;x)+I(z;x|y)</script><h1 id="变量的平均自信息——熵"><a href="#变量的平均自信息——熵" class="headerlink" title="变量的平均自信息——熵"></a>变量的平均自信息——熵</h1><script type="math/tex; mode=display">H(X)=E(I(X))=-\sum_{x\in\chi}q(x)\log_aq(x)</script><ul><li>熵是随机变量不确定性的度量</li><li>熵是随机变量每次观察结果平均对外界所提供的信息量</li><li>熵是为了确证随机变量的取值外界平均所需要的与之相<br>关的信息量</li></ul><h2 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h2><ul><li>以事件 Y=y 为条件的变量X的熵<script type="math/tex; mode=display">H(X|y)=\sum_{x\in X}p(x|y)I(X|y)=-\sum_{x\in X}p(x|y)\log p(x|y)</script></li><li>以变量 Y 为条件的变量X的熵<script type="math/tex; mode=display">H(X|Y)=\sum_{y\in Y}w(y)H(X|y)=-\sum_x\sum_yp(x,y)\log p(x|y)</script><blockquote><p>疑义度，在Y已知X剩余的不确定性</p></blockquote></li></ul><h2 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h2><script type="math/tex; mode=display">H(X,Y)=-\sum_x\sum_yp(x,y)\log p(x,y)</script><h3 id="联合链式法则"><a href="#联合链式法则" class="headerlink" title="联合链式法则"></a>联合链式法则</h3><script type="math/tex; mode=display">H(X,Y)=H(X)+H(X|Y)</script><script type="math/tex; mode=display">=H(Y)+H(Y|X)</script><h2 id="熵的性质"><a href="#熵的性质" class="headerlink" title="熵的性质"></a>熵的性质</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/06/9c110e8b8faa97da5ec3ae1db288c979.png" alt=""><br>(5)可加性</p><script type="math/tex; mode=display">H(X_2)|_{x_2\in\chi_2}=H(X_1)_{X_1\in\chi_1}+H(X_2|X_1)_{X_2\in\chi_2}^{X_1\in\chi_1}</script><blockquote><p>对于变量X可以进行多步观察，每一步都可以从上一步观察的结果中得到更为细致的结果</p></blockquote><p>(6)极值性</p><script type="math/tex; mode=display">H_k<\log K</script><blockquote><p>均匀分布时熵最大</p></blockquote><h3 id="凸性质"><a href="#凸性质" class="headerlink" title="凸性质"></a>凸性质</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/06/5bd1ddd18426df0225ffec178af929cc.png" alt=""></p><h1 id="平均互信息"><a href="#平均互信息" class="headerlink" title="平均互信息"></a>平均互信息</h1><script type="math/tex; mode=display">I(X;Y)=\sum_x\sum_yp(x,y)\log\frac{p(x|y)}{q(x)}</script><ul><li>非负性</li><li>对称性</li><li>互信息与熵的关联性<script type="math/tex; mode=display">I(X,Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)=H(X)+H(Y)-H(X,Y)</script><blockquote><p>求互信息常用上面的公式,相互独立的事件互信息为0</p></blockquote></li></ul><h2 id="条件互信息-1"><a href="#条件互信息-1" class="headerlink" title="条件互信息"></a>条件互信息</h2><script type="math/tex; mode=display">I(X;Y|Z)=\sum_x\sum_y\sum_zp(x,y,z)\log\frac{p(x|y,z)}{p(x)}</script><h2 id="联合互信息-1"><a href="#联合互信息-1" class="headerlink" title="联合互信息"></a>联合互信息</h2><script type="math/tex; mode=display">I(X;Y,Z)=\sum_x\sum_y\sum_zp(x,y,z)\log\frac{p(x|y,z}{p(x)}</script><script type="math/tex; mode=display">=I(X;Z)+I(X;Y|Z)</script><h1 id="相对熵"><a href="#相对熵" class="headerlink" title="相对熵"></a>相对熵</h1><p>一个变量的两种概率分布</p><script type="math/tex; mode=display">D(p//q)=\sum_xp(x)\log\frac{p(x)}{q(x)}</script><blockquote><p>表示实际分布p(x)和假定分布q(x)之间的平均差距，也称为鉴别熵</p></blockquote><h2 id="相对熵的性质"><a href="#相对熵的性质" class="headerlink" title="相对熵的性质"></a>相对熵的性质</h2><ul><li>非负<script type="math/tex; mode=display">D(p//q)>=0</script></li><li>非对称<script type="math/tex; mode=display">D(p//q)\not D(q//p)</script></li><li>与互信息关系<script type="math/tex; mode=display">I(X;Y)=D(p(xy//p(x)p(y))>=0</script></li></ul><h1 id="疑义度"><a href="#疑义度" class="headerlink" title="疑义度"></a>疑义度</h1><h2 id="错误概率"><a href="#错误概率" class="headerlink" title="错误概率"></a>错误概率</h2><script type="math/tex; mode=display">P_E=\sum_{k=0}^{K-1}\sum_{j\not k}Pr\{X=k,\hat{X}=j\}</script><h2 id="fano不等式"><a href="#fano不等式" class="headerlink" title="fano不等式"></a>fano不等式</h2><script type="math/tex; mode=display">H(X|\hat{X})<=H(P_E)+P_E\log(K-1)</script><h1 id="马尔可夫链"><a href="#马尔可夫链" class="headerlink" title="马尔可夫链"></a>马尔可夫链</h1><p>每个随机变量都是前一个随机变量一步处理的结果,任意一个节点已知，后面的变量和前面没得变量条件独立。</p><script type="math/tex; mode=display">p(xyz)=p(x)p(y|x)p(z|y)</script><script type="math/tex; mode=display">p(z|xy)=p(z|y)</script><script type="math/tex; mode=display">p(xz|y)=p(x|y)p(z|y)</script><script type="math/tex; mode=display">Z\to Y\to X</script><script type="math/tex; mode=display">I(X;Z|Y)=0</script><ul><li>马尔可夫链是可逆的</li></ul><h2 id="数据处理定理"><a href="#数据处理定理" class="headerlink" title="数据处理定理"></a>数据处理定理</h2><p>如果$X\to Y \to Z$</p><p>则$I(X;Y)&gt;=I(X;Z),I(X;Y)&gt;=I(X;Y|Z)$</p><h2 id="四变量马尔科夫链"><a href="#四变量马尔科夫链" class="headerlink" title="四变量马尔科夫链"></a>四变量马尔科夫链</h2><script type="math/tex; mode=display">graph LRU-->X</script><script type="math/tex; mode=display">X-->Y</script><script type="math/tex; mode=display">Y-->V</script><script type="math/tex; mode=display">I(X;Y)>=I(U;V)</script><h2 id="互信息的凸性"><a href="#互信息的凸性" class="headerlink" title="互信息的凸性"></a>互信息的凸性</h2><script type="math/tex; mode=display">I(X;Y)=I(\{q(x)\},\{p(y|x)\}</script><blockquote><p>互信息I(X;Y)是关于输入分布{q(x)}和转移概率矩阵{p(y|x)}的函数</p></blockquote><h1 id="连续随机变量的互信息"><a href="#连续随机变量的互信息" class="headerlink" title="连续随机变量的互信息"></a>连续随机变量的互信息</h1><script type="math/tex; mode=display">I(X;Y)=\int\int p_{XY}(x,y)\log \frac{p_{XY}(x,y)}{p_X(x)p_{Y}(y)}dxdy</script><script type="math/tex; mode=display">I(X;Y|Z)=\int\int\int p_{XYZ}(x,y,z)\log \frac{p(x,y|z)}{p(x|z)p(y|z)}dxdydz</script><script type="math/tex; mode=display">I(X;YZ)=\int\int\int p_{XYZ}(x,y,z)\log \frac{p(x,y,z)}{p(x)p(yz)}dxdydz</script><h2 id="基本性质"><a href="#基本性质" class="headerlink" title="基本性质"></a>基本性质</h2><script type="math/tex; mode=display">I(X;Y)>=0</script><script type="math/tex; mode=display">I(X;Y)=I(Y;X)</script><script type="math/tex; mode=display">I(X;YZ)=I(X;Y)+I(X;Z|Y)</script><h2 id="连续随机变量微分熵"><a href="#连续随机变量微分熵" class="headerlink" title="连续随机变量微分熵"></a>连续随机变量微分熵</h2><p>连续随机变量的熵无穷大，所以引入微分熵的概念来衡量连续变量的相对不确定性</p><script type="math/tex; mode=display">H_C(X)=h(x)=-\int p_X(x)\log p_X(x)dx</script><blockquote><p>HC (X )不具有线性变换不变性,可正、可负</p></blockquote><h2 id="条件微分熵"><a href="#条件微分熵" class="headerlink" title="条件微分熵"></a>条件微分熵</h2><script type="math/tex; mode=display">H_C(X|Y)=-\int\int p_{XY}(x,y)\log p(x|y)dxdy</script><h2 id="联合微分熵"><a href="#联合微分熵" class="headerlink" title="联合微分熵"></a>联合微分熵</h2><script type="math/tex; mode=display">H_C(X,Y)=-\int\int p_{XY}(x,y)\log p(x,y)dxdy</script><script type="math/tex; mode=display">=H_C(X)+H_C(Y|X)</script><script type="math/tex; mode=display">=H_C(Y)+H_C(X|Y)</script><h2 id="微分熵极大化"><a href="#微分熵极大化" class="headerlink" title="微分熵极大化"></a>微分熵极大化</h2><h3 id="峰值受限"><a href="#峰值受限" class="headerlink" title="峰值受限"></a>峰值受限</h3><p>若 峰值受限于[-M,+M] 即 则 X为均匀分布微分熵最大</p><script type="math/tex; mode=display">H_C(X)<=In 2M</script><h3 id="功率受限"><a href="#功率受限" class="headerlink" title="功率受限"></a>功率受限</h3><p>若X的方差不大于$\sigma^2$,则X为高斯分布时微分熵最大</p><script type="math/tex; mode=display">H_C(X)<=1/2In 2\pi e\sigma^2</script><h3 id="熵功率"><a href="#熵功率" class="headerlink" title="熵功率"></a>熵功率</h3><ul><li>定义连续随机变量 的熵功率为<script type="math/tex; mode=display">\hat{\sigma_X^2}=\frac{1}{2\pi e}e^{2H_C(X)}</script></li><li>高斯随机变量的微分熵<script type="math/tex; mode=display">H_C(X)=\frac{1}{2}In(2\pi e\sigma^2)</script></li><li>高斯变量熵功率<script type="math/tex; mode=display">\hat{\sigma_X^2}=\frac{1}{2\pi e}e^{2H_C(X)}=\sigma^2</script></li></ul><h3 id="熵功率不等式"><a href="#熵功率不等式" class="headerlink" title="熵功率不等式"></a>熵功率不等式</h3><script type="math/tex; mode=display">H_C(X)<=In(2\pi e\sigma^2)</script><script type="math/tex; mode=display">\hat{\sigma_X^2}=\frac{1}{2\pi e}e^{2H_C(X)}<=\sigma^2</script><blockquote><p>功率一定时，高斯变量的熵功率最大，与功率相等</p></blockquote><h1 id="平稳源"><a href="#平稳源" class="headerlink" title="平稳源"></a>平稳源</h1><p>平稳源：任意长度的片段的联合概率分布与时间起点无关</p><script type="math/tex; mode=display">Pr(X_1X_2...X_N=x_1x_2...x_N)=Pr(X_{L+1}X_{L+2}...X_{L+N}=x_1x_2...x_N)</script><h2 id="简单无记忆源"><a href="#简单无记忆源" class="headerlink" title="简单无记忆源"></a>简单无记忆源</h2><script type="math/tex; mode=display">Pr(X_1X_2...X_N=x_1x_2...x_N)=\prod_{n=1}^N Pr(X_n=x_n)</script><h2 id="平稳源的熵"><a href="#平稳源的熵" class="headerlink" title="平稳源的熵"></a>平稳源的熵</h2><h3 id="平均每符号熵"><a href="#平均每符号熵" class="headerlink" title="平均每符号熵"></a>平均每符号熵</h3><script type="math/tex; mode=display">H_N(X)=\frac{1}{N}H(X_1X_2...X_N)</script><h3 id="熵速率"><a href="#熵速率" class="headerlink" title="熵速率"></a>熵速率</h3><script type="math/tex; mode=display">H_{\infty}(X)=\lim_{N\to \infty}H_N(X)</script><h4 id="熵相对率"><a href="#熵相对率" class="headerlink" title="熵相对率"></a>熵相对率</h4><script type="math/tex; mode=display">\eta=\frac{H_{\infty}(X)}{\log K}</script><h4 id="信源冗余度"><a href="#信源冗余度" class="headerlink" title="信源冗余度"></a>信源冗余度</h4><script type="math/tex; mode=display">R=1-\eta</script><h3 id="平均条件熵"><a href="#平均条件熵" class="headerlink" title="平均条件熵"></a>平均条件熵</h3><script type="math/tex; mode=display">H(X_N|X_{N-1}X_{N-2}...X_1)</script><h3 id="平稳源熵的性质"><a href="#平稳源熵的性质" class="headerlink" title="平稳源熵的性质"></a>平稳源熵的性质</h3><ul><li>单调增</li><li>$H<em>N(X)&gt;=H(X_N|X</em>{N-1}…X_1)$</li><li>$H<em>{\infty}(X)=\lim</em>{N\to\infty}H<em>N(X)=\lim</em>{N\to\infty}H(X<em>N|X</em>{N-1}…X_1)$</li></ul><h2 id="马尔科夫源"><a href="#马尔科夫源" class="headerlink" title="马尔科夫源"></a>马尔科夫源</h2><script type="math/tex; mode=display">Pr(X_l|X_{l-1}X_{l-2}...X_1X_0)=Pr(X_{l}|X_{l-1}...X_{l-m})</script><h3 id="马尔科夫源的状态图"><a href="#马尔科夫源的状态图" class="headerlink" title="马尔科夫源的状态图"></a>马尔科夫源的状态图</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/10/09/b85e6cc8026efebc73694c8427b08863.png" alt=""></p><ul><li>时齐（时不变）马尔科夫源：状态转移概率pij(n)与时间n无关。<br>到达任一其它状态。</li><li>既约（不可约）马尔科夫源：<br>从任一状态出发，经有限步总可以</li><li>状态转移概率矩阵：<script type="math/tex; mode=display">P=[p_{ij}]_{K^m\times K^m}</script></li><li>n时刻的状态概率分布：<script type="math/tex; mode=display">Q(n)=(q_1(n),q_2(n)...q_{K^m}(n))</script><script type="math/tex; mode=display">Q(n+1)=Q(n)P</script></li><li>对时齐既约马尔科夫源，状态的稳态分布存在<script type="math/tex; mode=display">\hat{Q}=\lim_{n\to \infty}Q(N+1)=\lim_{n\to \infty}Q(n)P=\hat{Q}P</script><h3 id="马尔科夫的熵率"><a href="#马尔科夫的熵率" class="headerlink" title="马尔科夫的熵率"></a>马尔科夫的熵率</h3><script type="math/tex; mode=display">H_{\infty}=H(X|S)</script></li></ul>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 香农yyds </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数字电路笔记</title>
      <link href="2020/08/30/digital-circle%E2%80%9C/"/>
      <url>2020/08/30/digital-circle%E2%80%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="逻辑运算"><a href="#逻辑运算" class="headerlink" title="逻辑运算"></a>逻辑运算</h1><h2 id="基本公式"><a href="#基本公式" class="headerlink" title="基本公式"></a>基本公式</h2><span id="more"></span><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/8e92915a66d8d4f231c227da492da482.png" alt=""></p><h2 id=""><a href="#" class="headerlink" title=""></a><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/29b4891c74db0d44aeb0aa57f0b3b7a5.png" alt=""></h2><h2 id="基本定理"><a href="#基本定理" class="headerlink" title="基本定理"></a>基本定理</h2><h3 id="代入定理"><a href="#代入定理" class="headerlink" title="代入定理"></a>代入定理</h3><h3 id="反演定理"><a href="#反演定理" class="headerlink" title="反演定理"></a>反演定理</h3><hr><h2 id="逻辑函数表示方法"><a href="#逻辑函数表示方法" class="headerlink" title="逻辑函数表示方法"></a>逻辑函数表示方法</h2><ul><li>真值表</li><li>逻辑式<ul><li>将输入/输出之间的逻辑关系用与/或/非的运算式表示 就得到逻辑式</li></ul></li><li>逻辑图</li><li>波形图</li><li>卡诺图</li></ul><h2 id="标准形式"><a href="#标准形式" class="headerlink" title="标准形式"></a>标准形式</h2><ul><li>最小项之和<ul><li>利用公式:A=(B’+B)A </li><li>A’B’C’=000=m0</li></ul></li><li>最大项之积<ul><li>A+B+C=000=M0</li><li>A’+B’+C’=111=M7<h3 id="转化方式"><a href="#转化方式" class="headerlink" title="转化方式"></a>转化方式</h3><script type="math/tex; mode=display">Y=\sum m_i</script><script type="math/tex; mode=display">\to Y'=\sum_{k=\not i} m_k</script><script type="math/tex; mode=display">\to Y=(\sum_{k=\not i} m_k)'</script><script type="math/tex; mode=display">\to Y=\prod_{i=\not k}m_k'=\prod_{i=\not k}M_k'</script></li></ul></li></ul><h2 id="逻辑函数化简"><a href="#逻辑函数化简" class="headerlink" title="逻辑函数化简"></a>逻辑函数化简</h2><h3 id="公式法"><a href="#公式法" class="headerlink" title="公式法"></a>公式法</h3><ul><li>AB+A’C+BC=AB+A’C</li><li>A+AB=A<h3 id="卡诺图"><a href="#卡诺图" class="headerlink" title="卡诺图"></a>卡诺图</h3></li></ul><ul><li>用卡诺图表示逻辑函数</li><li>找出可合并的最小项</li><li>化简后的乘积项相加</li></ul><h4 id="约束项"><a href="#约束项" class="headerlink" title="约束项"></a>约束项</h4><p>在逻辑函数中，对输入变量取值的 限制，在这些取值下为1的最小项称 为约束项</p><h4 id="任意项"><a href="#任意项" class="headerlink" title="任意项"></a>任意项</h4><p>在输入变量某些取值下，函数值为1或 为0不影响逻辑电路的功能，在这些取 值下为1的最小项称为任意项</p><h3 id="常用编码"><a href="#常用编码" class="headerlink" title="常用编码"></a>常用编码</h3><h4 id="补码反码"><a href="#补码反码" class="headerlink" title="补码反码"></a>补码反码</h4><p><strong>反码</strong></p><script type="math/tex; mode=display">(N)_{1NV}=\begin{cases}N&N>0\\2^n-1-N&N<0\end{cases}</script><p>负数补码为反码+1</p><h4 id="格雷码"><a href="#格雷码" class="headerlink" title="格雷码"></a>格雷码</h4><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/4b23dcbbf4bbab30acb637c5144ca451.png" alt=""></p><h1 id="组合逻辑"><a href="#组合逻辑" class="headerlink" title="组合逻辑"></a>组合逻辑</h1><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul><li>任意时刻输出仅取决于输入，没有反馈</li><li>不含存储单元</li></ul><h2 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h2><h3 id="普通编码器"><a href="#普通编码器" class="headerlink" title="普通编码器"></a>普通编码器</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/3dafecfcfdb8748e3b78aefe984b2e56.png" alt=""></p><h3 id="优先编码器"><a href="#优先编码器" class="headerlink" title="优先编码器"></a>优先编码器</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/3dafecfcfdb8748e3b78aefe984b2e56.png" alt=""></p><h3 id="两片编码器合并"><a href="#两片编码器合并" class="headerlink" title="两片编码器合并"></a>两片编码器合并</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/47318ffa87981c9121cb533abd61cec9.png" alt=""></p><h2 id="译码器"><a href="#译码器" class="headerlink" title="译码器"></a>译码器</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/6407612531b958e19cb0b0c1d2a69d37.png" alt=""></p><h2 id="数据选择器"><a href="#数据选择器" class="headerlink" title="数据选择器"></a>数据选择器</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/3d478372091bf08492331f428a6cad30.png" alt=""></p><h2 id="半加器"><a href="#半加器" class="headerlink" title="半加器"></a>半加器</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/9e65b07450d9abe5d27e2c5e0a2388a0.png" alt=""></p><h2 id="全加器"><a href="#全加器" class="headerlink" title="全加器"></a>全加器</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/ef91ccff1032bc06f75fcee2dae18e95.png" alt=""></p><h3 id="用半加器实现全加器"><a href="#用半加器实现全加器" class="headerlink" title="用半加器实现全加器"></a>用半加器实现全加器</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/44f8cad67b8c43ce2173ec6d607e35d9.png" alt=""></p><h2 id="数值比较器"><a href="#数值比较器" class="headerlink" title="数值比较器"></a>数值比较器</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/8dd053a1a2eebc458891eb36e144a730.png" alt=""></p><h3 id="多位数值比较器"><a href="#多位数值比较器" class="headerlink" title="多位数值比较器"></a>多位数值比较器</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/7b7a86568eebce30b90f40ded69963e7.png" alt=""></p><h3 id="组合电路的竞争和冒险"><a href="#组合电路的竞争和冒险" class="headerlink" title="组合电路的竞争和冒险"></a>组合电路的竞争和冒险</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/db54f98d06b427de82aeee77fb6804c4.png" alt=""><br>增加冗余项可以消除改现象</p><hr><h1 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h1><p><strong>如何存储数据？</strong></p><h2 id="SR锁存器"><a href="#SR锁存器" class="headerlink" title="SR锁存器"></a>SR锁存器</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/630ef5c5f7c05305a877cfc57244a195.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/a65bb1bd7eb84e3a2bd20298f9efd50b.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/c5ca359b23826a6067feb32316409faf.png" alt=""></p><h2 id="电平触发器"><a href="#电平触发器" class="headerlink" title="电平触发器"></a>电平触发器</h2><ul><li>只有在clk信号为高电平时才会改变输出</li><li>缺陷：抗干扰能力差</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/e6452f3b86dbd3673f433dedb121c2cd.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/d7bcf2a902a5de57da284a9659bf3065.png" alt=""></p><h2 id="边沿触发器"><a href="#边沿触发器" class="headerlink" title="边沿触发器"></a>边沿触发器</h2><ul><li>仅在电平上升下降沿发生变化<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/5cef502bbb8a6f5462c3872ac4273d01.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/c47c4c90ad167daf01aacd2801e9f889.png" alt=""><h2 id="脉冲触发器"><a href="#脉冲触发器" class="headerlink" title="脉冲触发器"></a>脉冲触发器</h2></li><li>把边沿触发器的D触发器变为SR触发器</li><li>在clk高电平期间所有输入决定下降沿时的输出变化，因为SR都变为0时第一个触发器输出不变<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/b68a8737431ccac41aaa799f2eb60d97.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/33ced500518724939bd310f16b69d7e7.png" alt=""><h2 id="锁存器分类"><a href="#锁存器分类" class="headerlink" title="锁存器分类"></a>锁存器分类</h2><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/d7461cd449d4a8f8eeec95571e8d3ff8.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/e8a7d67812a0223a51e2be60e9994da4.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/bc22041af8755f0c05a063ee16e55152.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/24e4f04968e85fa665547aa6d3416e45.png" alt=""></li></ul><h3 id="D触发器实现JK触发器"><a href="#D触发器实现JK触发器" class="headerlink" title="D触发器实现JK触发器"></a>D触发器实现JK触发器</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/19/6b9b912c12513e9003768e5b131de9b5.png" alt=""></p><blockquote><p>RS，T触发器都可以用JK触发器转化</p></blockquote><h2 id="触发器动态特性"><a href="#触发器动态特性" class="headerlink" title="触发器动态特性"></a>触发器动态特性</h2><ul><li>建立时间</li><li>保持时间</li><li>传输延迟时间</li><li>最高时钟频率</li></ul><hr><h1 id="时序电路"><a href="#时序电路" class="headerlink" title="时序电路"></a>时序电路</h1><h2 id="时序逻辑电路类型"><a href="#时序逻辑电路类型" class="headerlink" title="时序逻辑电路类型"></a>时序逻辑电路类型</h2><ul><li>Mealy型<ul><li>输出信号取决于存储电路状态和输入变量</li></ul></li><li>Moore型<ul><li>输出只是存储电路现态的函数</li><li>输出与时钟同步<h3 id="三个方程"><a href="#三个方程" class="headerlink" title="三个方程"></a>三个方程</h3></li></ul></li><li>驱动方程</li><li>状态方程</li><li>输出方程</li></ul><h3 id="移位寄存器"><a href="#移位寄存器" class="headerlink" title="移位寄存器"></a>移位寄存器</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/bf1491725d774e6f6a34c59b1c25f102.png" alt=""></p><h3 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/338638bd127f0db616eb85bb84bacf50.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/50be44c389dee6a5afd71faf0ae7c04c.png" alt=""></p><h4 id="计数器设计"><a href="#计数器设计" class="headerlink" title="计数器设计"></a>计数器设计</h4><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/6016833e368c940d14e84ac4bbbd568d.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/3e922bc992d2b179a0e86a1b3deea238.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/5c53976db301e995f1a45e296b027df8.png" alt=""></p><h1 id="存储器与可编程逻辑阵列"><a href="#存储器与可编程逻辑阵列" class="headerlink" title="存储器与可编程逻辑阵列"></a>存储器与可编程逻辑阵列</h1><h2 id="只读存储器ROM"><a href="#只读存储器ROM" class="headerlink" title="只读存储器ROM"></a>只读存储器ROM</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/82e6321b289919805a28fa3486224e58.png" alt=""></p><hr><h1 id="控制器"><a href="#控制器" class="headerlink" title="控制器"></a>控制器</h1><h2 id="控制器设计基本方法"><a href="#控制器设计基本方法" class="headerlink" title="控制器设计基本方法"></a>控制器设计基本方法</h2><ul><li>每个状态一个触发器（one-hot，热位）</li><li>序列寄存器―译码器法</li><li>可编程逻辑阵列PLA控制法（选择器法）</li><li>微程序控制法<h2 id="算法流程图"><a href="#算法流程图" class="headerlink" title="算法流程图"></a>算法流程图</h2></li><li><strong>状态框</strong><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/eee27af82e50fb299080b111f89d4d7b.png" alt=""></li><li><strong>判断框</strong><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/c7a6bb0427add204d4253c4b9a2c1f58.png" alt=""></li><li><strong>条件框</strong><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/08bc38c8212f168a1266030338b81dcd.png" alt=""><h2 id="控制器类型"><a href="#控制器类型" class="headerlink" title="控制器类型"></a>控制器类型</h2></li><li>计数器型</li><li>数据选择器型</li></ul><h2 id="微程序控制器"><a href="#微程序控制器" class="headerlink" title="微程序控制器"></a>微程序控制器</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/daec2de369c6936908f33be54d12d24b.png" alt=""></p><h2 id="微命令"><a href="#微命令" class="headerlink" title="微命令"></a>微命令</h2><ul><li>微指令除给出微命令信息外，还给出测试判别信息</li><li>微指令中还包含一个下址字段，该字段将指明存储器中下一条<br>微指令的地址</li><li>微程序是由若干条微码指令组成的序列<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/30/6cfc36e268cdcbbf284885f237e7c1d7.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/30/8c8bb5ea2d07bfc7bddae58a31ded079.png" alt=""></li></ul><h1 id="TTL电路"><a href="#TTL电路" class="headerlink" title="TTL电路"></a>TTL电路</h1><h3 id="三极管非门"><a href="#三极管非门" class="headerlink" title="三极管非门"></a>三极管非门</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/3c8b93eb14b3ab1a8a13403474e3c5a0.png" alt=""></p><h3 id="TTL反相器"><a href="#TTL反相器" class="headerlink" title="TTL反相器"></a>TTL反相器</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/67c76ff25dcdfbaea40046d47385b762.png" alt=""></p><h1 id="CMOS"><a href="#CMOS" class="headerlink" title="CMOS"></a>CMOS</h1><h3 id="传输门"><a href="#传输门" class="headerlink" title="传输门"></a>传输门</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/cfc93ba8869ea0cb0ffe74cfb041d9cb.png" alt=""></p><h3 id="三态门"><a href="#三态门" class="headerlink" title="三态门"></a>三态门</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/dfd338364f5507d127901f4c98c6e7f7.png" alt=""></p><blockquote><p>CMOS1和TTL电路最大区别，TTL电路输入端接大电阻变为高电平输入</p></blockquote><h1 id="脉冲电路"><a href="#脉冲电路" class="headerlink" title="脉冲电路"></a>脉冲电路</h1><h2 id="施密特电路"><a href="#施密特电路" class="headerlink" title="施密特电路"></a>施密特电路</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/b9eceacb663c025f80018e935ce1f091.png" alt=""></p><ul><li>正向阈值电压<script type="math/tex; mode=display">v_i=(1+\frac{R_1}{R_2})v_{TH}</script></li><li>负向阈值电压<script type="math/tex; mode=display">v_i=(1-\frac{R_1}{R_2})v_{TH}</script></li><li>特点：滞后特性</li></ul><h3 id="作用："><a href="#作用：" class="headerlink" title="作用："></a>作用：</h3><ul><li>波形变换（整形），抗干扰能力强</li><li>用于脉冲鉴幅</li></ul><h2 id="单稳态电路"><a href="#单稳态电路" class="headerlink" title="单稳态电路"></a>单稳态电路</h2><h3 id="微分电路"><a href="#微分电路" class="headerlink" title="微分电路"></a>微分电路</h3><script type="math/tex; mode=display">t_w=In2RC=0.69RC</script><h3 id="积分电路"><a href="#积分电路" class="headerlink" title="积分电路"></a>积分电路</h3><script type="math/tex; mode=display">t_w=In2(R+R_o)C</script><h2 id="多谐振荡器"><a href="#多谐振荡器" class="headerlink" title="多谐振荡器"></a>多谐振荡器</h2><h3 id="对称式"><a href="#对称式" class="headerlink" title="对称式"></a>对称式</h3><script type="math/tex; mode=display">T=2R_ECIn\frac{V_E-V_{IK}}{V_E-V_{TH}}</script><h3 id="非对称"><a href="#非对称" class="headerlink" title="非对称"></a>非对称</h3><script type="math/tex; mode=display">T=2R_FCIn3</script><h3 id="环形震荡器"><a href="#环形震荡器" class="headerlink" title="环形震荡器"></a>环形震荡器</h3><script type="math/tex; mode=display">T=2nt_{pd}</script><h2 id="集电极开路门电路（OC门）"><a href="#集电极开路门电路（OC门）" class="headerlink" title="集电极开路门电路（OC门）"></a>集电极开路门电路（OC门）</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/0e1513e0c121639db13e6c9f646d925d.png" alt=""></p><h2 id="555定时器"><a href="#555定时器" class="headerlink" title="555定时器"></a>555定时器</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/4e8155002d47fcee20b42b092cb84baf.png" alt=""></p><h3 id="555做施密特触发器"><a href="#555做施密特触发器" class="headerlink" title="555做施密特触发器"></a>555做施密特触发器</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/7c554d55db34728ef6927b95ee8b7261.png" alt=""></p><h3 id="555做单稳态"><a href="#555做单稳态" class="headerlink" title="555做单稳态"></a>555做单稳态</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/88368cda5707cf762cf0ffc5c255162b.png" alt=""></p><script type="math/tex; mode=display">T=1.1RC</script><h3 id="555做多谐振荡"><a href="#555做多谐振荡" class="headerlink" title="555做多谐振荡"></a>555做多谐振荡</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/29/dcd9a43ac32ea76bda7fd73474811fb9.png" alt=""></p><script type="math/tex; mode=display">T=(R_1+2R_2)CIn2</script><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><h2 id="SA1-amp-SA0"><a href="#SA1-amp-SA0" class="headerlink" title="SA1&amp;SA0"></a>SA1&amp;SA0</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/30/9c23bda387263fbcd582641ee9b82d49.png" alt=""></p><h2 id="路径敏化"><a href="#路径敏化" class="headerlink" title="路径敏化"></a>路径敏化</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/30/7eb382150ceac2caaea24b6392ace302.png" alt=""></p><p>只考虑单个输入变化，其他输入固定，让门退化，与门输入1，或门输入0</p><script type="math/tex; mode=display">w_2=1</script><script type="math/tex; mode=display">w_3=0</script><script type="math/tex; mode=display">w_4=1</script><p>可以测试</p><script type="math/tex; mode=display">w_1=1</script><script type="math/tex; mode=display">a/0</script><script type="math/tex; mode=display">b/0</script><script type="math/tex; mode=display">c/1</script><hr><script type="math/tex; mode=display">w_1=0</script><script type="math/tex; mode=display">a/1</script><script type="math/tex; mode=display">b/1</script><script type="math/tex; mode=display">c/0</script><blockquote><p>路径敏化可以一组输入测试多个故障</p></blockquote><h2 id="树型结构"><a href="#树型结构" class="headerlink" title="树型结构"></a>树型结构</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/30/680ed85a1bc29dd541011938feb141f6.png" alt=""></p><h2 id="时序电路测试"><a href="#时序电路测试" class="headerlink" title="时序电路测试"></a>时序电路测试</h2><h3 id="扫描测试"><a href="#扫描测试" class="headerlink" title="扫描测试"></a>扫描测试</h3><p>加入多路选择器把内部状态逐级传递出来</p><p>在测试时钟的控制下通过扫描输入（ScanIn）引线输入逻辑模块 A（和/或 B）的<br>激励向量并将其移入寄存器。<br>激励被加到逻辑模块的输入并传播到它的输出。通过产生一个系统时钟事件把它<br>的结果锁存到寄存器中。<br>寄存器中的结果通过扫描输出（ScanOut）引线送出电路并与期望的数据进行比较。<br>此时一个新的激励向量可以被同时输入。</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/30/9992c841a2c31e9a0d895c124c6a6867.png" alt=""></p><h3 id="内建自测试"><a href="#内建自测试" class="headerlink" title="内建自测试"></a>内建自测试</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/30/571abc9b7f9a7324d0486c51796b5646.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/30/6af28ea46e9b62eeea5c9d84a18a12c4.png" alt=""></p><h3 id="专门测试"><a href="#专门测试" class="headerlink" title="专门测试"></a>专门测试</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/30/834946b7da49c32910baf7db6d41afab.png" alt=""></p><h1 id="微处理器"><a href="#微处理器" class="headerlink" title="微处理器"></a>微处理器</h1><h2 id="什么是微处理器？"><a href="#什么是微处理器？" class="headerlink" title="什么是微处理器？"></a>什么是微处理器？</h2><p>• 微处理器由一片或少数几片大规模集成电路组成的中央处理器。</p><p>• 这些电路执行控制部件和算术逻辑部件的功能。</p><p>• 微处理器能完成取指令、执行指令，以及与外界存储器和逻辑部件交换信息等操作，<br>是微型计算机的运算控制部分。</p><h2 id="微处理器的主要结构"><a href="#微处理器的主要结构" class="headerlink" title="微处理器的主要结构"></a>微处理器的主要结构</h2><h3 id="冯诺依曼结构"><a href="#冯诺依曼结构" class="headerlink" title="冯诺依曼结构"></a>冯诺依曼结构</h3><p>• 将程序存储和数据存储放在同一物理存储空间</p><p>• 相同的总线</p><p>• 硬件简单</p><p><strong>组成</strong></p><ul><li>输入 INPUT</li><li>输出 OUTPUT</li><li>存储器 MEMORY</li><li>微处理器 CPU</li></ul><h3 id="哈佛结构"><a href="#哈佛结构" class="headerlink" title="哈佛结构"></a>哈佛结构</h3><p>• 将程序存储和数据存储分别放在不同的物理存储空间</p><p>• 不同的总线</p><p>• 灵活、速度快</p><h2 id="微处理器内部结构"><a href="#微处理器内部结构" class="headerlink" title="微处理器内部结构"></a>微处理器内部结构</h2><h3 id="控制单元"><a href="#控制单元" class="headerlink" title="控制单元"></a>控制单元</h3><ul><li>用于控制数据通路的所有操作，实现微处理器运算的正确性<h3 id="数据通路"><a href="#数据通路" class="headerlink" title="数据通路"></a>数据通路</h3></li><li>主要包括运算单元ALU、存储单元（寄存器）及其相互连接</li></ul><h2 id="指令集"><a href="#指令集" class="headerlink" title="指令集"></a>指令集</h2><p>专门执行一些指令集的微处理器<br>• 利用指令集编写不同的程序完成不同的处理任务</p><h2 id="微处理器三个执行步骤"><a href="#微处理器三个执行步骤" class="headerlink" title="微处理器三个执行步骤"></a>微处理器三个执行步骤</h2><ul><li>取指</li><li>译指</li><li>执行指令</li></ul><h2 id="微处理器指令集-ISA"><a href="#微处理器指令集-ISA" class="headerlink" title="微处理器指令集 ISA"></a>微处理器指令集 ISA</h2><ul><li>指令（Instruction） <ul><li>计算机语言里的单词</li></ul></li><li>指令集（Instruction Set Architecture） <ul><li>计算机的词汇</li></ul></li><li>指令需指明要执行的操作和要使用的操作数</li><li>机器语言（Machine Language）<ul><li>指令编码为二进制数格式</li></ul></li><li>汇编语言（Assembly Language）<ul><li>符号格式表示各种指令</li></ul></li><li>基本指令<ul><li>加、减和跳转指令</li></ul></li></ul><h2 id="操作数"><a href="#操作数" class="headerlink" title="操作数"></a>操作数</h2><ul><li>常数（Constants）和变量</li><li>寄存器（register）<ul><li>寄存器组（register set） + 寄存器文件（register file） </li></ul></li><li>寄存器操作<br>$$<br>$ S_0=a,$ S_1=b,$ S_2=c</li></ul><p>add $S_0.$S_1.$S_2</p><script type="math/tex; mode=display">+ MIPS寄存器名称由$符号开头+ 变量a、b和c放置在$S0、$S1和$S2+ 该指令将存在$S1(b)和$S2(C)的32位值相加，32位结果写入$S0(a)## 立即数+ 立即数操作</script><p>addi<br>$S_0.$S_0.4</p><script type="math/tex; mode=display"></script><p># a=a+4</p><p>$$</p><ul><li>立即数是一个16位二进制补码数据</li><li>范围是[-32768，32767]</li></ul><h2 id="MIPS指令格式"><a href="#MIPS指令格式" class="headerlink" title="MIPS指令格式"></a>MIPS指令格式</h2><h3 id="R型———三个寄存器操作数格式"><a href="#R型———三个寄存器操作数格式" class="headerlink" title="R型———三个寄存器操作数格式"></a>R型———三个寄存器操作数格式</h3><p>• 用于如add和sub指令，有三个寄存器操作数</p><ul><li>R-type Instructions<ul><li>R型是寄存器类型（register-type）的缩写</li><li>32位指令</li></ul></li></ul><div class="table-container"><table><thead><tr><th>op</th><th>rs</th><th>rt</th><th>rd</th><th>shamt</th><th>funct</th></tr></thead><tbody><tr><td>6bits</td><td>5bits</td><td>5bits</td><td>5bits</td><td>5bits</td><td>6bits</td></tr></tbody></table></div><ul><li>六个数字域<ul><li>op，（也称为opcode或操作码），R型指令的opcode为6‘b000000</li><li>rs，rt，源寄存器</li><li>rd，目标寄存器</li><li>shamt，只用于移位操作，其它R型指令，shamt为5’b00000</li><li>funct，（也称为功能码），确定特定的R型操作</li></ul></li><li>每个字段为5比特或6比特</li></ul><h3 id="I型————两个寄存器操作数格式"><a href="#I型————两个寄存器操作数格式" class="headerlink" title="I型————两个寄存器操作数格式"></a>I型————两个寄存器操作数格式</h3><p>• 用于如lw和sw指令，具有两个寄存器操作数和一个16位立即数</p><ul><li>I型是立即数类型（immediate-type）的缩写</li></ul><div class="table-container"><table><thead><tr><th>op</th><th>rs</th><th>rt</th><th>imm</th></tr></thead><tbody><tr><td>6bits</td><td>5bits</td><td>5bits</td><td>16bits</td></tr></tbody></table></div><ul><li>四个字段<ul><li>op，操作码</li><li>rs，源操作数</li><li>rt，如addi和lw用作目标操作数，sw作为另一种源操作数</li><li>imm，源操作立即数 </li><li>rs和imm始终用作源操作数</li></ul></li></ul><h3 id="J型———-无寄存器格式"><a href="#J型———-无寄存器格式" class="headerlink" title="J型———-无寄存器格式"></a>J型———-无寄存器格式</h3><p>• 有一个26位的立即数，无寄存器</p><p>J型是跳转型（jump-type）的缩写</p><p>• 此格式只用跳转指令使用。</p><p>• 用26位的地址操作数，addr</p><p>• 用于指定一个地址</p><div class="table-container"><table><thead><tr><th>op</th><th>addr</th></tr></thead><tbody><tr><td>6bits</td><td>26bits</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数字电路 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>z变换</title>
      <link href="2020/08/21/z-transformation/"/>
      <url>2020/08/21/z-transformation/</url>
      
        <content type="html"><![CDATA[<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p><strong>傅里叶变换的推广</strong></p><p><strong>针对离散信号</strong><br><span id="more"></span></p><h2 id="双边z变换"><a href="#双边z变换" class="headerlink" title="双边z变换"></a>双边z变换</h2><p>选择合适的指数信号$r^{-n}$,使得，可以进行傅里叶变换</p><p>假设r使X(z)收敛</p><script type="math/tex; mode=display">x[n]r^{-n}=F^{-1}\{X(re^{iw})\}</script><script type="math/tex; mode=display">x[n]=r^{n}F^{-1}\{X(re^{iw})\}</script><script type="math/tex; mode=display">x[n]=\frac{1}{2\pi}\int_{2\pi} X(re^{iw})(re^{jw})^ndw</script><script type="math/tex; mode=display">令z=re^{jw}</script><script type="math/tex; mode=display">\tox[n]=\frac{1}{2\pi j}\oint_{2\pi} X(z)z^{n-1}dz</script><script type="math/tex; mode=display">X(z)=\sum_{n=-\infty}^{\infty}x[n]z^{-n}</script><blockquote><p>$\oint$围线积分，以原点为中心，逆时针的线积分</p></blockquote><p>==z变换需要明确收敛域==</p><h3 id="常见离散信号z变换"><a href="#常见离散信号z变换" class="headerlink" title="常见离散信号z变换"></a>常见离散信号z变换</h3><ul><li>单边指数信号<script type="math/tex; mode=display">x[n]=u[n]a^n</script><script type="math/tex; mode=display">X(z)=\frac{z}{z-a} (|z|>|a|)</script><script type="math/tex; mode=display">x[n]=-u[-n-1]a^n</script><script type="math/tex; mode=display">X(z)=\frac{z}{z-a} (|z|<|a|)</script></li><li>双边信号<script type="math/tex; mode=display">x[n]=b^{|n|}</script><script type="math/tex; mode=display">X(z)=\frac{1}{1-bz^{-1}}-\frac{1}{1-b^{-1}z^{-1}} (|b|<|z|<|1/b|)</script><blockquote><p>只有b&lt;1时收敛</p></blockquote></li></ul><blockquote><p>当左边信号线性叠加右边信号，就会形成圆环，总共有三种情况</p><ul><li>幂指数信号<script type="math/tex; mode=display">x[n]=nu[n]a^n</script><script type="math/tex; mode=display">X(z)=\frac{az}{(z-a)^2} (|z|>|a|)</script></li><li>三角信号<script type="math/tex; mode=display">x[n]=cos[w_0n]u[n]</script><script type="math/tex; mode=display">X(z)=\frac{1-z^{-1}[cosw_0]}{1-2z^{-1}[cosw_0]+z^{-2}}(|z|>1)</script><script type="math/tex; mode=display">x[n]=sin[w_0n]u[n]</script><script type="math/tex; mode=display">X(z)=\frac{1-z^{-1}[sinw_0]}{1-2z^{-1}[cosw_0]+z^{-2}}(|z|>1)</script><script type="math/tex; mode=display">x[n]=r^ncos[w_0n]u[n]</script><script type="math/tex; mode=display">X(z)=\frac{1-z^{-1}[rcosw_0]}{1-2z^{-1}[2rcosw_0]+r^2z^{-2}}(|z|>1)</script><script type="math/tex; mode=display">x[n]=rsin[w_0n]u[n]</script><script type="math/tex; mode=display">X(z)=\frac{1-z^{-1}[rsinw_0]}{1-2z^{-1}[2rcosw_0]+r^2z^{-2}}(|z|>1)</script></li></ul></blockquote><ul><li>单位冲击<script type="math/tex; mode=display">x[n]=\delta[n]</script><script type="math/tex; mode=display">X(z)=1(0=<|z|<\infty)</script></li><li>单位阶跃<script type="math/tex; mode=display">x[n]=u[n]</script><script type="math/tex; mode=display">X(z)=\frac{z}{z-1}(|z|>1)</script><script type="math/tex; mode=display">x[n]=-u[-n-1]</script><script type="math/tex; mode=display">X(z)=\frac{z}{z-1}(|z|<1)</script><h2 id="z变换的收敛域"><a href="#z变换的收敛域" class="headerlink" title="z变换的收敛域"></a>z变换的收敛域</h2>信号绝对可和<script type="math/tex; mode=display">\sum_{n=-\infty}^{+\infty}|x[n]|r^{-n}<\infty</script></li></ul><blockquote><p> z变换的收敛域都是以原点为圆心的环状</p></blockquote><h3 id="有限长序列"><a href="#有限长序列" class="headerlink" title="有限长序列"></a>有限长序列</h3><script type="math/tex; mode=display">\sum_{n=n_1}^{n_2}|x[n]|r^{-n}<\infty</script><p>一定收敛，收敛域为$z=0,z=\infty$,外的整个z平面</p><ul><li>$n_1&gt;0,0&lt;|z|&lt;=\infty$</li><li>$n_2&lt;0,0=&lt;|z|&lt;\infty$</li><li>$n_1&gt;0,n_2&lt;0,0&lt;|z|&lt;\infty$</li></ul><h3 id="右边序列"><a href="#右边序列" class="headerlink" title="右边序列"></a>右边序列</h3><p>当 n&lt; n1,x[n]=0</p><p>收敛域为</p><script type="math/tex; mode=display">R_{x^-}<z<\infty</script><h3 id="左边序列"><a href="#左边序列" class="headerlink" title="左边序列"></a>左边序列</h3><p>当 n&gt; n2,x[n]=0</p><p>收敛域为</p><script type="math/tex; mode=display">0<z<R_{x^+}</script><h3 id="双边序列"><a href="#双边序列" class="headerlink" title="双边序列"></a>双边序列</h3><p>当 n&lt; n1,n&gt;n2,x[n]=0</p><p>收敛域为</p><script type="math/tex; mode=display">R_{x^-}<z<R_{x^+}</script><h2 id="z变换的几何表示"><a href="#z变换的几何表示" class="headerlink" title="z变换的几何表示"></a>z变换的几何表示</h2><script type="math/tex; mode=display">X(z)=\frac{N(z)}{D(z)}</script><p>零极点</p><h2 id="z变换的性质"><a href="#z变换的性质" class="headerlink" title="z变换的性质"></a>z变换的性质</h2><ul><li>线性性质<ul><li>收敛域至少为交集，可能出现0极点抵消，收敛域扩大<script type="math/tex; mode=display">x[n]=u[n]a^n，y[n]=u[n-1]a^n</script><script type="math/tex; mode=display">X(z)=\frac{z}{z-a} (|z|>|a|)</script><script type="math/tex; mode=display">Y(z)=\frac{a}{z-a} (|z|>|a|)</script><script type="math/tex; mode=display">X(z)-Y(z)=1 (0=<|z|<\infty)</script></li></ul></li><li><p>时域位移</p><script type="math/tex; mode=display">x[n-m]\underrightarrow{z}z^{-m}X(z)</script><blockquote><p>应用：延时器</p></blockquote></li><li><p>频域微分</p><script type="math/tex; mode=display">nx[n]\underrightarrow{z}-z\frac{d}{dz}X(z)</script></li><li>尺度变换<script type="math/tex; mode=display">a^nx[n]\underrightarrow{z}X(\frac{z}{a})</script></li><li>时域扩展<script type="math/tex; mode=display">x[n]_k=\begin{cases}x[n/k], n是k的整数倍\\0，, n不是k的整数倍\end{cases}</script><script type="math/tex; mode=display">sax[n]_k\underrightarrow{z}X(z^k)（R_{k^-}^{1/k}<|z|<R_{k^+}^{1/k}）</script>特殊情况，k=-1<script type="math/tex; mode=display">（R_{k^-}<|1/z|<R_{k^+}）</script></li><li><p>时域卷积</p><script type="math/tex; mode=display">x[n]*y[n]\underrightarrow{z}X(z)Y(z)</script><script type="math/tex; mode=display">（max\{R_{x^-},R_{y^-}\}<|z|<min\{R_{x^+},R_{y^+}\}）</script></li><li><p>共轭性质</p><script type="math/tex; mode=display">x^*[n]\underrightarrow{Z}X^*(z^*)</script></li><li>累加性质<script type="math/tex; mode=display">\sum_{k=-\infty}^nx[k]\underrightarrow{Z}\frac{1}{1-z^{-1}}X(z)</script></li><li><p>初值性质</p><script type="math/tex; mode=display">x[0]=lim_{z\rightarrow\infty}X(z)</script><blockquote><p>x[n]为因果序列,|z|&gt;Rx-</p></blockquote></li><li><p>终值性质</p><script type="math/tex; mode=display">x[0]=lim_{z\rightarrow1}(z-1)X(z)</script><blockquote><p>x[n]为因果序列,|z|&gt;=1,(z-1)X(Z)收敛</p></blockquote></li></ul><h2 id="Z反变换"><a href="#Z反变换" class="headerlink" title="Z反变换"></a>Z反变换</h2><h3 id="幂级数展开法"><a href="#幂级数展开法" class="headerlink" title="幂级数展开法"></a>幂级数展开法</h3><script type="math/tex; mode=display">X(z)=x(0)z^0+x(1)z^{-1}+x(2)z^{-2}+……+x(n)z^{-n}</script><h3 id="利用泰勒级数"><a href="#利用泰勒级数" class="headerlink" title="利用泰勒级数"></a>利用泰勒级数</h3><h3 id="部分分式展开"><a href="#部分分式展开" class="headerlink" title="部分分式展开"></a>部分分式展开</h3><h3 id="留数法"><a href="#留数法" class="headerlink" title="留数法"></a>留数法</h3><ul><li>|z|&gt;a<script type="math/tex; mode=display">x[n]=\begin{cases}0,n<n_0\\\sum_m Res[X(z)z^{n-1}]_{z=p_m},n>=n_0\end{cases}</script></li><li>|z|&lt;a<script type="math/tex; mode=display">x[n]=\begin{cases}-\sum_m Res[X(z)z^{n-1}]_{z=p_m},n<n_0\\0,n>=n_0\\\end{cases}</script>其中<script type="math/tex; mode=display">Res[X(z)z^{n-1}]_{z=p_m}=\frac{1}{(L-1)!}[\frac{d^{L-1}}{dz^{L-1}}(z-p_m)^LX(z)z^{n-1}]_{z=p_m}</script></li></ul><h2 id="单边z变换"><a href="#单边z变换" class="headerlink" title="单边z变换"></a>单边z变换</h2><ul><li>移位性质<script type="math/tex; mode=display">x[n+m]u[n]\underrightarrow{UZ}z^m（X(z)-\sum_{k=0}^{m-1}x[k]z^{-k})</script></li></ul><h2 id="系统分析"><a href="#系统分析" class="headerlink" title="系统分析"></a>系统分析</h2><ul><li>因果性</li></ul><p>一个因果 LTI系统其单位脉冲响应h[n]是对于n&lt;0, h[n]=0，ROC包括无限远点</p><ul><li>稳定性</li></ul><p>一个稳定离散时间LTI系要求其单位脉冲响应h[n]<br>满足绝对可和的，ROC必须包含单位圆，一个LTI系统当且仅当它的系统函数H(z)的ROC<br>包括单位圆，|z|=1时，该系统就是稳定的。</p><ul><li>因果稳定离散时间LTI系统</li></ul><p>一个具有有理系统函数的因果LTI系统，当且仅<br>当H(z)的全部极点都位于单位圆内时，也即全部极点<br>其模均小于1时，系统就是稳定的。</p>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手撕拉普拉斯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>拉普拉斯变换</title>
      <link href="2020/08/21/laplace-transformation/"/>
      <url>2020/08/21/laplace-transformation/</url>
      
        <content type="html"><![CDATA[<h2 id="拉普拉斯变换"><a href="#拉普拉斯变换" class="headerlink" title="拉普拉斯变换"></a>拉普拉斯变换</h2><p>作用：不是所有的信号可以进行傅里叶变换，引入衰减因子$e^{-\sigma t}$,使$x(t)e^{-\sigma t}$的傅里叶变换收敛</p><span id="more"></span><script type="math/tex; mode=display">F\{x(t)e^{-\sigma t}\}=\int_{-\infty}^{\infty}x(t)e^{-\sigma t}e^{-jwt}dt=\int_{-\infty}^{\infty}x(t)e^{-t(\sigma+jw)}dt</script><script type="math/tex; mode=display">X(\sigma+jw)=\int_{-\infty}^{\infty}x(t)e^{-(\sigma+jw)t}dt</script><p>其傅里叶反变换</p><script type="math/tex; mode=display">x(t)e^{-\sigma t}=\frac{1}{2\pi}\int_{-\infty}^{\infty}X(\sigma+jw)e^{jwt}dt</script><script type="math/tex; mode=display">x(t)=\frac{1}{2\pi}\int_{-\infty}^{\infty}X(\sigma+jw)e^{t(\sigma+jw)}dt</script><p>令$s=\sigma+jw$</p><script type="math/tex; mode=display">X(s)=\int_{-\infty}^{\infty}x(t)e^{-st}dt</script><script type="math/tex; mode=display">x(t)=\frac{1}{2\pi j}\int_{\sigma-j\infty}^{\sigma+j\infty}X(s)e^{st}dt</script><p>简写为</p><script type="math/tex; mode=display">X(s)=L\{x(t)\}</script><script type="math/tex; mode=display">x(t)=L^{-1}\{X(s)\}</script><blockquote><p>当x（t）满足傅里叶变换的条件，x（t）的拉普拉斯变换等价于傅里叶变换</p></blockquote><h2 id="拉普拉斯变换的收敛域"><a href="#拉普拉斯变换的收敛域" class="headerlink" title="拉普拉斯变换的收敛域"></a>拉普拉斯变换的收敛域</h2><p>ROC：能让x（t）收敛的s范围</p><h3 id="x-t-的时域特性与拉氏变换X-s-的收敛域ROC关系"><a href="#x-t-的时域特性与拉氏变换X-s-的收敛域ROC关系" class="headerlink" title="x(t)的时域特性与拉氏变换X(s)的收敛域ROC关系"></a>x(t)的时域特性与拉氏变换X(s)的收敛域ROC关系</h3><ul><li>X (s)的ROC在S平面上由平行于jω轴的带状区域构成。</li><li>对有理拉氏变换来说，在ROC内不包含任何极点</li><li>如果 是时限的，并且绝对可积, 则<br>ROC是整个S平面</li><li>如果 是右边信号，<br>而且如果$Re{s}=\sigma_0$这条线位于<br>ROC 内，那么$Re{s}&gt;\sigma_0$的<br>全部s值在ROC内。左边信号相反，双边信号为带状</li><li>:如果 的拉氏变换 是有理的，则ROC的边界由<br>极点限定，或延伸到无穷远，且在ROC内不包含任何极点</li><li>如果 的拉氏变换 是有理的，若 是右边信<br>号，则其ROC 在s平面上位于最右边极点的右边；若 是左<br>边信号，则其ROC 在s平面上位于最左边极点的左边。</li></ul><h3 id="常见信号拉氏变换"><a href="#常见信号拉氏变换" class="headerlink" title="常见信号拉氏变换"></a>常见信号拉氏变换</h3><ul><li>阶跃信号<script type="math/tex; mode=display">u(t)\underrightarrow{L}=\frac{1}{s}</script></li><li>冲激信号<script type="math/tex; mode=display">\delta(t)\underrightarrow{L}=1</script></li><li>单边指数信号<script type="math/tex; mode=display">x_1(t)=e^{-at}u(t)</script><script type="math/tex; mode=display">X_1(s)=\frac{1}{s+a} (Re\{s\}>-a)</script><script type="math/tex; mode=display">x_2(t)=-e^{-at}u(-t)</script><script type="math/tex; mode=display">X_2(s)=\frac{1}{s+a} (Re\{s\}<-a)</script></li><li>双边指数信号</li></ul><script type="math/tex; mode=display">x(t)=e^{-|b|t}</script><script type="math/tex; mode=display">X_1(s)=\frac{2b}{s^2-b^2} (Re\{s\}<|b|)</script><ul><li>三角信号<script type="math/tex; mode=display">sinwtu(t)\underrightarrow{L^{-1}}\frac{w}{s^2+w^2}</script><script type="math/tex; mode=display">coswtu(t)\underrightarrow{L^{-1}}\frac{s}{s^2+w^2}</script></li><li>幂函数<script type="math/tex; mode=display">t^nu(t)\underrightarrow{L}=\frac{n!}{s^{n+1}}</script></li><li>指数三角信号<script type="math/tex; mode=display">e^{-at}sinwtu(t)\underrightarrow{L^{-1}}\frac{w}{(s+a)^2+w^2}</script><script type="math/tex; mode=display">e^{-at}coswtu(t)\underrightarrow{L^{-1}}\frac{s+a}{(s+a)^2+w^2}</script><h2 id="双边拉氏变换性质"><a href="#双边拉氏变换性质" class="headerlink" title="双边拉氏变换性质"></a>双边拉氏变换性质</h2></li><li>线性性质<ul><li>Roc=R1+R2</li></ul></li><li>时移性质<script type="math/tex; mode=display">x(t-t_0)\underrightarrow{L}e^{-st_0}X(s)</script>ROC=R</li><li>频移性质<script type="math/tex; mode=display">e^{s_0t}x(t)\underrightarrow{L}X(s-s_0)</script>ROC=R+Re{s0}</li><li>尺度变换<script type="math/tex; mode=display">x(at)\underrightarrow{L}=\frac{1}{|a|}X(\frac{s}{a})</script>ROC=aR</li><li>共轭</li></ul><script type="math/tex; mode=display">x^*(t)\underrightarrow{L}X^*(s^*)</script><ul><li>时域卷积<script type="math/tex; mode=display">x_1(t)*x_2(t)\underrightarrow{L}X_1(s)X_2(s)</script></li><li>时域微分<script type="math/tex; mode=display">\frac{dx(t)}{dt}\underrightarrow{L}sX(s)</script></li><li>频域微分<script type="math/tex; mode=display">-tdx(t)\underrightarrow{L}\frac{dX(s)}{ds}</script></li><li>时域积分<script type="math/tex; mode=display">\int_{-\infty}^{t}x(\tau)d\tau=\frac{1}{s}X(s)</script>R+Re{s}</li><li>初值定理和终值定理<ul><li>若t&lt;0,x(t)=0并且在t=0时,不包含冲激或高阶奇异函数<script type="math/tex; mode=display">x(0^+)=\underset{s\to\infty}{lim}sX(s)</script><script type="math/tex; mode=display">x(\infty)=\underset{s\to0}{lim}sX(s)</script></li></ul></li></ul><h2 id="周期信号与抽样信号的拉氏变换"><a href="#周期信号与抽样信号的拉氏变换" class="headerlink" title="周期信号与抽样信号的拉氏变换"></a>周期信号与抽样信号的拉氏变换</h2><h3 id="周期信号"><a href="#周期信号" class="headerlink" title="周期信号"></a>周期信号</h3><script type="math/tex; mode=display">X(s)=X_1(s)\sum_{n=0}^{\infty}e^{-nST}</script><script type="math/tex; mode=display">=X_1(s)\frac{e^{sT}}{e^{sT}-1}</script><script type="math/tex; mode=display">(Re\{s\}>0)</script><blockquote><p>x1(t)为第一个周期内时间函数</p></blockquote><h2 id="抽样信号"><a href="#抽样信号" class="headerlink" title="抽样信号"></a>抽样信号</h2><script type="math/tex; mode=display">x_s(t)=x(t)\delta_T(t)\cdot u(t)</script><script type="math/tex; mode=display">=\sum_{n=0}^{\infty}x(nT)\delta(t-nT)</script><script type="math/tex; mode=display">\underrightarrow{L}\sum_{n=0}^{\infty}x(nT)(e^{-sT})^n</script><script type="math/tex; mode=display">如果令e^{sT}=z</script><script type="math/tex; mode=display">L[x_s(t)]=\sum_{n=0}^{\infty}x(nT)z^{-n}</script><script type="math/tex; mode=display">变为z变换</script><h2 id="拉氏反变换"><a href="#拉氏反变换" class="headerlink" title="拉氏反变换"></a>拉氏反变换</h2><ul><li>长除法法</li><li>公式法</li></ul><h2 id="单边拉氏变换"><a href="#单边拉氏变换" class="headerlink" title="单边拉氏变换"></a>单边拉氏变换</h2><ul><li>时域微分<script type="math/tex; mode=display">\frac{dx(t)}{dt}\underrightarrow{UL}s\hat{X}(s)-x(0)</script></li><li>时域积分<script type="math/tex; mode=display">\int_{-\infty}^{t}x(\tau)d\tau=\frac{1}{s}\hat{X}(s)+\frac{\int_{-\infty}^{0^-}x(\tau)d\tau}{s}</script></li><li>卷积积分<script type="math/tex; mode=display">x_1(t)*x_2(t)\underrightarrow{L}\hat{X}_1(s)\hat{X}_2(s)</script></li></ul><h2 id="系统复频域分析"><a href="#系统复频域分析" class="headerlink" title="系统复频域分析"></a>系统复频域分析</h2><h3 id="因果性"><a href="#因果性" class="headerlink" title="因果性"></a>因果性</h3><p>一个因果LTI系统，其收敛域为右半平面；如果系<br>统是反因果的，收敛域为左半平面。 相反的结论不一<br>定都成立 。</p><h3 id="稳定性"><a href="#稳定性" class="headerlink" title="稳定性"></a>稳定性</h3><p>稳定系统的冲激响应应是绝对可积，表明稳定系统的频率响应存在。<br>稳定系统的ROC必包含虚轴（jw轴）</p><h3 id="因果稳定系统"><a href="#因果稳定系统" class="headerlink" title="因果稳定系统"></a>因果稳定系统</h3><script type="math/tex; mode=display">H(s)=\frac{s-1}{(s+1)(s-2)}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/21/e33ee988b52c3100fb7d965ebd6c1420.png" alt=""></p><h3 id="系统响应求解"><a href="#系统响应求解" class="headerlink" title="系统响应求解"></a>系统响应求解</h3><p>已知某因果的 LTI 系统的微分方程：$y^{‘’}+3y^{‘}+2y=x(t),y(0^-)=3,y^{‘}(0^-)=-5$，求当<br>x(t)=2u(t)时系统的全响应、零输入响应、零状态响应。</p><script type="math/tex; mode=display">取拉氏变换s^2Y(s)-sy(0)-y^{'}(0)+3sY(s)-3y(0)+2Y(s)=X(s)</script><script type="math/tex; mode=display">\to Y(s)=\frac{X(s)+3(s+3)-5}{s^2+3s+2}</script><script type="math/tex; mode=display">\to Y_{zi}(s)=\frac{3(s+3)-5}{s^2+3s+2}=\frac{2}{s+2}+\frac{1}{s+1}</script><script type="math/tex; mode=display">\to Y_{zs}(s)=\frac{X(s)}{s^2+3s+2}=\frac{1}{s}-\frac{2}{s+1}+\frac{1}{s+2}</script><script type="math/tex; mode=display">\to \begin{cases}零输入响应：y_{zi}(t)=(e^{-t}+2e^{-2t})u(t)\\零状态响应:y_{zs}(t)=(1-2e^{-t}+e^{-2t})u(t)\\全响应：y(t)=(1-e^{-t}+3e^{-2t})u(t)\end{cases}</script><h2 id="S域原件模型"><a href="#S域原件模型" class="headerlink" title="S域原件模型"></a>S域原件模型</h2><script type="math/tex; mode=display">\hat{V}_R(s)=R\hat{I}_R(s)</script><script type="math/tex; mode=display">\hat{V}_L(s)=sL\hat{I}_L(s)-Li_L(0^-)</script><script type="math/tex; mode=display">\hat{V}_C(s)=\frac{1}{sC}\hat{I}_C(s)+\frac{1}{s}v_C(0^-)</script><script type="math/tex; mode=display">\hat{I}_R(s)=\frac{1}{R}\hat{V}_R(s)</script><script type="math/tex; mode=display">\hat{I}_L(s)=\frac{1}{sL}\hat{V}_L(s)+\frac{1}{s}i_L(0^-)</script><script type="math/tex; mode=display">\hat{I}_C(s)=sC\hat{V}_C(s)-Cv_C(0^-)</script>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手撕拉普拉斯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>采样和调制</title>
      <link href="2020/08/21/Sampling-and-modulation/"/>
      <url>2020/08/21/Sampling-and-modulation/</url>
      
        <content type="html"><![CDATA[<h1 id="连续时间采样定理"><a href="#连续时间采样定理" class="headerlink" title="连续时间采样定理"></a>连续时间采样定理</h1><h2 id="冲激串采样定理"><a href="#冲激串采样定理" class="headerlink" title="冲激串采样定理"></a>冲激串采样定理</h2><script type="math/tex; mode=display">x_p(t)=x(t)p(t)</script><script type="math/tex; mode=display">p(t)=\sum_{n=-\infty}^\infty\delta(t-nT)</script><p>T为采样周期</p><span id="more"></span><script type="math/tex; mode=display">X_p(jw)=\frac{1}{2\pi}X(jw)*P(jw)</script><script type="math/tex; mode=display">=\frac{1}{T}\sum_{k=-\infty}^\infty X(j(w-kw_s))</script><p>相当于将信号频域的图像左右平移<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/21/cd375deb4debf84d4eec89018faaf266.png" alt=""></p><h2 id="采样定理"><a href="#采样定理" class="headerlink" title="采样定理"></a>采样定理</h2><ul><li>$x(t)$是一带限信号，$X(jw)=0,|w|&gt;w_M$</li><li>采样频率$w_s&gt;2w_M$,其中$w_s=2\pi/T$,T为抽样周期，当$w_s=2w_M$,称作奈奎斯特率。满足条件的信号可以采样后重建。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/21/99a6952d0e3d2da0ff8aaff1d2f26420.png" alt=""></p><ul><li>H( jw)为一低通滤波器</li></ul><script type="math/tex; mode=display">x_r(t)=\sum_{n=-\infty}^\infty x(nT)\delta(t-nT)*h(t)</script><script type="math/tex; mode=display">=\sum_{n=-\infty}^\infty x(nT)h(t-nT)</script><h3 id="带限内插"><a href="#带限内插" class="headerlink" title="带限内插"></a>带限内插</h3><ul><li>当h(t)为理想滤波器时，即</li></ul><script type="math/tex; mode=display">h(t)=T\frac{w_c}{\pi}Sa(w_ct)</script><p>当$w_c$满足：$w_M&lt;w_c&lt;w_s-w_M$时，重建是精确的。 </p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/21/c8aacd6ecdb8de1aeaf235fdaf867e99.png" alt=""></p><script type="math/tex; mode=display">x_r(t)=\sum_{n=-N}^N x(nT)T\frac{w_c}{\pi}Sa(w_c(t-nT))</script><h3 id="零阶保持"><a href="#零阶保持" class="headerlink" title="零阶保持"></a>零阶保持</h3><p>如果h(t)取矩形窗函数<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/21/c92525f0e7e3da23eda6d3cefd8ca72b.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/21/e2a0babfc3c05b2aff3e432089c665b0.png" alt=""></p><h3 id="零阶保持采样"><a href="#零阶保持采样" class="headerlink" title="零阶保持采样"></a>零阶保持采样</h3><p>冲激串抽样的数学模型在实际工程应用中是无法实<br>现的，其重要意义是在理论上建立采样定理。在实际工<br>程应用中，往往采用零阶保持的方法来获取采样信号。</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/21/bb5d4862a3afe394d519b8f6c21149cd.png" alt=""></p><h3 id="欠采样"><a href="#欠采样" class="headerlink" title="欠采样"></a>欠采样</h3><p>被采用的信号若是正弦信号，在频谱混叠的情况下可以获得重建信号</p><h1 id="离散时间信号的时域采样定理"><a href="#离散时间信号的时域采样定理" class="headerlink" title="离散时间信号的时域采样定理"></a>离散时间信号的时域采样定理</h1><h2 id="脉冲串采样"><a href="#脉冲串采样" class="headerlink" title="脉冲串采样"></a>脉冲串采样</h2><script type="math/tex; mode=display">p[n]=\sum_{k=-\infty}^\infty \delta[n-kN]</script><script type="math/tex; mode=display">x_p[n]=x[n]p[n]</script><script type="math/tex; mode=display">=\sum_{k=-\infty}^\infty x[kN] \delta[n-kN]</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/21/04275e4d4edb28371ab3a55dad3c6998.png" alt=""></p><script type="math/tex; mode=display">P(e^{jw})=\frac{2\pi}{N}\sum_{k=-\infty}^\infty \delta(w-kw_s)</script><script type="math/tex; mode=display">X_p(e^{jw})=\frac{1}{N}\sum_{k=0}^{N-1}X(e^{j(w-kw_s)})</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/21/a2786c4c28f8bf0599e8798e8df94a1d.png" alt=""></p><h2 id="离散时间的采样定理"><a href="#离散时间的采样定理" class="headerlink" title="离散时间的采样定理"></a>离散时间的采样定理</h2><p>设x[n]是某一带限信号，即在$w_M&lt;|w|&lt;\pi$时,$X(e^{jw})=0$。如果采样频<br>率$w_s&gt;2w_M$，那么x[n] 就唯一地由其样<br>本x[kN]， 所确定。</p><h2 id="离散时间的抽取与内插"><a href="#离散时间的抽取与内插" class="headerlink" title="离散时间的抽取与内插"></a>离散时间的抽取与内插</h2><h3 id="抽取和减采样"><a href="#抽取和减采样" class="headerlink" title="抽取和减采样"></a>抽取和减采样</h3><script type="math/tex; mode=display">x_s[n]=x[nN]=x_p[nN]</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/21/c68233b27e0550fd3ffb7954428ab64c.png" alt=""></p><script type="math/tex; mode=display">X_s(e^{jw})=X_p(e^{jw/N})</script><p>$x_s[n]$的频谱将$X_p(e^{jw})$扩展了N 倍</p><h3 id="内插和增采样"><a href="#内插和增采样" class="headerlink" title="内插和增采样"></a>内插和增采样</h3><script type="math/tex; mode=display">x[n]=\begin{cases}x_s(n/N)\\0\end{cases}</script><script type="math/tex; mode=display">X(e^{jw})=X_s(e^{jwN})</script><h1 id="连续时间系统的离散实现"><a href="#连续时间系统的离散实现" class="headerlink" title="连续时间系统的离散实现"></a>连续时间系统的离散实现</h1><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/21/2734e97ea60f009c907ff61dd321c77a.png" alt=""></p><h1 id="正弦载波调制"><a href="#正弦载波调制" class="headerlink" title="正弦载波调制"></a>正弦载波调制</h1><script type="math/tex; mode=display">y(t)=x(t)\cdot c(t)</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/21/4cf88c3169d4d98b334db888eedacadd.png" alt=""><br>正弦载波幅度调制的主要功能是实现频谱搬移功能。<br>DSB调制通常也称为抑制载波的正弦载波调制</p><script type="math/tex; mode=display">x_r=(y(t)\cos w_ct)*h(t)</script><script type="math/tex; mode=display">=(x(t)\frac{\cos(2w_ct)+1}{2})*h(t)=\frac{1}{2}x(t)</script><h1 id="脉冲幅度调制"><a href="#脉冲幅度调制" class="headerlink" title="脉冲幅度调制"></a>脉冲幅度调制</h1><p>PAM是脉冲载波的幅度随调制信号变化的<br>一种调制方式：它是对调制信号的取样，即抽<br>取某一时间间隙内的调制信号的信息。脉冲调<br>制有两种基本形式：</p><ul><li>自然采样</li><li>平顶采样（零阶保持采样）。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手撕拉普拉斯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>离散时间信号频域分析</title>
      <link href="2020/08/20/DFT/"/>
      <url>2020/08/20/DFT/</url>
      
        <content type="html"><![CDATA[<h1 id="离散时间傅里叶级数"><a href="#离散时间傅里叶级数" class="headerlink" title="离散时间傅里叶级数"></a>离散时间傅里叶级数</h1><script type="math/tex; mode=display">x[n]=\sum_{k=<N>}a_ke^{jkw_0n}=\sum_{k=<N>}a_ke^{jk\frac{2\pi}{T_0}n}</script><script type="math/tex; mode=display">a_k=\frac{1}{N}\sum_{n=<N>}x[n]e^{-jkw_0n}</script><span id="more"></span><blockquote><p>离散时间傅里叶级数不存在着收敛问题和吉布斯现象</p></blockquote><h1 id="离散傅里叶变换"><a href="#离散傅里叶变换" class="headerlink" title="离散傅里叶变换"></a>离散傅里叶变换</h1><script type="math/tex; mode=display">X(e^{jw})=\sum_{n=-\infty}^\infty x[n]e^{-jwn}</script><script type="math/tex; mode=display">x[n]=\frac{1}{2\pi}\int_{2\pi}X(e^{jw})e^{jwn}dw</script><script type="math/tex; mode=display">X(e^{jw})=|X(e^{jw})|e^{j\theta(w)}</script><h2 id="离散时间傅里叶变换的收敛性与吉布斯现象"><a href="#离散时间傅里叶变换的收敛性与吉布斯现象" class="headerlink" title="离散时间傅里叶变换的收敛性与吉布斯现象"></a>离散时间傅里叶变换的收敛性与吉布斯现象</h2><p>x[n]要求绝对可和并能量有限</p><script type="math/tex; mode=display">\sum_{n=-\infty}^\infty |x[n]|<\infty</script><script type="math/tex; mode=display">\sum_{n=-\infty}^\infty |x[n]|^2<\infty</script><h2 id="常见函数离散傅里叶变换"><a href="#常见函数离散傅里叶变换" class="headerlink" title="常见函数离散傅里叶变换"></a>常见函数离散傅里叶变换</h2><h3 id="单位脉冲"><a href="#单位脉冲" class="headerlink" title="单位脉冲"></a>单位脉冲</h3><script type="math/tex; mode=display">\delta[n]\to 1</script><h3 id="单边指数"><a href="#单边指数" class="headerlink" title="单边指数"></a>单边指数</h3><script type="math/tex; mode=display">x[n]=a^nu[n] \to\frac{1}{1-ae^{-jw}}</script><h3 id="双边指数"><a href="#双边指数" class="headerlink" title="双边指数"></a>双边指数</h3><script type="math/tex; mode=display">x[n]=a^{|n|} \to\frac{1-a^2}{1-2a\cos w+a^2}</script><h3 id="矩形脉冲"><a href="#矩形脉冲" class="headerlink" title="矩形脉冲"></a>矩形脉冲</h3><script type="math/tex; mode=display">x[n]=\begin{cases}1&|n|<N_1\\0&|n|>N_1\end{cases}</script><script type="math/tex; mode=display">X(e^{jw})=\frac{\sin[(2N_1+1)w/2]}{\sin(w/2)}</script><h3 id="冲激串"><a href="#冲激串" class="headerlink" title="冲激串"></a>冲激串</h3><script type="math/tex; mode=display">1 \to 2\pi\sum_{k=-\infty}^{\infty}\delta(w-2\pi k)</script><h1 id="离散周期信号傅里叶变换"><a href="#离散周期信号傅里叶变换" class="headerlink" title="离散周期信号傅里叶变换"></a>离散周期信号傅里叶变换</h1><script type="math/tex; mode=display">X(e^{jw})=2\pi\sum_{k=-\infty}^{\infty}a_k\delta(w-2\pi k/N)</script><h1 id="离散傅里叶变换性质"><a href="#离散傅里叶变换性质" class="headerlink" title="离散傅里叶变换性质"></a>离散傅里叶变换性质</h1><h3 id="周期性"><a href="#周期性" class="headerlink" title="周期性"></a>周期性</h3><h3 id="线性性质"><a href="#线性性质" class="headerlink" title="线性性质"></a>线性性质</h3><h3 id="时移性质"><a href="#时移性质" class="headerlink" title="时移性质"></a>时移性质</h3><script type="math/tex; mode=display">x[n-n_0]\to e^{-jwn_0}X(e^{jw})</script><h3 id="频移性质"><a href="#频移性质" class="headerlink" title="频移性质"></a>频移性质</h3><script type="math/tex; mode=display">X(e^{j(w-w_0)})\to x(t)e^{jw_0n}</script><h3 id="共轭"><a href="#共轭" class="headerlink" title="共轭"></a>共轭</h3><script type="math/tex; mode=display">x^*(t)\to X^*(e^{-jw})</script><ul><li>x(t)为实值函数，X(jw)实部是偶函数，虚部是奇函数，幅度是偶函数，相位是奇函数</li><li>x(t)为实值偶函数，则X(jw)也是实值偶函数</li><li>x(t)为实值奇函数，则X(jw)也是纯虚奇函数</li><li>该性质也适用于傅里叶级数情况，</li></ul><h3 id="差分求和"><a href="#差分求和" class="headerlink" title="差分求和"></a>差分求和</h3><script type="math/tex; mode=display">x[n]-x[n-1]\to (1-e^{-jw})X(jw)</script><script type="math/tex; mode=display">\sum_{m=-\infty}^nx[m] \to \frac{1}{(1-e^{-jw})}X(e^{jw})\pi X(e^{j0})\sum_{m=-\infty}^\infty \delta(w-2\pi k)</script><h3 id="时域扩展"><a href="#时域扩展" class="headerlink" title="时域扩展"></a>时域扩展</h3><script type="math/tex; mode=display">x_(k)=\begin{cases}x[n/k]&n为k整数倍\\0\end{cases}</script><script type="math/tex; mode=display">x(k)[n]\to X(e^{jkw})</script><h3 id="频域微分"><a href="#频域微分" class="headerlink" title="频域微分"></a>频域微分</h3><script type="math/tex; mode=display">j\frac{dX(e^{jw})}{dw}\to nx[n]</script><h3 id="卷积性质"><a href="#卷积性质" class="headerlink" title="卷积性质"></a>卷积性质</h3><script type="math/tex; mode=display">x[n]*h[n] \to X(e^{jw})H(e^{jw})</script><h4 id="周期卷积"><a href="#周期卷积" class="headerlink" title="周期卷积"></a>周期卷积</h4><script type="math/tex; mode=display">\sum_{r=<N>}x[r]h[n-r] \to Na_kb_k</script><h3 id="调制性质"><a href="#调制性质" class="headerlink" title="调制性质"></a>调制性质</h3><script type="math/tex; mode=display">x[n]y[n] \to \frac{1}{2\pi}X(e^{jw})*Y(e^{jw})</script><p>当傅里叶级数存在时</p><script type="math/tex; mode=display">x[n]y[n] \to a_k*b_k</script><h3 id="帕斯瓦尔定理"><a href="#帕斯瓦尔定理" class="headerlink" title="帕斯瓦尔定理"></a>帕斯瓦尔定理</h3><script type="math/tex; mode=display">\sum_{k=-\infty}^\infty |x(t)|^2dt=\frac{1}{2\pi}\int_{2\pi}|X(e^{jw})|^2dw</script><h1 id="离散傅里叶变换的对偶性"><a href="#离散傅里叶变换的对偶性" class="headerlink" title="离散傅里叶变换的对偶性"></a>离散傅里叶变换的对偶性</h1><p>离散时间傅里叶变换和连续时间傅里叶<br>级数在数学形式上也是十分相似的，它们之间<br>也存在着一种对偶关系。</p><script type="math/tex; mode=display">x[n] \to a[k]</script><script type="math/tex; mode=display">a[n] \to \frac{1}{N}x[-k]</script><h1 id="离散时间LTI系统频率响应"><a href="#离散时间LTI系统频率响应" class="headerlink" title="离散时间LTI系统频率响应"></a>离散时间LTI系统频率响应</h1><p>离散LTI系统的频率响应另一种定义可表示为</p><script type="math/tex; mode=display">H(e^{jw})=\frac{Y(e^{jw})}{X(e^{jw})}</script>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手撕拉普拉斯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>连续时间信号频域分析</title>
      <link href="2020/08/19/constant-Flourier/"/>
      <url>2020/08/19/constant-Flourier/</url>
      
        <content type="html"><![CDATA[<h1 id="连续时间周期信号的傅里叶级数"><a href="#连续时间周期信号的傅里叶级数" class="headerlink" title="连续时间周期信号的傅里叶级数"></a>连续时间周期信号的傅里叶级数</h1><p>复指数信号是LTI系统的特征函数，对于连续时间周期信号，都可以表示为成谐波关系的复指数信号<br><span id="more"></span></p><script type="math/tex; mode=display">x(t)=\sum_ka_ke^{jkw_0t}=\sum_ka_ke^{jk\frac{2\pi}{T_0}t}</script><script type="math/tex; mode=display">a_k=\frac{1}{T_0}\int_{T_0}x(t)e^{-jkw_0t}dt</script><h2 id="三角形式的傅里叶级数"><a href="#三角形式的傅里叶级数" class="headerlink" title="三角形式的傅里叶级数"></a>三角形式的傅里叶级数</h2><script type="math/tex; mode=display">x(t)=B_0+\sum_{k=1}^{\infty}(B_k\cos kw_0t+C_k\sin kw_0t)</script><script type="math/tex; mode=display">B_0=\frac{1}{T_0}\int_{T_0}x(t)dt</script><script type="math/tex; mode=display">B_k=\frac{2}{T_0}\int_{T_0}x(t)\cos kw_0tdt</script><script type="math/tex; mode=display">C_k=\frac{2}{T_0}\int_{T_0}x(t)\sin kw_0tdt</script><h2 id="常见信号的傅里叶级数"><a href="#常见信号的傅里叶级数" class="headerlink" title="常见信号的傅里叶级数"></a>常见信号的傅里叶级数</h2><h3 id="三角信号"><a href="#三角信号" class="headerlink" title="三角信号"></a>三角信号</h3><script type="math/tex; mode=display">x(t)=\sin w_0t=\frac{1}{2j}(e^{jw_0t}-e^{-jw_0t})</script><h3 id="周期方波"><a href="#周期方波" class="headerlink" title="周期方波"></a>周期方波</h3><script type="math/tex; mode=display">x(t)=\begin{cases}1&|t|<T_1\\0&T_1<|t|<T/2\end{cases}</script><script type="math/tex; mode=display">a_k=\frac{w_0T_1}{\pi}Sa(wT_1)|_{w=kw_0}</script><blockquote><p>周期方波信号傅里叶级数按1/k衰减</p></blockquote><h3 id="冲激串信号"><a href="#冲激串信号" class="headerlink" title="冲激串信号"></a>冲激串信号</h3><script type="math/tex; mode=display">\delta_T(t)=\sum_{n=-\infty}^\infty\delta(t-nT)</script><script type="math/tex; mode=display">a_k=\frac{1}{T}</script><h2 id="连续时间周期信号傅里叶级数的收敛性"><a href="#连续时间周期信号傅里叶级数的收敛性" class="headerlink" title="连续时间周期信号傅里叶级数的收敛性"></a>连续时间周期信号傅里叶级数的收敛性</h2><h3 id="等效性"><a href="#等效性" class="headerlink" title="等效性"></a>等效性</h3><p>傅里叶级数是有限项级数近似x(t)的最佳表示</p><script type="math/tex; mode=display">e_N=x(t)-\sum_{k=-N}^Na_ke^{jkw_0t}</script><script type="math/tex; mode=display">E_N=\int_T|e_N(t)|^2dt</script><script type="math/tex; mode=display">\lim_{N\to\infty}E_N=0</script><h3 id="收敛性"><a href="#收敛性" class="headerlink" title="收敛性"></a>收敛性</h3><p>一个周期信号具有傅里叶级数的表示形式，必需具备条件：<br><strong>傅里叶系数是一个有限值</strong></p><h4 id="收敛条件"><a href="#收敛条件" class="headerlink" title="收敛条件"></a>收敛条件</h4><script type="math/tex; mode=display">\int_T|x(t)|^2dt<\infty</script><h4 id="狄里赫利（Dirichlet）收敛条件"><a href="#狄里赫利（Dirichlet）收敛条件" class="headerlink" title="狄里赫利（Dirichlet）收敛条件"></a>狄里赫利（Dirichlet）收敛条件</h4><ul><li>在一周期内， 必须绝对可积</li></ul><script type="math/tex; mode=display">\int_T|x(t)|dt<\infty</script><ul><li>在一周期间隔内， 的最大值和最小值的数目有<br>限；即在任何有限间隔内， 具有有限个起伏变化。</li><li>在的一个周期内，只有有限个不连续点，而且在这些不连续点上，函数值是有限的。</li></ul><h4 id="吉布斯现象"><a href="#吉布斯现象" class="headerlink" title="吉布斯现象"></a>吉布斯现象</h4><p>当用傅里叶级数的前N次谐波分量去近似原来的信号时<br>会产生 <strong>吉布斯(Gibbs)</strong> 现象。</p><ul><li>信号的间断点两侧将呈现高频起伏和9%超量。</li><li>当N增大时，这些高频起伏和超量所拥有的能量将<br>减少，并趋向于信号的间断点，无论N多大，都不会<br>消失。但$\lim_{N\to\infty}E_N=0$</li><li>均方误差等于零的意义下，傅里叶级数收敛于原<br>来信号。</li></ul><h1 id="连续时间傅里叶变换"><a href="#连续时间傅里叶变换" class="headerlink" title="连续时间傅里叶变换"></a>连续时间傅里叶变换</h1><p>对于非周期的信号我们改如何处理？</p><p>任意的非周期信号我们可以看做周期为无穷大的周期信号，因此引入了傅里叶变换</p><p>比如对于周期方波</p><script type="math/tex; mode=display">a_k=\frac{w_0T_1}{\pi}Sa(wT_1)|_{w=kw_0}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/19/dd8912b9d9ede45e7369bf59611a551b.png" alt=""></p><h2 id="傅里叶变换公式"><a href="#傅里叶变换公式" class="headerlink" title="傅里叶变换公式"></a>傅里叶变换公式</h2><script type="math/tex; mode=display">X(jw)=\int_{-\infty}^{\infty}x(t)e^{-jwt}dt</script><script type="math/tex; mode=display">x(t)=\frac{1}{2\pi}\int_{-\infty}^{\infty}X(jw)e^{jwt}dw</script><script type="math/tex; mode=display">x(t)\to X(jw)</script><h3 id="极坐标形式"><a href="#极坐标形式" class="headerlink" title="极坐标形式"></a>极坐标形式</h3><script type="math/tex; mode=display">X(iw)=|X(jw)|e^{j\theta(jw)}</script><h2 id="连续时间傅里叶变换的收敛条件"><a href="#连续时间傅里叶变换的收敛条件" class="headerlink" title="连续时间傅里叶变换的收敛条件"></a>连续时间傅里叶变换的收敛条件</h2><script type="math/tex; mode=display">\int_{-\infty}^\infty|x(t)|^2dt<\infty</script><script type="math/tex; mode=display">\int_{-\infty}^\infty|x(t)|dt<\infty</script><h2 id="常见函数傅里叶变换"><a href="#常见函数傅里叶变换" class="headerlink" title="常见函数傅里叶变换"></a>常见函数傅里叶变换</h2><h3 id="单边指数"><a href="#单边指数" class="headerlink" title="单边指数"></a>单边指数</h3><script type="math/tex; mode=display">x(t)=e^{-at}u(t),t>0</script><script type="math/tex; mode=display">X(jw)=\frac{1}{a+jw}</script><script type="math/tex; mode=display">|X(jw)|=1/\sqrt{a^2+w^2}</script><script type="math/tex; mode=display">\phi(w)=-arctan(w/a)</script><hr><script type="math/tex; mode=display">\frac{t^{n-1}}{(n-1)!}e^{-at}u(t)\to \frac{1}{(a+jw)^n}</script><h3 id="双边指数"><a href="#双边指数" class="headerlink" title="双边指数"></a>双边指数</h3><script type="math/tex; mode=display">x(t)=e^{-a|t|},a>0</script><script type="math/tex; mode=display">X(jw)=\frac{2a}{a^2+w^2}</script><h3 id="单位冲激"><a href="#单位冲激" class="headerlink" title="单位冲激"></a>单位冲激</h3><script type="math/tex; mode=display">\delta(t) \to X(jw)=1</script><h3 id="冲激偶函数"><a href="#冲激偶函数" class="headerlink" title="冲激偶函数"></a>冲激偶函数</h3><script type="math/tex; mode=display">\delta'(t) \to jw</script><h3 id="矩形窗函数"><a href="#矩形窗函数" class="headerlink" title="矩形窗函数"></a>矩形窗函数</h3><script type="math/tex; mode=display">x(t)=\begin{cases}1&|t|<T_1\\0&other\end{cases}</script><script type="math/tex; mode=display">X(jw)=2T_1Sa(wT_1)</script><h3 id="高斯脉冲"><a href="#高斯脉冲" class="headerlink" title="高斯脉冲"></a>高斯脉冲</h3><script type="math/tex; mode=display">x(t)=Ee^{-(\frac{t}{\tau})^2}</script><script type="math/tex; mode=display">X(jw)=\sqrt{\pi}E\tau e^{-(\frac{w\tau}{2})^2}</script><h2 id="周期信号的傅里叶变换"><a href="#周期信号的傅里叶变换" class="headerlink" title="周期信号的傅里叶变换"></a>周期信号的傅里叶变换</h2><p>周期函数的周期信号会聚集在谐波频率上，在频域上呈冲激函数</p><script type="math/tex; mode=display">e^{jw_0t} \to 2\pi\delta(w-w_0)</script><p>所以对于周期信号</p><script type="math/tex; mode=display">x(t)=\sum_{k=-\infty}^{\infty}a_ke^{jkw_0t} \toX(jw)=\sum_{k=-\infty}^{\infty}a_k\delta(w-kw_0)</script><h2 id="连续时间傅里叶变换的性质"><a href="#连续时间傅里叶变换的性质" class="headerlink" title="连续时间傅里叶变换的性质"></a>连续时间傅里叶变换的性质</h2><h3 id="线性性质"><a href="#线性性质" class="headerlink" title="线性性质"></a>线性性质</h3><h3 id="时移性质"><a href="#时移性质" class="headerlink" title="时移性质"></a>时移性质</h3><script type="math/tex; mode=display">x(t-t_0)\to e^{-jwt_0}X(jw)</script><ul><li>幅度不变</li><li>相位线性变化<script type="math/tex; mode=display">\phi'=\phi-wt_0</script><h3 id="频移性质"><a href="#频移性质" class="headerlink" title="频移性质"></a>频移性质</h3><script type="math/tex; mode=display">X(j(w-w_0))\to x(t)e^{jw_0t}</script><h3 id="共轭"><a href="#共轭" class="headerlink" title="共轭"></a>共轭</h3><script type="math/tex; mode=display">x^*(t)\to X^*(-jw)</script></li><li>x(t)为实值函数，X(jw)实部是偶函数，虚部是奇函数，幅度是偶函数，相位是奇函数</li><li>x(t)为实值偶函数，则X(jw)也是实值偶函数</li><li>x(t)为实值奇函数，则X(jw)也是纯虚奇函数</li></ul><h3 id="微分积分"><a href="#微分积分" class="headerlink" title="微分积分"></a>微分积分</h3><script type="math/tex; mode=display">\frac{dx(t)}{dt}\to jwX(jw)</script><script type="math/tex; mode=display">\int_{-\infty}^tx(\tau)d\tau\to \frac{1}{jw}+\pi X(0)\delta(w)</script><h3 id="尺度变换"><a href="#尺度变换" class="headerlink" title="尺度变换"></a>尺度变换</h3><script type="math/tex; mode=display">x(at) \to\frac{1}{|a|}X(j\frac{w}{a})</script><h3 id="对偶性"><a href="#对偶性" class="headerlink" title="对偶性"></a>对偶性</h3><script type="math/tex; mode=display">x(t)\to X(jw)</script><script type="math/tex; mode=display">X(t)\to 2\pi x(-w)</script><h3 id="帕斯瓦尔定理"><a href="#帕斯瓦尔定理" class="headerlink" title="帕斯瓦尔定理"></a>帕斯瓦尔定理</h3><script type="math/tex; mode=display">\int_{-\infty}^\infty|x(t)|^2dt=\frac{1}{2\pi}\int_{-\infty}^\infty|X(w)|^2dw</script><h4 id="周期信号"><a href="#周期信号" class="headerlink" title="周期信号"></a>周期信号</h4><script type="math/tex; mode=display">\frac{1}{T_0}\int_{T_0}|x(t)|^2dt=\sum_{k=-\infty}^\infty|a_k|^2</script><h3 id="时域卷积性质"><a href="#时域卷积性质" class="headerlink" title="时域卷积性质"></a>时域卷积性质</h3><script type="math/tex; mode=display">x(t)*h(t)\to X(jw)H(jw)</script><h3 id="调制性质"><a href="#调制性质" class="headerlink" title="调制性质"></a>调制性质</h3><script type="math/tex; mode=display">x(t)h(t)\to \frac{1}{2\pi}X(jw)*H(jw)</script><h1 id="LTI系统频域响应"><a href="#LTI系统频域响应" class="headerlink" title="LTI系统频域响应"></a>LTI系统频域响应</h1><p>LTI系统的频域响应定义为</p><script type="math/tex; mode=display">H(jw)=\frac{Y(jw)}{X(jw)}=|H(jw)|e^{j\theta(w)}</script><blockquote><p>一般说来，LTI系统的频域分析法仅适用于稳定系统。</p></blockquote><h2 id="LTI系统频域求解方法"><a href="#LTI系统频域求解方法" class="headerlink" title="LTI系统频域求解方法"></a>LTI系统频域求解方法</h2><ul><li>求H</li><li>频域相乘</li><li>求反变换</li></ul><h2 id="周期信号系统响应"><a href="#周期信号系统响应" class="headerlink" title="周期信号系统响应"></a>周期信号系统响应</h2><script type="math/tex; mode=display">x(t)=\sum_{k=-\infty}^\infty a_ke^{jkw_0t}</script><script type="math/tex; mode=display">y(t)=\sum_{k=-\infty}^\infty a_kH(jkw_0)e^{jkw_0t}</script>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手撕拉普拉斯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LTI系统时域分析</title>
      <link href="2020/08/18/LTI/"/>
      <url>2020/08/18/LTI/</url>
      
        <content type="html"><![CDATA[<h1 id="离散时间LTI系统"><a href="#离散时间LTI系统" class="headerlink" title="离散时间LTI系统"></a>离散时间LTI系统</h1><h2 id="离散信号单位脉冲分解"><a href="#离散信号单位脉冲分解" class="headerlink" title="离散信号单位脉冲分解"></a>离散信号单位脉冲分解</h2><p>对于任意离散信号x[n]，都可以利用移位脉冲的加权之和来表示</p><script type="math/tex; mode=display">x[n]=\sum_{k=-\infty}^{\infty}x[k]\delta[n-k]</script><span id="more"></span><h2 id="离散时间LTI系统的卷积和"><a href="#离散时间LTI系统的卷积和" class="headerlink" title="离散时间LTI系统的卷积和"></a>离散时间LTI系统的卷积和</h2><p>h[n]为系统对单位脉冲信号的响应</p><script type="math/tex; mode=display">x[n]=\sum_{k=-\infty}^{\infty}x[k]\delta[n-k]</script><script type="math/tex; mode=display">y[n]=\sum_{k=-\infty}^{\infty}x[k]h[n-k]=x[n]*h[n]</script><h3 id="卷积计算方法"><a href="#卷积计算方法" class="headerlink" title="卷积计算方法"></a>卷积计算方法</h3><ul><li>公式</li><li>列表法：适合小量数据</li><li>画图</li></ul><h2 id="卷积的性质"><a href="#卷积的性质" class="headerlink" title="卷积的性质"></a>卷积的性质</h2><ul><li>交换</li><li>结合</li><li>分配</li></ul><p><strong>x[n]</strong> 不为<strong>0</strong>的区间长度为<strong>m</strong>，<strong>h[n]</strong> 为 <strong>n</strong>，卷积后区间为<strong>m+n-1</strong></p><p><strong>x[n]</strong> 区间为 <strong>[m,n]</strong>,    <strong>h[n]</strong> 区间为 <strong>[q,p]</strong><br>卷积后区间为 <strong>[m+q,n+p]</strong></p><h3 id="特殊函数的卷积"><a href="#特殊函数的卷积" class="headerlink" title="特殊函数的卷积"></a>特殊函数的卷积</h3><script type="math/tex; mode=display">x[n]*\delta[n]=x[n]</script><script type="math/tex; mode=display">x[n]*\delta[n-n_0]=x[n-n_0]</script><script type="math/tex; mode=display">x[n]*u[n]=\sum_{k=-\infty}^{\infty}x[k]u[n-k]=\sum_{k=-\infty}^{n}x[k]</script><h1 id="连续时间LTI系统的卷积积分"><a href="#连续时间LTI系统的卷积积分" class="headerlink" title="连续时间LTI系统的卷积积分"></a>连续时间LTI系统的卷积积分</h1><p>任何连续信号也可以分解为脉冲信号，我们可以由此得到连续系统的卷积积分</p><script type="math/tex; mode=display">\int_{-\infty}^{\infty}x(\tau)h(t-\tau)d\tau=x(t)*h(t)</script><p>连续卷积的积分计算通常要分段积分</p><h2 id="连续卷积的性质"><a href="#连续卷积的性质" class="headerlink" title="连续卷积的性质"></a>连续卷积的性质</h2><p>交换结合分配与离散情况相同</p><ul><li>卷积微分</li></ul><script type="math/tex; mode=display">\frac{d}{dt}[x(t)*h(t)]</script><script type="math/tex; mode=display">=\frac{dx(t)}{dt}*h(t)</script><script type="math/tex; mode=display">=\frac{dh(t)}{dt}*x(t)</script><ul><li>卷积积分<script type="math/tex; mode=display">\int_{-\infty}^t[x(\lambda)*h(\lambda)]d\lambda</script><script type="math/tex; mode=display">=x(\lambda)*\int_{-\infty}^th(\lambda)d\lambda</script><script type="math/tex; mode=display">=h(\lambda)*\int_{-\infty}^tx(\lambda)d\lambda</script></li></ul><h3 id="积分与微分性质推广"><a href="#积分与微分性质推广" class="headerlink" title="积分与微分性质推广"></a>积分与微分性质推广</h3><script type="math/tex; mode=display">r^{(i)}(t)=x_1^{(j)}*x_2^{(i-j)}(t)</script><p>其中<strong>i, j, i-j</strong>取正数时为导数的阶次，负数时为重积分的次数</p><blockquote><p>离散域的差分和累加性质与之类似</p></blockquote><h2 id="与冲激阶跃函数卷积"><a href="#与冲激阶跃函数卷积" class="headerlink" title="与冲激阶跃函数卷积"></a>与冲激阶跃函数卷积</h2><script type="math/tex; mode=display">x(t)*\delta(t)=x(t)</script><script type="math/tex; mode=display">x(t)*\delta(t-t_0)=x(t-t_0)</script><script type="math/tex; mode=display">x(t)*\delta'(t)=x'(t)</script><script type="math/tex; mode=display">x(t)*u(t)=\int_{-\infty}^{t}x(\tau)d\tau</script><h1 id="LTI系统性质"><a href="#LTI系统性质" class="headerlink" title="LTI系统性质"></a>LTI系统性质</h1><h2 id="可逆性"><a href="#可逆性" class="headerlink" title="可逆性"></a>可逆性</h2><p>如果一个LTI系统是可逆，那么它就有一个LTI的逆系<br>统存在，原系统和其逆系统的级联为一恒等系统</p><h2 id="稳定性"><a href="#稳定性" class="headerlink" title="稳定性"></a>稳定性</h2><p>LTI系统稳定性充要条件为：</p><script type="math/tex; mode=display">\int_{-\infty}^{\infty}|h(t)|dt<\infty</script><script type="math/tex; mode=display">\sum_{n=-\infty}^{\infty}|h[n]|<\infty</script><h2 id="因果性"><a href="#因果性" class="headerlink" title="因果性"></a>因果性</h2><p>LTI系统因果性的充要条件</p><script type="math/tex; mode=display">h[n]=0,n<0</script><script type="math/tex; mode=display">h(t)=0,t<0</script><h2 id="LTI系统单位阶跃响应"><a href="#LTI系统单位阶跃响应" class="headerlink" title="LTI系统单位阶跃响应"></a>LTI系统单位阶跃响应</h2><ul><li>连续<script type="math/tex; mode=display">s(t)=u(t)*h(t)=\int_{-\infty}^t=h(\tau)d\tau</script></li><li>离散<script type="math/tex; mode=display">s[n]=u[n]*h[n]=\sum_{k=-\infty}^nh[k]</script></li><li>与单位冲激响应关系<script type="math/tex; mode=display">h(t)=\frac{ds(t)}{dt}</script><script type="math/tex; mode=display">h[n]=s[n]-s[n-1]</script></li></ul><h2 id="LTI系统的特征函数"><a href="#LTI系统的特征函数" class="headerlink" title="LTI系统的特征函数"></a>LTI系统的特征函数</h2><p>复指数信号的重要性在于它是LTI系统的特征函数，利用这样的性质和线性性质，将输入信号分解为指数信号的线性组合就可以化简</p><script type="math/tex; mode=display">y(t)=e^{st}*h(t)=e^{st}\int_{-\infty}^{\infty}h(\tau)e^{-s\tau}d\tau=H(s)e^{st}</script><script type="math/tex; mode=display">y[n]=z^n\sum_{k=-\infty}^{\infty}h[k]z^{-k}=H(z)z^n</script><h1 id="差分方程和微分方程"><a href="#差分方程和微分方程" class="headerlink" title="差分方程和微分方程"></a>差分方程和微分方程</h1><p>任意LTI系统的响应都可以利用微分和差分方程表示，比如LC电路</p><script type="math/tex; mode=display">\sum_{k=0}^na_k\frac{d^k}{dt^k}y(t)=\sum_{k=0}^mb_k\frac{d^k}{dt^k}x(t)</script><h2 id="微分方程解法"><a href="#微分方程解法" class="headerlink" title="微分方程解法"></a>微分方程解法</h2><p>微分方程的完全解由两部分组成</p><script type="math/tex; mode=display">y(t)=y_h(t)+y_p(t)</script><ul><li>求齐次解<ul><li>转化为特征方程</li><li>求特征根</li><li>确定齐次解样式</li></ul></li><li>求特解</li><li>根据起始条件确定参数</li></ul><h3 id="特征根对应函数形式"><a href="#特征根对应函数形式" class="headerlink" title="特征根对应函数形式"></a>特征根对应函数形式</h3><div class="table-container"><table><thead><tr><th>特征根$\lambda_i$</th><th>函数样式</th></tr></thead><tbody><tr><td>单实根</td><td>$C_ie^{\lambda_i(t)}$</td></tr><tr><td>k重实根</td><td>$(C<em>1t^{k-1}+C_2t^{k-2}……C</em>{k-1}t+C_k)e^{\lambda_i(t)}$</td></tr><tr><td>共轭复根$\alpha\pm j\beta$</td><td>$e^{\alpha t}(C_1\cos\beta t+C_2\sin\beta t)$ </td></tr><tr><td>k重共轭复根</td><td>$(C<em>1t^{k-1}+C_2t^{k-2}……C</em>{k-1}t+C_k)e^{\alpha t}(C_1\cos\beta t+C_2\sin\beta t)$ </td></tr></tbody></table></div><h3 id="常见激励函数对应特解"><a href="#常见激励函数对应特解" class="headerlink" title="常见激励函数对应特解"></a>常见激励函数对应特解</h3><div class="table-container"><table><thead><tr><th>函数</th><th>特解</th></tr></thead><tbody><tr><td>常数C</td><td>常数</td></tr><tr><td>$t^m$</td><td>$B<em>1t^m……+B_mt+B</em>{m+1}$特征根不等于0。$t^r(B<em>1t^m……+B_mt+B</em>{m+1})$，有r重等于0的实根</td></tr><tr><td>$e^{at}$</td><td>$Be^{at}$,a不等于特征根。 $B<em>1t^re^{at}……+B_rte^{at}+B</em>{r+1}e^{at}$,a=r重特征根</td></tr><tr><td>$\cos\beta t,\sin\beta t$</td><td>$B_1\cos\beta t+B_2\sin\beta t$</td></tr><tr><td>$t^me^{at}\cos t$</td><td>$(B<em>1t^m……+B_mt+B</em>{m+1})e^{at}\cos t+(D<em>1t^m……+D_mt+D</em>{m+1})e^{at}\sin t$</td></tr></tbody></table></div><h3 id="带奇异函数的微分方程——冲激函数匹配法"><a href="#带奇异函数的微分方程——冲激函数匹配法" class="headerlink" title="带奇异函数的微分方程——冲激函数匹配法"></a>带奇异函数的微分方程——冲激函数匹配法</h3><p>例子</p><script type="math/tex; mode=display">i''+4i'+3i=\delta'+3u</script><script type="math/tex; mode=display">求齐次解：\lambda=-3,-1</script><script type="math/tex; mode=display">i=(C_1e^{-3t}+C_2e^{-t})u(t)</script><script type="math/tex; mode=display">求特解，冲激函数匹配\delta '系数相同\to i=u(t)</script><script type="math/tex; mode=display">全解：i=(C_1e^{-3t}+C_2e^{-t}+1)u(t)</script><script type="math/tex; mode=display">i'=(C_1e^{-3t}+C_2e^{-t}+1)\delta(t)+(-3C_1e^{-3t}-C_2e^{-t})u(t)</script><script type="math/tex; mode=display">i''=(9C_1e^{-3t}+C_2e^{-t})u(t)+(-6C_1e^{-3t}-2C_2e^{-t})\delta(t)+(C_1e^{-3t}+C_2e^{-t}+1)\delta'(t)</script><script type="math/tex; mode=display">代入方程\begin{cases}-2C_1+2C_2+4=0\\C_1+C_2+1=1\end{cases}</script><script type="math/tex; mode=display">\to\begin{cases}C_1=1\\C_2=-1\end{cases}</script><script type="math/tex; mode=display">\to i=(e^{-3t}-e^{-t}+1)u(t)</script><h1 id="LTI系统响应分解"><a href="#LTI系统响应分解" class="headerlink" title="LTI系统响应分解"></a>LTI系统响应分解</h1><ul><li><strong>零输入响应</strong> ：不考虑外加信号, 即输入信号等于零<br>（x(t)=0 ）, 仅由系统的起始状态（y(0)_ ）所产生的响应。<br><strong>x(t),t&lt;0部分的响应</strong></li></ul><ul><li><strong>零状态响应</strong> ：不考虑系统的起始状态的作用, 即起始状态等于零（y(0)_ ） , 仅由系统的外加激励信号x(t)所产的响应。<br><strong>x(t),t&gt;0部分的响应</strong></li></ul><h2 id="LTI系统响应的求解方法"><a href="#LTI系统响应的求解方法" class="headerlink" title="LTI系统响应的求解方法"></a>LTI系统响应的求解方法</h2><h3 id="直接法"><a href="#直接法" class="headerlink" title="直接法"></a>直接法</h3><ul><li>根据定义求解两个微分方程再组合</li></ul><h3 id="单位冲激函数法"><a href="#单位冲激函数法" class="headerlink" title="单位冲激函数法"></a>单位冲激函数法</h3><ul><li>求解单位冲激响应</li><li>通过卷积求解两部分响应</li><li>利用线性性质进行组合计算</li></ul><h1 id="LTI框图"><a href="#LTI框图" class="headerlink" title="LTI框图"></a>LTI框图</h1><ul><li><p>连续<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/18/29dc7afdd210ea6b1801166d5c6310e4.png" alt=""></p></li><li><p>离散</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/18/ddbaf52bda3ffa522bedd003f03c857c.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手撕拉普拉斯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>系统的基本概念</title>
      <link href="2020/08/17/system-basic/"/>
      <url>2020/08/17/system-basic/</url>
      
        <content type="html"><![CDATA[<h1 id="信号的分类"><a href="#信号的分类" class="headerlink" title="信号的分类"></a>信号的分类</h1><ul><li>随机信号和确定信号</li><li>连续和离散</li><li>周期</li><li>奇偶<ul><li>所有信号可以被分解为奇偶信号   </li></ul></li><li>功率和能量</li></ul><span id="more"></span><p>能量信号（能量有限信号）：<br>如果信号x(t)满足: 0&lt;E &lt;$\infin$,而P = 0。</p><p>功率信号（功率有限信号）:<br>如果信号x(t)满足：0 &lt; P &lt;$\infin$, 而E=$\infin$。</p><blockquote><p>周期信号一般属于功率信号，属于能量信号的非周期信号也称为脉冲信号</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/17/74b0c8b8c1a8b64273a37443ae7599b6.png" alt=""></p></blockquote><h1 id="复指数信号与正弦信号"><a href="#复指数信号与正弦信号" class="headerlink" title="复指数信号与正弦信号"></a>复指数信号与正弦信号</h1><h2 id="连续时间复指数信号"><a href="#连续时间复指数信号" class="headerlink" title="连续时间复指数信号"></a>连续时间复指数信号</h2><script type="math/tex; mode=display">x(t)=ce^{st}s=\sigma+jwT_0=\frac{2\pi}{|w_0|}</script><h2 id="正弦与复指数信号转换"><a href="#正弦与复指数信号转换" class="headerlink" title="正弦与复指数信号转换"></a>正弦与复指数信号转换</h2><script type="math/tex; mode=display">\cos wt=\frac{1}{2}(e^{jwt}+e^{-jwt})</script><script type="math/tex; mode=display">\sin wt=\frac{1}{2j}(e^{jwt}-e^{-jwt})</script><script type="math/tex; mode=display">A\cos(wt+\phi)=\frac{A}{2}e^{j\phi}e^{jwt}+\frac{A}{2}e^{-j\phi}e^{-jwt}=ARe\{e^{j(wt+\phi)}\}</script><script type="math/tex; mode=display">A\sin(wt+\phi)=Aim\{e^{j(wt+\phi)}\}</script><h2 id="离散时间复指数信号"><a href="#离散时间复指数信号" class="headerlink" title="离散时间复指数信号"></a>离散时间复指数信号</h2><script type="math/tex; mode=display">x[n]=e^{jwn}=\cos wn+\sin wn</script><p>当$\frac{2\pi}{w}$为有理数数时为周期信号</p><h1 id="单位冲击和单位阶跃函数"><a href="#单位冲击和单位阶跃函数" class="headerlink" title="单位冲击和单位阶跃函数"></a>单位冲击和单位阶跃函数</h1><h2 id="离散脉冲阶跃"><a href="#离散脉冲阶跃" class="headerlink" title="离散脉冲阶跃"></a>离散脉冲阶跃</h2><h3 id="单位脉冲"><a href="#单位脉冲" class="headerlink" title="单位脉冲"></a>单位脉冲</h3><script type="math/tex; mode=display">\delta[n]=\begin{cases}0& n \not= 0 \\1& n=0\end{cases}</script><p><strong>采样特性</strong></p><script type="math/tex; mode=display">\delta[n]x[n]=x[0]\delta[n]</script><script type="math/tex; mode=display">x[n]\delta[n-n_0]=x[n_0]\delta[n-n_0]</script><h3 id="单位阶跃"><a href="#单位阶跃" class="headerlink" title="单位阶跃"></a>单位阶跃</h3><script type="math/tex; mode=display">u[n]=\begin{cases}0& n < 0 \\1& n>=0\end{cases}</script><p><strong>单位脉冲和阶跃关系</strong></p><script type="math/tex; mode=display">\delta[n]=u[n]-u[n-1]</script><script type="math/tex; mode=display">u[n]=\sum_{k=0}^{\infin}\delta[n-k]=\sum_{k=-\infin}^{n}\delta[k]</script><h3 id="矩形序列"><a href="#矩形序列" class="headerlink" title="矩形序列"></a>矩形序列</h3><script type="math/tex; mode=display">g_N[n]=\begin{cases}0& 0<n < N-1 \\1& other\end{cases}</script><script type="math/tex; mode=display">=u[n]-u[n-N]</script><h3 id="单位斜坡"><a href="#单位斜坡" class="headerlink" title="单位斜坡"></a>单位斜坡</h3><script type="math/tex; mode=display">r[n]=\begin{cases}n& n >= 0 \\0& n<0\end{cases}</script><script type="math/tex; mode=display">=nu[n]</script><h3 id="单位脉冲串"><a href="#单位脉冲串" class="headerlink" title="单位脉冲串"></a>单位脉冲串</h3><script type="math/tex; mode=display">\delta_N[n]=\sum_{k=-\infin}^{\infin}\delta[n-kN]</script><h2 id="连续时间阶跃冲激"><a href="#连续时间阶跃冲激" class="headerlink" title="连续时间阶跃冲激"></a>连续时间阶跃冲激</h2><h3 id="单位阶跃-1"><a href="#单位阶跃-1" class="headerlink" title="单位阶跃"></a>单位阶跃</h3><script type="math/tex; mode=display">u(t)=\begin{cases}0& t < 0 \\1& t>0\end{cases}</script><p>t=0处无定义</p><h3 id="矩形脉冲"><a href="#矩形脉冲" class="headerlink" title="矩形脉冲"></a>矩形脉冲</h3><script type="math/tex; mode=display">g(t)=u(t)-u(t-t_0)</script><h3 id="符号函数sgn"><a href="#符号函数sgn" class="headerlink" title="符号函数sgn"></a>符号函数sgn</h3><script type="math/tex; mode=display">sgn(t)=\begin{cases}1& t > 0 \\-1& t<0\end{cases}</script><script type="math/tex; mode=display">=u(t)-u(-t)</script><script type="math/tex; mode=display">=2u(t)-1</script><h3 id="单位冲激信号"><a href="#单位冲激信号" class="headerlink" title="单位冲激信号"></a>单位冲激信号</h3><p><strong>狄拉克定义</strong></p><script type="math/tex; mode=display">\begin{cases}\int_{-\infin}^{+\infin}\delta(t)dt=1\\\delta(t)=0 & t=\not0\end{cases}</script><script type="math/tex; mode=display">\delta(t)=\frac{du(t)}{dt}</script><script type="math/tex; mode=display">u(t)=\int_{-\infin}^{t}\delta(\tau)d\tau</script><h4 id="基本性质"><a href="#基本性质" class="headerlink" title="基本性质"></a>基本性质</h4><ul><li>抽样<script type="math/tex; mode=display">x(t)\delta(t)=x(0)\delta(t)</script><script type="math/tex; mode=display">x(t)\delta(t-t_0)=x(0)\delta(t-t_0)</script></li><li>筛选<script type="math/tex; mode=display">\int_{-\infin}^{+\infin}x(t)\delta(t)=x(0)</script><script type="math/tex; mode=display">\int_{-\infin}^{+\infin}x(t)\delta(t-t_0)=x(t_0)</script></li><li>偶函数</li></ul><h3 id="冲激偶信号"><a href="#冲激偶信号" class="headerlink" title="冲激偶信号"></a>冲激偶信号</h3><p>冲激函数的微分称为冲激偶信号</p><script type="math/tex; mode=display">\int_{-\infin}^{+\infin}\delta'(t)dt=0</script><script type="math/tex; mode=display">\int_{-\infin}^{+\infin}\delta'(t)x(t)dt=-x'(0)</script><h3 id="单位冲激串"><a href="#单位冲激串" class="headerlink" title="单位冲激串"></a>单位冲激串</h3><script type="math/tex; mode=display">\delta_T(t)=\sum_{n=-\infin}^{\infin}\delta(t-nT)</script><h3 id="Sa"><a href="#Sa" class="headerlink" title="Sa"></a>Sa</h3><script type="math/tex; mode=display">Sa(t)=\frac{\sin t}{t}</script><script type="math/tex; mode=display">\int_{-\infin}^{\infin}Sa(t)dt=\pi</script><h1 id="信号的运算和变换"><a href="#信号的运算和变换" class="headerlink" title="信号的运算和变换"></a>信号的运算和变换</h1><h2 id="运算"><a href="#运算" class="headerlink" title="运算"></a>运算</h2><ul><li>相加</li><li>相乘</li><li>微分积分<script type="math/tex; mode=display">x(t)=u(t-1)-u(t-2)</script><script type="math/tex; mode=display">\frac{dx(t)}{dt}=\delta(t-1)-\delta(t-2)</script></li></ul><blockquote><p>对于任意函数可以用u(t)进行分段表示，然后利用积分的变换进行计算</p></blockquote><h2 id="自变量变换"><a href="#自变量变换" class="headerlink" title="自变量变换"></a>自变量变换</h2><ul><li>平移，正左负右</li><li>反褶</li><li>尺度变化</li><li><p>抽取</p><script type="math/tex; mode=display">x_1=x[Nn]</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/17/9bd9b1dad2d3e0646bff6332d2e3862d.png" alt=""></p><blockquote><p>信号间隔不变，抽取部分信号</p></blockquote></li><li><p>内插</p><blockquote><p>在信号间插入0信号</p><script type="math/tex; mode=display">x_2[n]=\begin{cases}x[n/N]\\0\end{cases}</script></blockquote></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/17/10d0da71152d6e8de478c5c36d2979d7.png" alt=""></p><h1 id="系统基本性质"><a href="#系统基本性质" class="headerlink" title="系统基本性质"></a>系统基本性质</h1><p>连续系统可以用微分方程表示</p><p>离散系统可以用差分方程表示</p><h2 id="线性"><a href="#线性" class="headerlink" title="线性"></a>线性</h2><p>满足,离散同理</p><script type="math/tex; mode=display">ax_1(t)+bx_2(t)\to ay_1(t)+by_2(t)</script><h3 id="常见线性"><a href="#常见线性" class="headerlink" title="常见线性"></a>常见线性</h3><script type="math/tex; mode=display">y(t)=x(t)</script><script type="math/tex; mode=display">y(t)=tx(t)</script><script type="math/tex; mode=display">y(t)=dx(t)/dt</script><script type="math/tex; mode=display">y[t]=x[n+1]-x[n]</script><h3 id="常见非线性"><a href="#常见非线性" class="headerlink" title="常见非线性"></a>常见非线性</h3><script type="math/tex; mode=display">y(t)=x^2(t)</script><script type="math/tex; mode=display">y(t)=e^{x(t)}</script><script type="math/tex; mode=display">y(t)=2x(t)+3</script><h3 id="判断方法"><a href="#判断方法" class="headerlink" title="判断方法"></a>判断方法</h3><ul><li>直接利用定义</li><li>与x括号内参数无关，x(t)需要为一次且x(t)外无常数项</li></ul><h2 id="时变，时不变"><a href="#时变，时不变" class="headerlink" title="时变，时不变"></a>时变，时不变</h2><script type="math/tex; mode=display">x[n]\to y[n]</script><script type="math/tex; mode=display">x[n-n_0]\to y[n-n_0]</script><script type="math/tex; mode=display">x(t)\to y(t)</script><script type="math/tex; mode=display">x(t-t_0)\to y(t-t_0)</script><h3 id="判断方法-1"><a href="#判断方法-1" class="headerlink" title="判断方法"></a>判断方法</h3><ul><li>定义</li><li>t只在x括号内且为一次</li></ul><script type="math/tex; mode=display">y=x(sin(t))</script><script type="math/tex; mode=display">y_1(t)=x_1(sin(t))</script><script type="math/tex; mode=display">x_2(t)=x_1(t-t_0)=x_1(sin(t-t_0))</script><script type="math/tex; mode=display">y_2(t)=x_2(sin(t))=x_1(sin(sin(t)-t_0))</script><script type="math/tex; mode=display">\not =y_1(t-t_0)=x_1(sin(t-t_0))</script><h3 id="记忆和无记忆"><a href="#记忆和无记忆" class="headerlink" title="记忆和无记忆"></a>记忆和无记忆</h3><p>输入仅取决于当前输出为无机盐</p><p>输入取决于其他时间为记忆<br>比如累加器，延迟单元</p><h3 id="因果"><a href="#因果" class="headerlink" title="因果"></a>因果</h3><p>系统输出不取决于未来</p><p>非因果系统如</p><script type="math/tex; mode=display">y(t)=x(t+2)</script><p>在图像处理中就会有非因果系统</p><p>通常把0时刻开始的信号称为因果信号</p><script type="math/tex; mode=display">x(t)=0 ,t<0</script><h2 id="可逆"><a href="#可逆" class="headerlink" title="可逆"></a>可逆</h2><p>不可逆</p><script type="math/tex; mode=display">y(t)=x^2(t)</script><p>可逆</p><script type="math/tex; mode=display">y[n]=\sum x[k]</script><h2 id="稳定"><a href="#稳定" class="headerlink" title="稳定"></a>稳定</h2><p>输入有界则输出有界</p>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手撕拉普拉斯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>com base</title>
      <link href="2020/08/15/com-base/"/>
      <url>2020/08/15/com-base/</url>
      
        <content type="html"><![CDATA[<p>最近科研碰上大量通讯相关的背景，因此对通讯的一些基本背景知识做简单整理</p><span id="more"></span><h2 id="词汇-amp-简称"><a href="#词汇-amp-简称" class="headerlink" title="词汇&amp;简称"></a>词汇&amp;简称</h2><ul><li>UE：user equipment</li><li>BS: base station</li><li>Tx: transmitter beam</li><li>Rx: receive beam</li><li>DFT:离散傅里叶变换</li><li>LOS:视距</li><li>NLOS:非视距</li><li>scattering:散射分量</li><li>constant：直射分量 </li><li>AOA：接收角</li><li>AOD：发射角</li></ul><h2 id="SNR-Signal-to-noise-信噪比"><a href="#SNR-Signal-to-noise-信噪比" class="headerlink" title="SNR:Signal-to-noise,信噪比"></a>SNR:Signal-to-noise,信噪比</h2><p><strong>公式</strong></p><script type="math/tex; mode=display">SNR=\frac{Signal Power}{Noise Power}=\frac{E[|x(t)|^2]}{E[|v(t)|^2]}=\frac{E_s}{N_0}</script><script type="math/tex; mode=display">SNR_{db}=10log_{10}SNR</script><h2 id="香农公式"><a href="#香农公式" class="headerlink" title="香农公式"></a>香农公式</h2><p>香农公式定义了再高斯白噪声的信道中，信息传输的最大速率</p><script type="math/tex; mode=display">C=W\log_2(1+S/N)</script><script type="math/tex; mode=display">S/N=SNR_{db}</script><h2 id="信号离散傅里叶变化"><a href="#信号离散傅里叶变化" class="headerlink" title="信号离散傅里叶变化"></a>信号离散傅里叶变化</h2><script type="math/tex; mode=display">\sqrt{(1/N_T)}*e^{(-i2\pi(-1/2+(slot_{ind}-1)/N_T)*[0:N_T-1]^T)}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/15/6b7a4e44022c8563e56d40530707232f.png" alt=""></p><h2 id="信道类型：channel-type"><a href="#信道类型：channel-type" class="headerlink" title="信道类型：channel type"></a>信道类型：channel type</h2><h3 id="los：视距"><a href="#los：视距" class="headerlink" title="los：视距"></a>los：视距</h3><script type="math/tex; mode=display">K_{fdB} = 13.2; % Rician K factor</script><script type="math/tex; mode=display">K_f = 10^{(K_{fdB}/10)}</script><script type="math/tex; mode=display">NumPath = 1</script><script type="math/tex; mode=display">\gamma_k = rand(NumPath)^2\times 10^{-0.2randn(NumPath)}</script><script type="math/tex; mode=display">\gamma_k = \gamma_k/\sum(\gamma_k)</script><script type="math/tex; mode=display">power_{const} = \frac{K_f}{K_f+1}</script><script type="math/tex; mode=display">power_{scatter} = \frac{1}{K_f+1}</script><script type="math/tex; mode=display">g_{constant} = \sqrt{\gamma_k*power_{const}}</script><script type="math/tex; mode=display">g_{scattering} = \sqrt{\gamma_k*power_{scatter}}</script><h3 id="Nlos：非视距"><a href="#Nlos：非视距" class="headerlink" title="Nlos：非视距"></a>Nlos：非视距</h3><script type="math/tex; mode=display">K_{fdB} = 6; % Rician K factor</script><script type="math/tex; mode=display">K_f = 10^{(K_{fdB}/10)}</script><script type="math/tex; mode=display">NumPath = 1.8</script><script type="math/tex; mode=display">\gamma_k = rand(NumPath)^2\times 10^{-0.2randn(NumPath)}</script><script type="math/tex; mode=display">\gamma_k = \gamma_k/\sum(\gamma_k)</script><script type="math/tex; mode=display">power_{const} = \frac{K_f}{K_f+1}</script><script type="math/tex; mode=display">power_{scatter} = \frac{1}{K_f+1}</script><script type="math/tex; mode=display">g_{constant} = \sqrt{\gamma_k*power_{const}}</script><script type="math/tex; mode=display">g_{scattering} = \sqrt{\gamma_k*power_{scatter}}</script><h2 id="信道矩阵"><a href="#信道矩阵" class="headerlink" title="信道矩阵"></a>信道矩阵</h2><script type="math/tex; mode=display">v = e^{-i2\pi k \sin(AOD)}</script><script type="math/tex; mode=display">u = e^{-i2\pi k \sin(AOA)}</script><script type="math/tex; mode=display">H = H+g_{constant}uv+ \frac{g_{scattering}}{\sqrt{2}}N+i*N;</script><script type="math/tex; mode=display">N\to N(0,\sigma^2)</script><h2 id="信道衰减模型"><a href="#信道衰减模型" class="headerlink" title="信道衰减模型"></a>信道衰减模型</h2><h3 id="什么是信道衰落"><a href="#什么是信道衰落" class="headerlink" title="什么是信道衰落"></a>什么是信道衰落</h3><p>对于S—D这样一个发送接收系统来说，理想的无线信号传播（自由空间传播模型）是由S发送的电磁信号经过一定的衰减（attenuation）到达D点，我们可以理解为信号沿着S—D的直线。虽然这样，电磁波实际上是以球面波的形式向周围360度辐射，但其是只有沿着S—D直线传播的信号才能抵达D点，我们也可以把S—D路径称为直射路径。这是对于自由空间来说的，在自由空间模型里面除了S和D什么也没有。</p><p>而对于实际的大气传播环境，大气中包含着许多的小颗粒（悬浮物），或者小粒子。从S出发，沿着非S—D方向的其它传播方向的额电磁波可能经过一系列的反射（散射）后而抵达接收端D，我们把这种路径称为散射路径。由于每一条散射路径经历的路程都不一样，这样，他们抵达接收端的相位各不相同，导致总的信号强度变低。这样我们把由于信号经过了多个路径而抵达接收端导致信号强度发生随机变化的现象称为衰落（fading）。</p><h3 id="瑞利衰落信道模型-（Rayleigh）"><a href="#瑞利衰落信道模型-（Rayleigh）" class="headerlink" title="瑞利衰落信道模型 （Rayleigh）"></a>瑞利衰落信道模型 （Rayleigh）</h3><p>假设发送信号为单一频率正弦波，即</p><script type="math/tex; mode=display">s(t)=A\cos w_c t</script><p>若不考虑直射路径，多径信道共有n条路径，各条路径具有时变衰耗和时变传输时延，且从各条路径到达接收端的信号相互独立，则接收端接受到的合成波为</p><script type="math/tex; mode=display">r(t)=a_1\cos w_c[t-\tau_1(t)]+a_2\cos w_c[t-\tau_2(t)]+a_3\cos w_c[t-\tau_3(t)]……a_n\cos w_c[t-\tau_n(t)]</script><script type="math/tex; mode=display">=\sum_{i=1}^{n}a_i\cos w_c[t-\tau_i(t)]</script><p>r(t)也可表示为如下形式：</p><script type="math/tex; mode=display">r(t)=\sum_{i=1}^{n}a_i\cos\phi_i\cos- \sum_{i=1}^{n}a_i\sin\phi_i\sin</script><script type="math/tex; mode=display">=X(t)\cos w_ct-Y(t)\sin w_ct</script><p>由于X(t)和Y(t)都是相互独立的随机变量之后，根据中心极限定理，大量独立随机变量之和的分布趋于正态分布。因此，当n足够大时，X(t)和Y(t)都趋于正态分布。通常情况下X(t)和Y(t)的均值为0（由于没有直射路径），方差相等。这种表示方式也叫做同相-正交表示法。</p><p><strong>由于包络服从瑞利分布，故其称为瑞利信道模型。</strong></p><p>对于陆地移动信道、短波电离层反射等随参信道，其路径幅度ai(t)和相位函数φi(t)随时间变化与发射信号载波频率相比要缓慢得多。因此，相对于载波来说V(t)和φ(t)是慢变化随机过程，于是r(t)可以看成是一个窄带随机过程。由此得出以下两个结论：</p><ol><li>多径传播使单一的正弦信号变成了包络和相位受调制的窄带信号，这种信号称为衰落信号，即多径传播使信号产生瑞利型衰落（多径衰落）。</li></ol><p>2.从频谱上看，多径传播使单一谱线变成了窄带频谱，即多径传播引起了频率弥散。</p><h3 id="莱斯衰落信道模型（Rician）"><a href="#莱斯衰落信道模型（Rician）" class="headerlink" title="莱斯衰落信道模型（Rician）"></a>莱斯衰落信道模型（Rician）</h3><p>莱斯信道是当移动台与基站间存在直射波信号时，即有一条主路径，通过主路径传输过来被接收的信号为一个稳定幅度$Ak$和相位$\phi _k$，其余多径传输过来的信号仍如“瑞利衰落概率模型”所述。</p><p>当信道中存在一个固定的直射分量时，X(t)与Y(t)的均值不再为0。其接收信号是复高斯信号和直射分量的叠加（即正弦波加窄带高斯过程），其包络的概率密度函数服从莱斯分布，即下式</p><script type="math/tex; mode=display">f(z)=\frac{z}{\sigma_n^2}exp[-\frac{1}{2\sigma^2_n}(z^2+A^2)]I_0(\frac{A_z}{\sigma^2_n}),z>0</script><h4 id="莱斯衰弱信道模型matlab仿真程序"><a href="#莱斯衰弱信道模型matlab仿真程序" class="headerlink" title="莱斯衰弱信道模型matlab仿真程序"></a>莱斯衰弱信道模型matlab仿真程序</h4><pre><code>%% 参数设置Modulation_Order=2;Packet_Length=16;SymbolRate=1000;SamplesPerSymbol=8;%每个符号的样点数df=400;%FSK调制相邻载波频率间隔，单位Hzfs=SamplesPerSymbol*SymbolRate;SNR=10;ts=1/fs;fd=0;k=3;tau=[0 1 2]*ts;%每条径的时延pdb=[2 1 0];%每条径的增益%% 调制器、解调器、莱斯信道设置H_Mod = comm.FSKModulator(&#39;ModulationOrder&#39;,Modulation_Order,&#39;SymbolRate&#39;,1000 ,&#39;SamplesPerSymbol&#39;,8, &#39;FrequencySeparation&#39;,df); %默认连续相位、整数输入 H_Demod = comm.FSKDemodulator(&#39;ModulationOrder&#39;,Modulation_Order,&#39;SymbolRate&#39;,1000 ,&#39;SamplesPerSymbol&#39;,8, &#39;FrequencySeparation&#39;,df);chan_Rici = ricianchan(ts,fd,k,tau,pdb);</code></pre><h3 id="Nakagami-m信道衰落模型"><a href="#Nakagami-m信道衰落模型" class="headerlink" title="Nakagami-m信道衰落模型"></a>Nakagami-m信道衰落模型</h3><p>瑞利和莱斯分布与实验数据有时不太吻合，因此人们提出了能吻合更多实验数据的一种更通用的信道衰落分布，就是Nakagami-m衰落，</p>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> communication </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>建立你的知识神经网络</title>
      <link href="2020/08/08/nn/"/>
      <url>2020/08/08/nn/</url>
      
        <content type="html"><![CDATA[<p>早在6月份学习深度学习的时候就有想在自己的博客中加入一张神经网络图记录学习的知识体系，一直拖到现在（主要是看门狗2太好玩了，不是）。这篇文章将详细的介绍如何在hexo博客中加入一张神经网络图。</p><span id="more"></span><h1 id="在博客中加入新页面"><a href="#在博客中加入新页面" class="headerlink" title="在博客中加入新页面"></a>在博客中加入新页面</h1><p>在博客中加入新页面的方式有两种，一种是直接创建html文件，自己做一个网页，令一种是新建一个md格式的page，，最后考虑到博客的整体性，我选择了后者，下面是两种方法的具体过程</p><h2 id="加入新html子页面"><a href="#加入新html子页面" class="headerlink" title="加入新html子页面"></a>加入新html子页面</h2><ul><li>首先，在博客根目录的source文件夹下，新建文件夹用于存放HTML文件</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/05/a19e058aacfc62f785cfd69176e78296.png" alt=""></p><p>hexo会对source文件夹中所有的页面进行渲染，所以我们自己的HTML文件需要跳过渲染。</p><ul><li><p>在博客根目录的配置文件_config.yml文件里，跳过渲染</p><h1 id="跳过文件夹下所有文件"><a href="#跳过文件夹下所有文件" class="headerlink" title="跳过文件夹下所有文件"></a>跳过文件夹下所有文件</h1><p>  skip_render: </p><pre><code>- &quot;文件夹名/*&quot;  </code></pre><h2 id="新建一个page"><a href="#新建一个page" class="headerlink" title="新建一个page"></a>新建一个page</h2></li></ul><p>cmd+win调出命令行，在博客对应目录输入</p><pre><code>hexo new page &quot;页面名称&quot;</code></pre><p>hexo 会在source文件夹下创建一个对应的文件夹，里面有一个<code>index.md</code>的文件，在配置文件<code>_config.yml</code>中的<code>menu</code>下加入<code>/创建页面名称</code>，这样就创建好了新页面。</p><pre><code># Header-菜单menu:Home: /Archives:  /archives/index.htmlCategories : /categoriesKnowledge nn : /nn</code></pre><h1 id="Echart框架下载"><a href="#Echart框架下载" class="headerlink" title="Echart框架下载"></a>Echart框架下载</h1><p>第二步我们需要建立我们的神经图，这里我选择的JS的图表框架<a href="https://echarts.apache.org/zh/index.html">Echart</a>，一个功能非常强大的框架，图表非常的美观<br>在这里我们总共需要两个js文件</p><ul><li>echarts.min.js 主要的js文件，echart的压缩版本<ul><li>因为我们只用到关系图，所以也可以在官网通过定制只下载关系图的代码</li></ul></li><li>jquery.js 当要用jquery调用json文件时加入</li></ul><p><strong>下载地址</strong></p><p><strong>echarts.min.js</strong>：<a href="https://echarts.apache.org/zh/download.html">https://echarts.apache.org/zh/download.html</a></p><p><strong>jquery.js</strong>：<a href="http://blog.jquery.com/">http://blog.jquery.com/</a></p><p>也可以访问我的github直接下载所有源码文件<br><a href="https://github.com/Fazziekey/Echart-nn">https://github.com/Fazziekey/Echart-nn</a></p><p>下载完成后将这两个文件放在Hexo\themes\<theme~name>\source\js下  </p><h1 id="Echart制作神经网络"><a href="#Echart制作神经网络" class="headerlink" title="Echart制作神经网络"></a>Echart制作神经网络</h1><p>接下来在我们刚刚创建的md文件中直接插入下面的代码，我们需要给这个关系图提供一个容器</p><pre><code>&lt;div id=&quot;main&quot; style=&quot;width:100%;height:500px&quot;&gt;&lt;/div&gt; &lt;!--宽度推荐设置为100%占据整个网页--&gt;&lt;script src=&quot;echarts.min.js&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot;src=&quot;/js/jquery.js&quot;&gt;&lt;/script&gt;&lt;!--如果不调用外部json文件可以删去--&gt;&lt;script type=&quot;text/javascript&quot;&gt;</code></pre><p>下面是如何配置神经网络图</p><hr><pre><code>&lt;div id=&quot;main&quot; style=&quot;width:1000px;height:800px&quot;&gt;&lt;/div&gt;&lt;script type=&quot;text/javascript&quot;&gt;    var myChart = echarts.init(document.getElementById(&#39;main&#39;));    var categories = [];   //设置图例标签        categories[0] = &#123;            name: &quot;Brain Center&quot;        &#125;;        categories[1] = &#123;            name: &quot;工程&quot;        &#125;;        categories[2] = &#123;            name: &quot;艺术&quot;        &#125;;        categories[3] = &#123;            name: &quot;数理&quot;        &#125;;        categories[4] = &#123;            name: &quot;逻辑&quot;        &#125;;        categories[5] = &#123;            name: &quot;人文&quot;        &#125;;        categories[6] = &#123;            name: &quot;社科&quot;        &#125;;        categories[7] = &#123;            name: &quot;FCL&quot;        &#125;;    option = &#123;        // 图的标题        title: &#123;            text: &#39;Learning Neural Network&#39;        &#125;,        // 提示框的配置        tooltip: &#123;            formatter: function (x) &#123;                return x.data.des;            &#125;        &#125;,        // 工具箱        toolbox: &#123;            // 显示工具箱            show: true,            feature: &#123;                mark: &#123;                    show: true                &#125;,                // 还原                restore: &#123;                    show: true                &#125;,                // 保存为图片                saveAsImage: &#123;                    show: true                &#125;            &#125;        &#125;,        legend: [&#123;            // selectedMode: &#39;single&#39;,            data: categories.map(function (a) &#123;                return a.name;            &#125;)        &#125;],        series: [&#123;            type: &#39;graph&#39;, // 类型:关系图            layout: &#39;force&#39;, //图的布局，类型为力导图            symbolSize: 40, // 调整节点的大小            roam: true, // 是否开启鼠标缩放和平移漫游。默认不开启。如果只想要开启缩放或者平移,可以设置成 &#39;scale&#39; 或者 &#39;move&#39;。设置成 true 为都开启            focusNodeAdjacency: true,   // 是否在鼠标移到节点上的时候突出显示节点以及节点的边和邻接节点。[ default: false ]                    force: &#123;                // 力引导布局相关的配置项，力引导布局是模拟弹簧电荷模型在每两个节点之间添加一个斥力，每条边的两个节点之间添加一个引力，每次迭代节点会在各个斥力和引力的作用下移动位置，多次迭代后节点会静止在一个受力平衡的位置，达到整个模型的能量最小化。                                    // 力引导布局的结果有良好的对称性和局部聚合性，也比较美观。                repulsion: 1000,            // [ default: 50 ]节点之间的斥力因子(关系对象之间的距离)。支持设置成数组表达斥力的范围，此时不同大小的值会线性映射到不同的斥力。值越大则斥力越大                edgeLength: [150, 100]      // [ default: 30 ]边的两个节点之间的距离(关系对象连接线两端对象的距离,会根据关系对象值得大小来判断距离的大小)，                                            // 这个距离也会受 repulsion。支持设置成数组表达边长的范围，此时不同大小的值会线性映射到不同的长度。值越小则长度越长。如下示例:                                            // 值最大的边长度会趋向于 10，值最小的边长度会趋向于 50      edgeLength: [10, 50]            &#125;,            edgeSymbol: [&#39;circle&#39;, &#39;arrow&#39;],            edgeSymbolSize: [2, 10],            edgeLabel: &#123;                normal: &#123;                    textStyle: &#123;                        fontSize: 20                    &#125;                &#125;            &#125;,            force: &#123;                repulsion: 2500,                edgeLength: [10, 50]            &#125;,            draggable: true,            lineStyle: &#123;                normal: &#123;                    width: 2,               //线条宽度                    color: &#39;#4b565b&#39;,       //线条颜色                    type: &#39;solid&#39;,          // 线的类型[ default: solid ]   &#39;dashed&#39;    &#39;dotted&#39;                    opacity: 0.5,           // 图形透明度。支持从 0 到 1 的数字，为 0 时不绘制该图形。[ default: 0.5 ]                    curveness: 0.3          // 边的曲度，支持从 0 到 1 的值，值越大曲度越大。[ default: 0 ]                &#125;            &#125;,            edgeLabel: &#123;                    // 连接两个关系对象的线上的标签                normal: &#123;                    show: true,                    formatter: function (x) &#123;                        return x.data.name;                    &#125;                &#125;            &#125;,            label: &#123;                        //对象标签                normal: &#123;                    show: true,                    textStyle: &#123;&#125;                &#125;            &#125;,            // 数据            data: [&#123;                name: &#39;Brain Center&#39;,                des: &#39;nodedes01&#39;,                symbolSize: 100,                category: 0,            &#125;, &#123;                name: &#39;程序语言&#39;,                des: &#39;nodedes02&#39;,                symbolSize: 80,                category: 1,            &#125;, &#123;                name: &#39;音乐&#39;,                des: &#39;nodedes3&#39;,                symbolSize: 80,                category: 2,            &#125;, &#123;                name: &#39;课程&#39;,                des: &#39;nodedes04&#39;,                symbolSize: 80,                category: 7,            &#125;, &#123;                name: &#39;书籍&#39;,                des: &#39;nodedes05&#39;,                symbolSize: 80,                category: 7,            &#125;            ],            //节点关系            links: [&#123;                source: &#39;程序语言&#39;,                target: &#39;Brain Center&#39;,                name: &#39;Softmax&#39;,                des: &#39;link01des&#39;            &#125;, &#123;                source: &#39;音乐&#39;,                target: &#39;Brain Center&#39;,                name: &#39;Softmax&#39;,                des: &#39;link02des&#39;            &#125;, &#123;                source: &#39;课程&#39;,                target: &#39;Brain Center&#39;,                name: &#39;Softmax&#39;,                des: &#39;link03des&#39;            &#125;, &#123;                source: &#39;书籍&#39;,                target: &#39;Brain Center&#39;,                name: &#39;Softmax&#39;,                des: &#39;link05des&#39;            &#125;],            categories: categories,        &#125;]    &#125;;    myChart.setOption(option);&lt;/script&gt;&lt;/body&gt;</code></pre><p>&lt;/html&gt;</p><h2 id="最终效果"><a href="#最终效果" class="headerlink" title="最终效果"></a>最终效果</h2><p>也可以直接访问我侧边栏的<strong>Knowledge nn</strong></p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/08/af24bcbd2f1e99a78dca8b7228e0c9fc.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 《栈溢出工程师》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JavaScript </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Beautiful Soup</title>
      <link href="2020/08/06/%E7%88%AC%E8%99%ABplus/"/>
      <url>2020/08/06/%E7%88%AC%E8%99%ABplus/</url>
      
        <content type="html"><![CDATA[<p>在使用正则化匹配时无法去除标签，所以使用BeautifulSoup，BeautifulSoup可以很好的第三方用于解析网站，查找信息的包<br><span id="more"></span></p><h1 id="Beautiful-soup-basic"><a href="#Beautiful-soup-basic" class="headerlink" title="Beautiful soup basic"></a>Beautiful soup basic</h1><h2 id="安装Beautifulsoup"><a href="#安装Beautifulsoup" class="headerlink" title="安装Beautifulsoup"></a>安装Beautifulsoup</h2><p>命令行或Anaconda Promote cd到<code>../Lib/site-package</code></p><pre><code>pip install beautifulSoup4</code></pre><h2 id="基本使用方法"><a href="#基本使用方法" class="headerlink" title="基本使用方法"></a>基本使用方法</h2><h3 id="找到标签"><a href="#找到标签" class="headerlink" title="找到标签"></a>找到标签</h3><pre><code>from bs4 import BeautifulSoupsoup = BeautifulSoup(html, features=&#39;lxml&#39;)print(soup.h1) #打印&lt;h1&gt;标签内容</code></pre><h3 id="查找链接"><a href="#查找链接" class="headerlink" title="查找链接"></a>查找链接</h3><pre><code>all_href = soup.find_all(&#39;a&#39;)all_href = [l[&#39;href&#39;] for l in all_href]print(&#39;\n&#39;, all_href)</code></pre><h2 id="根据CSS查找"><a href="#根据CSS查找" class="headerlink" title="根据CSS查找"></a>根据CSS查找</h2><p>CSS定义了class，可以根据class来查找想要的信息，对于以下html代码</p><pre><code>&lt;head&gt;...&lt;style&gt;.jan &#123;    background-color: yellow;&#125;....month &#123;    color: red;&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;...&lt;ul&gt;    &lt;li class=&quot;month&quot;&gt;一月&lt;/li&gt;    &lt;ul class=&quot;jan&quot;&gt;        &lt;li&gt;一月一号&lt;/li&gt;        &lt;li&gt;一月二号&lt;/li&gt;        &lt;li&gt;一月三号&lt;/li&gt;    &lt;/ul&gt;    ...&lt;/ul&gt;&lt;/body&gt;</code></pre><p>  在python中查找</p><pre><code>    soup = BeautifulSoup(html, features=&#39;lxml&#39;)# use class to narrow searchmonth = soup.find_all(&#39;li&#39;, &#123;&quot;class&quot;: &quot;month&quot;&#125;)for m in month:    print(m.get_text())&quot;&quot;&quot;一月二月三月四月五月&quot;&quot;&quot;    jan = soup.find(&#39;ul&#39;, &#123;&quot;class&quot;: &#39;jan&#39;&#125;)d_jan = jan.find_all(&#39;li&#39;)              # use jan as a parentfor d in d_jan:    print(d.get_text())&quot;&quot;&quot;一月一号一月二号一月三号&quot;&quot;&quot;</code></pre><h2 id="BeautifulSoup使用正则表达式"><a href="#BeautifulSoup使用正则表达式" class="headerlink" title="BeautifulSoup使用正则表达式"></a>BeautifulSoup使用正则表达式</h2><h3 id="利用正则表达选取图片"><a href="#利用正则表达选取图片" class="headerlink" title="利用正则表达选取图片"></a>利用正则表达选取图片</h3><p>在网页中的图片都在这样一个<strong>tag</strong>中</p><pre><code>&lt;td&gt;    &lt;img src=&quot;https://.../.jpg&quot;&gt;&lt;/td&gt;</code></pre><p>所以, 我们可以用 soup 将这些 <img> tag 全部找出来, 但是每一个 img 的链接(src)都可能不同. 或者每一个图片有的可能是 jpg 有的是 png, 如果我们只想挑选 jpg 形式的图片, 我们就可以用这样一个正则 r’.*?.jpg’ 来选取. 把正则的 compile 形式放到 BeautifulSoup 的功能中, 就能选到符合要求的图片链接了.</p><pre><code>soup = BeautifulSoup(html, features=&#39;lxml&#39;)img_links = soup.find_all(&quot;img&quot;, &#123;&quot;src&quot;: re.compile(&#39;.*?\.jpg&#39;)&#125;)for link in img_links:    print(link[&#39;src&#39;])</code></pre><p>同样的，查找链接</p><pre><code>links = soup.find_all(&#39;a&#39;, &#123;&#39;href&#39;: re.compile(&#39;https://morvan.*&#39;)&#125;)for link in course_links:    print(link[&#39;href&#39;])</code></pre><h1 id="网络爬虫"><a href="#网络爬虫" class="headerlink" title="网络爬虫"></a>网络爬虫</h1><p>爬虫会在网络中按照一定规律移动</p><p>在百度百科的爬虫词条，<code>&lt;a href=https://baike.baidu.com/item/网络爬虫/5162711target=&#39;_blank&#39; &gt;</code>页面中所有的链接都以item开头，但不是所有词条，所有需要进一步筛选</p><pre><code>&lt;a target=&quot;_blank&quot; href=&quot;/item/%E8%9C%98%E8%9B%9B/8135707&quot; data-lemmaid=&quot;8135707&quot;&gt;蜘蛛&lt;/a&gt;&lt;a target=&quot;_blank&quot; href=&quot;/item/%E8%A0%95%E8%99%AB&quot;&gt;蠕虫&lt;/a&gt;&lt;a target=&quot;_blank&quot; href=&quot;/item/%E9%80%9A%E7%94%A8%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E&quot;&gt;通用搜索引擎&lt;/a&gt;</code></pre><h2 id="制作爬虫"><a href="#制作爬虫" class="headerlink" title="制作爬虫"></a>制作爬虫</h2><p>导入一些模块, 设置起始页. 并将 /item/… 的网页都放在 his 中, 做一个备案, 记录我们浏览过的网页.</p><pre><code>from bs4 import BeautifulSoupfrom urllib.request import urlopenimport reimport randombase_url = &quot;https://baike.baidu.com&quot;his = [&quot;/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711&quot;]</code></pre><p>接着我们先不用循环, 对一个网页进行处理, 走一遍流程, 然后加上循环, 让我们的爬虫能在很多网页中爬取. 下面做的事情, 是为了在屏幕上打印出来我们现在正在哪张网页上, 网页的名字叫什么.</p><pre><code>url = base_url + his[-1]html = urlopen(url).read().decode(&#39;utf-8&#39;)soup = BeautifulSoup(html, features=&#39;lxml&#39;)print(soup.find(&#39;h1&#39;).get_text(), &#39;    url: &#39;, his[-1])# 网络爬虫     url:  /item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711</code></pre><p>接下来我们开始在这个网页上找所有符合要求的 /item/ 网址. 使用一个正则表达式(正则教程) 过滤掉不想要的网址形式. 这样我们找到的网址都是 <code>/item/%xx%xx%xx...</code> 这样的格式了. 之后我们在这些过滤后的网页中随机选一个, 当做下一个要爬的网页. 不过有时候很不幸, 在 sub_urls 中并不能找到合适的网页, 我们就往回跳一个网页, 回到之前的网页中再随机抽一个网页做同样的事.</p><pre><code># find valid urlssub_urls = soup.find_all(&quot;a&quot;, &#123;&quot;target&quot;: &quot;_blank&quot;, &quot;href&quot;: re.compile(&quot;/item/(%.&#123;2&#125;)+$&quot;)&#125;)if len(sub_urls) != 0:    his.append(random.sample(sub_urls, 1)[0][&#39;href&#39;])else:    # no valid sub link found    his.pop()print(his)# [&#39;/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711&#39;, &#39;/item/%E4%B8%8B%E8%BD%BD%E8%80%85&#39;]</code></pre><p>有了这套体系, 我们就能把它放在一个 for loop 中, 让它在各种不同的网页中跳来跳去.</p><pre><code>his = [&quot;/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711&quot;]for i in range(20):    url = base_url + his[-1]html = urlopen(url).read().decode(&#39;utf-8&#39;)soup = BeautifulSoup(html, features=&#39;lxml&#39;)print(i, soup.find(&#39;h1&#39;).get_text(), &#39;    url: &#39;, his[-1])# find valid urlssub_urls = soup.find_all(&quot;a&quot;, &#123;&quot;target&quot;: &quot;_blank&quot;, &quot;href&quot;: re.compile(&quot;/item/(%.&#123;2&#125;)+$&quot;)&#125;)if len(sub_urls) != 0:    his.append(random.sample(sub_urls, 1)[0][&#39;href&#39;])else:    # no valid sub link found    his.pop()</code></pre><p>这样我们就能观看我们的爬虫现在爬去了哪? 是不是爬到了和 网页爬虫 起始页完全不相关的地方去了.</p><pre><code>0 网络爬虫     url:  /item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/51627111 路由器     url:  /item/%E8%B7%AF%E7%94%B1%E5%99%A82 服务等级     url:  /item/%E6%9C%8D%E5%8A%A1%E7%AD%89%E7%BA%A7...17 呼损率     url:  /item/%E5%91%BC%E6%8D%9F%E7%8E%8718 服务等级     url:  /item/%E6%9C%8D%E5%8A%A1%E7%AD%89%E7%BA%A719 呼损率     url:  /item/%E5%91%BC%E6%8D%9F%E7%8E%87</code></pre>]]></content>
      
      
      <categories>
          
          <category> 《栈溢出工程师》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> Statistic Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爬虫基础-正则表达匹配</title>
      <link href="2020/08/05/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/"/>
      <url>2020/08/05/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h1 id="网页结构"><a href="#网页结构" class="headerlink" title="网页结构"></a>网页结构</h1><p>网页由HTML组成，用JS和CSS修饰，爬虫会根据html中的标签来搜索合适的信息<br><span id="more"></span></p><h2 id="用python登录网页"><a href="#用python登录网页" class="headerlink" title="用python登录网页"></a>用python登录网页</h2><pre><code>from urllib.request import urlopenhtml = urlopen(&quot;https://fazzie-key.cool.html&quot;).read().decode(&#39;utf-8&#39;)print(html)</code></pre><h2 id="利用python搜索内容"><a href="#利用python搜索内容" class="headerlink" title="利用python搜索内容"></a>利用python搜索内容</h2><h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h3><p>正则表达式 (Regular Expression) 又称 RegEx, 是用来匹配字符的一种工具. </p><pre><code>import re #导入模块# regular expressionpattern1 = &quot;cat&quot;pattern2 = &quot;bird&quot;string = &quot;dog runs to cat&quot;print(re.search(pattern1, string)) print(re.search(pattern2, string))  </code></pre><p>打印结果    </p><pre><code>  &lt;_sre.SRE_Match object; span=(12, 15), match=&#39;cat&#39;&gt; None</code></pre><h4 id="匹配多种可能"><a href="#匹配多种可能" class="headerlink" title="匹配多种可能"></a>匹配多种可能</h4><p> <code>[]</code>可以匹配多种结果</p><pre><code>print(re.search(r&quot;r[au]n&quot;,&quot;dog runs to cat&quot;)print(re.search(r&quot;r[A-Z]n&quot;,&quot;dog runs to cat&quot;)print(re.search(r&quot;r[a-z]n&quot;,&quot;dog runs to cat&quot;)print(re.search(r&quot;r[0-9a-z]n&quot;,&quot;dog runs to cat&quot;)</code></pre><h4 id="特殊匹配"><a href="#特殊匹配" class="headerlink" title="特殊匹配"></a>特殊匹配</h4><p> 一些特殊字符可以利用<code>\</code>转义进行匹配</p><div class="table-container"><table><thead><tr><th>字符</th><th>含义</th></tr></thead><tbody><tr><td> \d</td><td>任何数字</td></tr><tr><td>\D</td><td>不是数字</td></tr><tr><td>\s</td><td>任何 white space, 如 [\t\n\r\f\v]</td></tr><tr><td>\S</td><td>不是 white space</td></tr><tr><td>\w</td><td>任何大小写字母, 数字和 <em> [a-zA-Z0-9</em>]</td></tr><tr><td>\W</td><td>不是 \w</td></tr><tr><td>\b</td><td>空白字符 (只在某个字的开头或结尾)</td></tr><tr><td>\B</td><td>空白字符 (不在某个字的开头或结尾)</td></tr><tr><td>\</td><td>匹配 \</td></tr><tr><td>.</td><td>匹配任何字符 (除了 \n)</td></tr><tr><td>^</td><td>匹配开头</td></tr><tr><td>$</td><td>匹配结尾</td></tr><tr><td>?</td><td>前面的字符可有可无</td></tr></tbody></table></div><p><strong>代码案例</strong></p><pre><code># \d : decimal digitprint(re.search(r&quot;r\dn&quot;, &quot;run r4n&quot;))           # &lt;_sre.SRE_Match object; span=(4, 7), match=&#39;r4n&#39;&gt;# \D : any non-decimal digitprint(re.search(r&quot;r\Dn&quot;, &quot;run r4n&quot;))           # &lt;_sre.SRE_Match object; span=(0, 3), match=&#39;run&#39;&gt;# \s : any white space [\t\n\r\f\v]print(re.search(r&quot;r\sn&quot;, &quot;r\nn r4n&quot;))          # &lt;_sre.SRE_Match object; span=(0, 3), match=&#39;r\nn&#39;&gt;# \S : opposite to \s, any non-white spaceprint(re.search(r&quot;r\Sn&quot;, &quot;r\nn r4n&quot;))          # &lt;_sre.SRE_Match object; span=(4, 7), match=&#39;r4n&#39;&gt;# \w : [a-zA-Z0-9_]print(re.search(r&quot;r\wn&quot;, &quot;r\nn r4n&quot;))          # &lt;_sre.SRE_Match object; span=(4, 7), match=&#39;r4n&#39;&gt;# \W : opposite to \wprint(re.search(r&quot;r\Wn&quot;, &quot;r\nn r4n&quot;))          # &lt;_sre.SRE_Match object; span=(0, 3), match=&#39;r\nn&#39;&gt;# \b : empty string (only at the start or end of the word)print(re.search(r&quot;\bruns\b&quot;, &quot;dog runs to cat&quot;))    # &lt;_sre.SRE_Match object; span=(4, 8), match=&#39;runs&#39;&gt;# \B : empty string (but not at the start or end of a word)print(re.search(r&quot;\B runs \B&quot;, &quot;dog   runs  to cat&quot;))  # &lt;_sre.SRE_Match object; span=(8, 14), match=&#39; runs &#39;&gt;# \\ : match \print(re.search(r&quot;runs\\&quot;, &quot;runs\ to me&quot;))     # &lt;_sre.SRE_Match object; span=(0, 5), match=&#39;runs\\&#39;&gt;# . : match anything (except \n)print(re.search(r&quot;r.n&quot;, &quot;r[ns to me&quot;))         # &lt;_sre.SRE_Match object; span=(0, 3), match=&#39;r[n&#39;&gt;# ^ : match line beginningprint(re.search(r&quot;^dog&quot;, &quot;dog runs to cat&quot;))   # &lt;_sre.SRE_Match object; span=(0, 3), match=&#39;dog&#39;&gt;# $ : match line endingprint(re.search(r&quot;cat$&quot;, &quot;dog runs to cat&quot;))   # &lt;_sre.SRE_Match object; span=(12, 15), match=&#39;cat&#39;&gt;# ? : may or may not occurprint(re.search(r&quot;Mon(day)?&quot;, &quot;Monday&quot;))       # &lt;_sre.SRE_Match object; span=(0, 6), match=&#39;Monday&#39;&gt;print(re.search(r&quot;Mon(day)?&quot;, &quot;Mon&quot;))          # &lt;_sre.SRE_Match object; span=(0, 3), match=&#39;Mon&#39;&gt;</code></pre><h4 id="重复匹配"><a href="#重复匹配" class="headerlink" title="重复匹配"></a>重复匹配</h4><div class="table-container"><table><thead><tr><th>字符</th><th>功能</th></tr></thead><tbody><tr><td>*</td><td>重复零次或多次</td></tr><tr><td>+</td><td>重复一次或多次</td></tr><tr><td>{n, m}</td><td>重复 n 至 m 次</td></tr><tr><td>{n}</td><td>重复 n 次</td></tr></tbody></table></div><p><strong>代码案例</strong></p><pre><code># * : occur 0 or more timesprint(re.search(r&quot;ab*&quot;, &quot;a&quot;))             # &lt;_sre.SRE_Match object; span=(0, 1), match=&#39;a&#39;&gt;print(re.search(r&quot;ab*&quot;, &quot;abbbbb&quot;))        # &lt;_sre.SRE_Match object; span=(0, 6), match=&#39;abbbbb&#39;&gt;# + : occur 1 or more timesprint(re.search(r&quot;ab+&quot;, &quot;a&quot;))             # Noneprint(re.search(r&quot;ab+&quot;, &quot;abbbbb&quot;))        # &lt;_sre.SRE_Match object; span=(0, 6), match=&#39;abbbbb&#39;&gt;# &#123;n, m&#125; : occur n to m timesprint(re.search(r&quot;ab&#123;2,10&#125;&quot;, &quot;a&quot;))        # Noneprint(re.search(r&quot;ab&#123;2,10&#125;&quot;, &quot;abbbbb&quot;))   # &lt;_sre.SRE_Match object; span=(0, 6), match=&#39;abbbbb&#39;&gt;</code></pre><h4 id="分组匹配"><a href="#分组匹配" class="headerlink" title="分组匹配"></a>分组匹配</h4><p>使用（）可以进行分组匹配<br> 比如在这个 (\d+) 组里, 需要找到的是一些数字, 在 (.+) 这个组里, 我们会找到 Date: 后面的所有内容. 当使用 match.group() 时, 他会返回所有组里的内容, 而如果给 .group(2) 里加一个数, 它就能定位你需要返回哪个组里的信息.</p><pre><code>match = re.search(r&quot;(\d+), Date: (.+)&quot;, &quot;ID: 021523, Date: Feb/12/2017&quot;)print(match.group())                   # 021523, Date: Feb/12/2017print(match.group(1))                  # 021523print(match.group(2))                  # Date: Feb/12/2017</code></pre><p>有时候, 组会很多, 光用数字可能比较难找到自己想要的组, 这时候, 如果有一个名字当做索引, 会是一件很容易的事. 我们字需要在括号的开头写上这样的形式 <code>?P&lt;名字&gt;</code> 就给这个组定义了一个名字. 然后就能用这个名字找到这个组的内容.</p><pre><code>match = re.search(r&quot;(?P&lt;id&gt;\d+), Date: (?P&lt;date&gt;.+)&quot;, &quot;ID: 021523, Date: Feb/12/2017&quot;)print(match.group(&#39;id&#39;))                # 021523print(match.group(&#39;date&#39;))              # Date: Feb/12/2017</code></pre><h4 id="寻找所有匹配"><a href="#寻找所有匹配" class="headerlink" title="寻找所有匹配"></a>寻找所有匹配</h4><pre><code>    # findallprint(re.findall(r&quot;r[ua]n&quot;, &quot;run ran ren&quot;))    # [&#39;run&#39;, &#39;ran&#39;]# | : orprint(re.findall(r&quot;(run|ran)&quot;, &quot;run ran ren&quot;)) # [&#39;run&#39;, &#39;ran&#39;]</code></pre><h4 id="替换"><a href="#替换" class="headerlink" title="替换"></a>替换</h4><p>我们还能通过正则表达式匹配上一些形式的字符串然后再替代掉这些字符串. 使用这种匹配 re.sub(), 将会比 python 自带的 string.replace() 要灵活多变.</p><pre><code>print(re.sub(r&quot;r[au]ns&quot;, &quot;catches&quot;, &quot;dog runs to cat&quot;))     # dog catches to cat</code></pre><h4 id="split"><a href="#split" class="headerlink" title="split"></a>split</h4><p>split方法可以分解句子中的单词</p><pre><code>print(re.split(r&quot;[,;\.]&quot;, &quot;a;b,c.d;e&quot;))             # [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]</code></pre><h4 id="compile"><a href="#compile" class="headerlink" title="compile"></a>compile</h4><p>compile方法，可以对一个正则匹配重复使用</p><pre><code>compiled_re = re.compile(r&quot;r[ua]n&quot;)print(compiled_re.search(&quot;dog ran to cat&quot;))  # &lt;_sre.SRE_Match object; span=(4, 7), match=&#39;ran&#39;&gt;</code></pre><hr><h3 id="利用正则表达式在网页中获取关键信息"><a href="#利用正则表达式在网页中获取关键信息" class="headerlink" title="利用正则表达式在网页中获取关键信息"></a>利用正则表达式在网页中获取关键信息</h3><h4 id="匹配title"><a href="#匹配title" class="headerlink" title="匹配title"></a>匹配title</h4><pre><code>res = re.findall(r&quot;&lt;title&gt;(.+?)&lt;/title&gt;&quot;, html)print(&quot;\nPage title is: &quot;, res[0])</code></pre><h4 id="匹配段落"><a href="#匹配段落" class="headerlink" title="匹配段落"></a>匹配段落</h4><pre><code>res = re.findall(r&quot;&lt;p&gt;(.*?)&lt;/p&gt;&quot;, html, flags=re.DOTALL)    # re.DOTALL if multi lineprint(&quot;\nPage paragraph is: &quot;, res[0])</code></pre><blockquote><p>因为这个段落在 HTML 中还夹杂着 tab, new line, 所以我们给一个 flags=re.DOTALL 来对这些 tab, new line 不敏感.</p></blockquote><h4 id="匹配链接"><a href="#匹配链接" class="headerlink" title="匹配链接"></a>匹配链接</h4><pre><code>res = re.findall(r&#39;href=&quot;(.*?)&quot;&#39;, html)print(&quot;\nAll links: &quot;, res)# All links:</code></pre>]]></content>
      
      
      <categories>
          
          <category> 《栈溢出工程师》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> Statistic Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch basic</title>
      <link href="2020/08/04/pytorch-basic/"/>
      <url>2020/08/04/pytorch-basic/</url>
      
        <content type="html"><![CDATA[<h1 id="激励函数"><a href="#激励函数" class="headerlink" title="激励函数"></a>激励函数</h1><h2 id="常见激励函数"><a href="#常见激励函数" class="headerlink" title="常见激励函数"></a>常见激励函数</h2><div class="table-container"><table><thead><tr><th>函数</th><th>作用场景</th></tr></thead><tbody><tr><td>relu</td><td>CNN</td></tr><tr><td>sigmoid</td><td>二分类</td></tr><tr><td>tanh</td><td>RNN</td></tr><tr><td>softmax</td><td>多分类</td></tr><tr><td>softplus</td><td></td></tr></tbody></table></div><span id="more"></span><h2 id="导入函数包"><a href="#导入函数包" class="headerlink" title="导入函数包"></a>导入函数包</h2><pre><code>import torch.nn.functional as F</code></pre><h1 id="神经网络搭建基本步骤"><a href="#神经网络搭建基本步骤" class="headerlink" title="神经网络搭建基本步骤"></a>神经网络搭建基本步骤</h1><ul><li>数据加载</li><li>网络搭建</li><li>优化器合损失函数定义</li><li>训练</li></ul><h2 id="网络搭建"><a href="#网络搭建" class="headerlink" title="网络搭建"></a>网络搭建</h2><pre><code>class Net(torch.nn.Module):  # 继承 torch 的 Module    def __init__(self, n_feature, n_hidden, n_output):        super(Net, self).__init__()     # 继承 __init__ 功能        # 定义每层用什么样的形式        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # 隐藏层线性输出        self.predict = torch.nn.Linear(n_hidden, n_output)   # 输出层线性输出def forward(self, x):   # 这同时也是 Module 中的 forward 功能    # 正向传播输入值, 神经网络分析出输出值    x = F.relu(self.hidden(x))      # 激励函数(隐藏层的线性值)    x = self.predict(x)             # 输出值    return x</code></pre><hr><h3 id="查看网络结构"><a href="#查看网络结构" class="headerlink" title="查看网络结构"></a>查看网络结构</h3><pre><code>    print (net)</code></pre><h2 id="optimizer优化器选择和损失函数定义"><a href="#optimizer优化器选择和损失函数定义" class="headerlink" title="optimizer优化器选择和损失函数定义"></a>optimizer优化器选择和损失函数定义</h2><pre><code>optimizer = torch.optim.SGD(net.parameters(), lr=0.2)  # 传入 net 的所有参数, 学习率loss_func = torch.nn.MSELoss()      # 预测值和真实值的误差计算公式 (均方差)</code></pre><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><pre><code>for t in range(100):    prediction = net(x)     # 喂给 net 训练数据 x, 输出预测值loss = loss_func(prediction, y)     # 计算两者的误差optimizer.zero_grad()   # 清空上一步的残余更新参数值loss.backward()         # 误差反向传播, 计算参数更新值optimizer.step()        # 将参数更新值施加到 net 的 parameters 上</code></pre><h1 id="快速搭建"><a href="#快速搭建" class="headerlink" title="快速搭建"></a>快速搭建</h1><p>使用<code>torch.nn.Sequential</code></p><script type="math/tex; mode=display">python    net2 = torch.nn.Sequential(        torch.nn.Linear(1, 10),        torch.nn.ReLU(),        torch.nn.Linear(10, 1)    )</script><h1 id="保存提取"><a href="#保存提取" class="headerlink" title="保存提取"></a>保存提取</h1><h2 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h2><pre><code>torch.save(net1,&#39;net.pkl&#39;)#保存整个网络torch.save(net.state_dict(),&#39;net_params.pkl&#39;) #保存参数</code></pre><h2 id="提取"><a href="#提取" class="headerlink" title="提取"></a>提取</h2><h3 id="提取网络"><a href="#提取网络" class="headerlink" title="提取网络"></a>提取网络</h3><pre><code>def restore_net():    # restore entire net1 to net2    net2 = torch.load(&#39;net.pkl&#39;)    prediction = net2(x)</code></pre><h3 id="只提取参数（速度快）"><a href="#只提取参数（速度快）" class="headerlink" title="只提取参数（速度快）"></a>只提取参数（速度快）</h3><pre><code>def restore_params():    # 新建 net3    net3 = torch.nn.Sequential(        torch.nn.Linear(1, 10),        torch.nn.ReLU(),        torch.nn.Linear(10, 1)    )    # 将保存的参数复制到 net3net3.load_state_dict(torch.load(&#39;net_params.pkl&#39;))prediction = net3(x)</code></pre><h1 id="批训练"><a href="#批训练" class="headerlink" title="批训练"></a>批训练</h1><p>pytorch的dataloader模块提供的批训练的基本函数</p><pre><code>import torchimport torch.utils.data as Datatorch.manual_seed(1)    # reproducibleBATCH_SIZE = 5      # 批训练的数据个数x = torch.linspace(1, 10, 10)       # x data (torch tensor)y = torch.linspace(10, 1, 10)       # y data (torch tensor)# 先转换成 torch 能识别的 Datasettorch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)# 把 dataset 放入 DataLoaderloader = Data.DataLoader(    dataset=torch_dataset,      # torch TensorDataset format    batch_size=BATCH_SIZE,      # mini batch size    shuffle=True,               # 要不要打乱数据 (打乱比较好)    num_workers=2,              # 多线程来读数据)for epoch in range(3):   # 训练所有!整套!数据 3 次    for step, (batch_x, batch_y) in enumerate(loader):  # 每一步 loader 释放一小批数据用来学习        # 假设这里就是你训练的地方...        # 打出来一些数据        print(&#39;Epoch: &#39;, epoch, &#39;| Step: &#39;, step, &#39;| batch x: &#39;,              batch_x.numpy(), &#39;| batch y: &#39;, batch_y.numpy())&quot;&quot;&quot;Epoch:  0 | Step:  0 | batch x:  [ 6.  7.  2.  3.  1.] | batch y:  [  5.   4.   9.   8.  10.]Epoch:  0 | Step:  1 | batch x:  [  9.  10.   4.   8.   5.] | batch y:  [ 2.  1.  7.  3.  6.]Epoch:  1 | Step:  0 | batch x:  [  3.   4.   2.   9.  10.] | batch y:  [ 8.  7.  9.  2.  1.]Epoch:  1 | Step:  1 | batch x:  [ 1.  7.  8.  5.  6.] | batch y:  [ 10.   4.   3.   6.   5.]Epoch:  2 | Step:  0 | batch x:  [ 3.  9.  2.  6.  7.] | batch y:  [ 8.  2.  9.  5.  4.]Epoch:  2 | Step:  1 | batch x:  [ 10.   4.   8.   1.   5.] | batch y:  [  1.   7.   3.  10.   6.]&quot;&quot;&quot;</code></pre><h1 id="训练优化"><a href="#训练优化" class="headerlink" title="训练优化"></a>训练优化</h1><h2 id="常见优化方法"><a href="#常见优化方法" class="headerlink" title="常见优化方法"></a>常见优化方法</h2><ul><li>SGD</li><li>Momentum</li><li>RMSprop</li><li>Adam</li></ul><h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><p>将数据分批放入神经网络训练</p><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/04/df3fce20d20e210064dbf23356b81f15.png" alt=""></p><h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/04/0900f7953aa3ec6ef67a6c5329565975.png" alt=""></p><h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/04/1c95af9622e62ebd91229abb64bf1ff2.png" alt=""></p><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/08/04/a30715a23498fb6271234e0cf101b31b.png" alt=""></p><pre><code># different optimizersopt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]</code></pre><h1 id="卷积神经网络搭建"><a href="#卷积神经网络搭建" class="headerlink" title="卷积神经网络搭建"></a>卷积神经网络搭建</h1><pre><code>class CNN(nn.Module):    def __init__(self):        super(CNN, self).__init__()        self.conv1 = nn.Sequential(  # input shape (1, 28, 28)            nn.Conv2d(                in_channels=1,      # input height                out_channels=16,    # n_filters                kernel_size=5,      # filter size                stride=1,           # filter movement/step                padding=2,      # 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1            ),      # output shape (16, 28, 28)            nn.ReLU(),    # activation            nn.MaxPool2d(kernel_size=2),    # 在 2x2 空间里向下采样, output shape (16, 14, 14)        )        self.conv2 = nn.Sequential(  # input shape (16, 14, 14)            nn.Conv2d(16, 32, 5, 1, 2),  # output shape (32, 14, 14)            nn.ReLU(),  # activation            nn.MaxPool2d(2),  # output shape (32, 7, 7)        )        self.out = nn.Linear(32 * 7 * 7, 10)   # fully connected layer, output 10 classes    def forward(self, x):        x = self.conv1(x)        x = self.conv2(x)        x = x.view(x.size(0), -1)   # 展平多维的卷积图成 (batch_size, 32 * 7 * 7)        output = self.out(x)</code></pre><h1 id="循环神经网络搭建"><a href="#循环神经网络搭建" class="headerlink" title="循环神经网络搭建"></a>循环神经网络搭建</h1><h2 id="RNN分类"><a href="#RNN分类" class="headerlink" title="RNN分类"></a>RNN分类</h2><pre><code>class RNN(nn.Module):    def __init__(self):        super(RNN, self).__init__()        self.rnn = nn.LSTM(     # LSTM 效果要比 nn.RNN() 好多了            input_size=28,      # 图片每行的数据像素点            hidden_size=64,     # rnn hidden unit            num_layers=1,       # 有几层 RNN layers            batch_first=True,   # input &amp; output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)        )        self.out = nn.Linear(64, 10)    # 输出层    def forward(self, x):        # x shape (batch, time_step, input_size)        # r_out shape (batch, time_step, output_size)        # h_n shape (n_layers, batch, hidden_size)   LSTM 有两个 hidden states, h_n 是分线, h_c 是主线        # h_c shape (n_layers, batch, hidden_size)        r_out, (h_n, h_c) = self.rnn(x, None)   # None 表示 hidden state 会用全0的 state        # 选取最后一个时间点的 r_out 输出        # 这里 r_out[:, -1, :] 的值也是 h_n 的值        out = self.out(r_out[:, -1, :])        return out</code></pre><h2 id="RNN回归"><a href="#RNN回归" class="headerlink" title="RNN回归"></a>RNN回归</h2><pre><code>class RNN(nn.Module):    def __init__(self):        super(RNN, self).__init__()        self.rnn = nn.RNN(  # 这回一个普通的 RNN 就能胜任            input_size=1,            hidden_size=32,     # rnn hidden unit            num_layers=1,       # 有几层 RNN layers            batch_first=True,   # input &amp; output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)        )        self.out = nn.Linear(32, 1)    def forward(self, x, h_state):  # 因为 hidden state 是连续的, 所以我们要一直传递这一个 state        # x (batch, time_step, input_size)        # h_state (n_layers, batch, hidden_size)        # r_out (batch, time_step, output_size)        r_out, h_state = self.rnn(x, h_state)   # h_state 也要作为 RNN 的一个输入        outs = []    # 保存所有时间点的预测值        for time_step in range(r_out.size(1)):    # 对每一个时间点计算 output            outs.append(self.out(r_out[:, time_step, :]))        return torch.stack(outs, dim=1), h_state</code></pre><p>前向传播也可以简化为</p><pre><code>        def forward(self, x, h_state):    r_out, h_state = self.rnn(x, h_state)    r_out = r_out.view(-1, 32)    outs = self.out(r_out)    return outs.view(-1, 32, TIME_STEP), h_state</code></pre><h1 id="GPU加速"><a href="#GPU加速" class="headerlink" title="GPU加速"></a>GPU加速</h1><h3 id="将变量移动到GPU"><a href="#将变量移动到GPU" class="headerlink" title="将变量移动到GPU"></a>将变量移动到GPU</h3><pre><code>x.cuda()</code></pre><h3 id="计算图移动到GPU"><a href="#计算图移动到GPU" class="headerlink" title="计算图移动到GPU"></a>计算图移动到GPU</h3><pre><code>net.cuda()</code></pre><h3 id="训练移动到GPU"><a href="#训练移动到GPU" class="headerlink" title="训练移动到GPU"></a>训练移动到GPU</h3><pre><code>pred_y = torch.max(test_output, 1)[1].cuda().data.squeeze()  # 将操作放去 GPU</code></pre><blockquote><p>pytorch 的GPU编号从0开始，调用多GPU时要注意</p></blockquote><h1 id="dropout防止过拟合"><a href="#dropout防止过拟合" class="headerlink" title="dropout防止过拟合"></a>dropout防止过拟合</h1><h2 id="无过拟合网络"><a href="#无过拟合网络" class="headerlink" title="无过拟合网络"></a>无过拟合网络</h2><pre><code>net_overfitting = torch.nn.Sequential(    torch.nn.Linear(1, N_HIDDEN),    torch.nn.ReLU(),    torch.nn.Linear(N_HIDDEN, N_HIDDEN),    torch.nn.ReLU(),    torch.nn.Linear(N_HIDDEN, 1),)</code></pre><h2 id="dropout网络"><a href="#dropout网络" class="headerlink" title="dropout网络"></a>dropout网络</h2><pre><code>net_dropped = torch.nn.Sequential(    torch.nn.Linear(1, N_HIDDEN),    torch.nn.Dropout(0.5),  # drop 50% of the neuron    torch.nn.ReLU(),    torch.nn.Linear(N_HIDDEN, N_HIDDEN),    torch.nn.Dropout(0.5),  # drop 50% of the neuron    torch.nn.ReLU(),    torch.nn.Linear(N_HIDDEN, 1),)</code></pre><h2 id="在使用预测时取出dropout"><a href="#在使用预测时取出dropout" class="headerlink" title="在使用预测时取出dropout"></a>在使用预测时取出dropout</h2><pre><code>net_dropped.eval()</code></pre><h1 id="Batch-Normalization归一化"><a href="#Batch-Normalization归一化" class="headerlink" title="Batch Normalization归一化"></a>Batch Normalization归一化</h1><h2 id="Batch-Normalization过程"><a href="#Batch-Normalization过程" class="headerlink" title="Batch Normalization过程"></a>Batch Normalization过程</h2><script type="math/tex; mode=display"> \frac{1}{m}\sum_{i=1}^m x_i \to \mu_{\beta}</script><script type="math/tex; mode=display"> \frac{1}{m}\sum_{i=1}^m (x_i-\mu_{\beta})^2 \to \sigma_{\beta}^2</script><script type="math/tex; mode=display"> \frac{x_i-\mu_{\beta}}{\sqrt{\sigma^2_{\beta}+\epsilon}} \to \hat{x_i}</script><script type="math/tex; mode=display">BN_{\gamma,\beta}(x_i)= \gamma\hat{x_i}+\beta \to y_i</script><hr><pre><code> class Net(nn.Module):    def __init__(self, batch_normalization=False):        super(Net, self).__init__()        self.do_bn = batch_normalization        self.fcs = []   # 太多层了, 我们用 for loop 建立        self.bns = []        self.bn_input = nn.BatchNorm1d(1, momentum=0.5)   # 给 input 的 BN        for i in range(N_HIDDEN):               # 建层            input_size = 1 if i == 0 else 10            fc = nn.Linear(input_size, 10)            setattr(self, &#39;fc%i&#39; % i, fc)       # 注意! pytorch 一定要你将层信息变成 class 的属性! 我在这里花了2天时间发现了这个 bug            self._set_init(fc)                  # 参数初始化            self.fcs.append(fc)            if self.do_bn:                bn = nn.BatchNorm1d(10, momentum=0.5)                setattr(self, &#39;bn%i&#39; % i, bn)   # 注意! pytorch 一定要你将层信息变成 class 的属性! 我在这里花了2天时间发现了这个 bug                self.bns.append(bn)        self.predict = nn.Linear(10, 1)         # output layer        self._set_init(self.predict)            # 参数初始化    def _set_init(self, layer):     # 参数初始化        init.normal_(layer.weight, mean=0., std=.1)        init.constant_(layer.bias, B_INIT)    def forward(self, x):        pre_activation = [x]        if self.do_bn: x = self.bn_input(x)    # 判断是否要加 BN        layer_input = [x]        for i in range(N_HIDDEN):            x = self.fcs[i](x)            pre_activation.append(x)    # 为之后出图            if self.do_bn: x = self.bns[i](x)  # 判断是否要加 BN            x = ACTIVATION(x)            layer_input.append(x)       # 为之后出图        out = self.predict(x)        return out, layer_input, pre_activation# 建立两个 net, 一个有 BN, 一个没有nets = [Net(batch_normalization=False), Net(batch_normalization=True)]       </code></pre>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep learning </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JS Basic</title>
      <link href="2020/07/31/JS-Basic/"/>
      <url>2020/07/31/JS-Basic/</url>
      
        <content type="html"><![CDATA[<h1 id="JavaScript-基础"><a href="#JavaScript-基础" class="headerlink" title="JavaScript 基础"></a>JavaScript 基础</h1><ol><li><p>所有主流浏览器都支持JavaScript。</p></li><li><p>目前，全世界大部分网页都使用JavaScript。</p></li><li><p>它可以让网页呈现各种动态效果。</p></li></ol><span id="more"></span><h2 id="如何插入JS"><a href="#如何插入JS" class="headerlink" title="如何插入JS"></a>如何插入JS</h2><p>使用<code>&lt;script&gt;</code>标签在HTML网页中插入JavaScript代码。注意， <code>&lt;script&gt;</code>标签要成对出现，并把JavaScript代码写在<code>&lt;script&gt;&lt;/script&gt;</code>之间。</p><p><code>&lt;script type=&quot;text/javascript&quot;&gt;</code>表示在<code>&lt;script&gt;&lt;/script&gt;</code>之间的是文本类型(text),javascript是为了告诉浏览器里面的文本是属于JavaScript语言。</p><h2 id="应用JS文件"><a href="#应用JS文件" class="headerlink" title="应用JS文件"></a>应用JS文件</h2><p>JS文件不能直接运行，需嵌入到HTML文件中执行，我们需在HTML中添加如下代码，就可将JS文件嵌入HTML文件中。</p><pre><code>&lt;script src=&quot;script.js&quot;&gt;&lt;/script&gt;</code></pre><h2 id="嵌入JS的位置"><a href="#嵌入JS的位置" class="headerlink" title="嵌入JS的位置"></a>嵌入JS的位置</h2><p>我们可以将JavaScript代码放在html文件中任何位置，但是我们一般放在网页的head或者body部分。<br>放在<code>&lt;head&gt;</code>部分<br>最常用的方式是在页面中head部分放置<code>&lt;script&gt;</code>元素，浏览器解析head部分就会执行这个代码，然后才解析页面的其余部分。<br>放在<code>&lt;body&gt;</code>部分<br>JavaScript代码在网页读取到该语句的时候就会执行。</p><blockquote><p>注意: javascript作为一种脚本语言可以放在html页面中任何位置，但是浏览器解释html时是按先后顺序的，所以前面的script就先被执行。比如进行页面显示初始化的js必须放在head里面，因为初始化都要求提前进行（如给页面body设置css等）；而如果是通过事件调用执行的function那么对位置没什么要求的。</p></blockquote><h2 id="JS语句和符号"><a href="#JS语句和符号" class="headerlink" title="JS语句和符号"></a>JS语句和符号</h2><p>JavaScript语句是发给浏览器的命令。这些命令的作用是告诉浏览器要做的事情。</p><p>每一句JavaScript代码格式: 语句;</p><p>先来看看下面代码</p><pre><code>&lt;script type=&quot;text/javascript&quot;&gt;   alert(&quot;hello!&quot;);&lt;/script&gt;</code></pre><p>例子中的alert(“hello!”);就是一个JavaScript语句。</p><p>一行的结束就被认定为语句的结束，通常在结尾加上一个分号”;”来表示语句的结束。</p><p>看看下面这段代码,有三条语句，每句结束后都有”;”，按顺序执行语句。</p><pre><code>&lt;script type=&quot;text/javascript&quot;&gt;   document.write(&quot;I&quot;);   document.write(&quot;love&quot;);   document.write(&quot;JavaScript&quot;);&lt;/script&gt;</code></pre><p>注意:</p><ol><li><p>“;”分号要在英文状态下输入，同样，JS中的代码和符号都要在英文状态下输入。</p></li><li><p>虽然分号“;”也可以不写，但我们要养成编程的好习惯，记得在语句末尾写上分号。</p></li></ol><h2 id="JS注释"><a href="#JS注释" class="headerlink" title="JS注释"></a>JS注释</h2><p>注释的作用是提高代码的可读性，帮助自己和别人阅读和理解你所编写的JavaScript代码，注释的内容不会在网页中显示。注释可分为单行注释与多行注释两种。</p><p>我们为了方便阅读，注释内容一般放到需要解释语句的结尾处或周围。</p><p>单行注释，在注释内容前加符号 “//”。</p><pre><code>&lt;script type=&quot;text/javascript&quot;&gt;  document.write(&quot;单行注释使用&#39;//&#39;&quot;);  // 我是注释，该语句功能在网页中输出内容&lt;/script&gt;多行注释以&quot;/*&quot;开始，以&quot;*/&quot;结束。&lt;script type=&quot;text/javascript&quot;&gt;   document.write(&quot;多行注释使用/*注释内容*/&quot;);   /*    多行注释    养成书写注释的良好习惯   */&lt;/script&gt;</code></pre><h2 id="JS变量"><a href="#JS变量" class="headerlink" title="JS变量"></a>JS变量</h2><p>定义变量使用关键字var,语法如下：</p><p>var 变量名<br>变量名可以任意取名，但要遵循命名规则:</p><pre><code>1.变量必须使用字母、下划线(_)或者美元符($)开始。2.然后可以使用任意多个英文字母、数字、下划线(_)或者美元符($)组成。3.不能使用JavaScript关键词与JavaScript保留字。</code></pre><p>变量要先声明再赋值，如下：</p><pre><code>var mychar;mychar=&quot;javascript&quot;;var mynum = 6;</code></pre><p>变量可以重复赋值，如下：</p><pre><code>var mychar;mychar=&quot;javascript&quot;;mychar=&quot;hello&quot;;</code></pre><p>注意:</p><ol><li><p>在JS中区分大小写，如变量mychar与myChar是不一样的，表示是两个变量。</p></li><li><p>变量虽然也可以不声明，直接使用，但不规范，需要先声明，后使用。</p></li></ol><h2 id="判断语句"><a href="#判断语句" class="headerlink" title="判断语句"></a>判断语句</h2><p>if…else语句是在指定的条件成立时执行代码，在条件不成立时执行else后的代码。</p><p>语法:</p><p>if(条件)<br>{ 条件成立时执行的代码 }<br>else<br>{ 条件不成立时执行的代码 }<br>假设我们通过年龄来判断是否为成年人，如年龄大于等于18岁，是成年人，否则不是成年人。代码表示如下:</p><pre><code>&lt;script type=&quot;text/javascript&quot;&gt;   var myage = 18;   if(myage&gt;=18)  //myage&gt;=18是判断条件   &#123; document.write(&quot;你是成年人。&quot;);&#125;   else  //否则年龄小于18   &#123; document.write(&quot;未满18岁，你不是成年人。&quot;);&#125;&lt;/script&gt;</code></pre><h2 id="JS函数"><a href="#JS函数" class="headerlink" title="JS函数"></a>JS函数</h2><p>如何定义一个函数呢？基本语法如下:</p><pre><code>function 函数名()&#123;     函数代码;&#125;</code></pre><p>说明:</p><ol><li><p>function定义函数的关键字。</p></li><li><p>“函数名”你为函数取的名字。</p></li><li><p>“函数代码”替换为完成特定功能的代码。</p></li></ol><p>我们来编写一个实现两数相加的简单函数,并给函数起个有意义的名字：“add2”，代码如下：</p><pre><code>function add2()&#123;   var sum = 3 + 2;   alert(sum);&#125;</code></pre><p>函数调用:</p><p>函数定义好后，是不能自动执行的，所以需调用它,只需直接在需要的位置写函数就ok了,代码如下:</p><h1 id="JS互动"><a href="#JS互动" class="headerlink" title="JS互动"></a>JS互动</h1><h2 id="JS输出内容"><a href="#JS输出内容" class="headerlink" title="JS输出内容"></a>JS输出内容</h2><p>document.write() 可用于直接向 HTML 输出流写内容。简单的说就是直接在网页中输出内容。</p><p>第一种:输出内容用””括起，直接输出””号内的内容。</p><pre><code>&lt;script type=&quot;text/javascript&quot;&gt;  document.write(&quot;I love JavaScript！&quot;); //内容用&quot;&quot;括起来，&quot;&quot;里的内容直接输出。&lt;/script&gt;</code></pre><p>第二种:通过变量，输出内容</p><pre><code>&lt;script type=&quot;text/javascript&quot;&gt;  var mystr=&quot;hello world!&quot;;  document.write(mystr);  //直接写变量名，输出变量存储的内容。&lt;/script&gt;</code></pre><p>第三种:输出多项内容，内容之间用+号连接。</p><pre><code>&lt;script type=&quot;text/javascript&quot;&gt;  var mystr=&quot;hello&quot;;  document.write(mystr+&quot;I love JavaScript&quot;); //多项内容之间用+号连接&lt;/script&gt;</code></pre><p>第四种:输出HTML标签，并起作用，标签使用””括起来。</p><pre><code>&lt;script type=&quot;text/javascript&quot;&gt;  var mystr=&quot;hello&quot;;document.write(mystr+&quot;&lt;br&gt;&quot;);//输出hello后，输出一个换行符  document.write(&quot;JavaScript&quot;);&lt;/script&gt;</code></pre><h2 id="警告框"><a href="#警告框" class="headerlink" title="警告框"></a>警告框</h2><p>我们在访问网站的时候，有时会突然弹出一个小窗口，上面写着一段提示信息文字。如果你不点击“确定”，就不能对网页做任何操作，这个小窗口就是使用alert实现的。</p><p><strong>语法:</strong></p><p>alert(字符串或变量);<br>看下面的代码:</p><pre><code>&lt;script type=&quot;text/javascript&quot;&gt;   var mynum = 30;   alert(&quot;hello!&quot;);   alert(mynum);&lt;/script&gt;</code></pre><p>注:alert弹出消息对话框(包含一个确定按钮)。</p><p>结果:按顺序弹出消息框</p><p>注意:</p><ol><li><p>在点击对话框”确定”按钮前，不能进行任何其它操作。</p></li><li><p>消息对话框通常可以用于调试程序。</p></li><li><p>alert输出内容，可以是字符串或变量，与document.write 相似。</p></li></ol><h2 id="confirm消息对话框"><a href="#confirm消息对话框" class="headerlink" title="confirm消息对话框"></a>confirm消息对话框</h2><p>onfirm 消息对话框通常用于允许用户做选择的动作，如：“你对吗？”等。弹出对话框(包括一个确定按钮和一个取消按钮)。</p><p><strong>语法:</strong></p><p>confirm(str);<br>参数说明:</p><p>str：在消息对话框中要显示的文本<br>返回值: Boolean值<br>返回值:</p><p>当用户点击”确定”按钮时，返回true<br>当用户点击”取消”按钮时，返回false<br>注: 通过返回值可以判断用户点击了什么按钮</p><p>看下面的代码:</p><pre><code>&lt;script type=&quot;text/javascript&quot;&gt;    var mymessage=confirm(&quot;你喜欢JavaScript吗?&quot;);    if(mymessage==true)    &#123;   document.write(&quot;很好,加油!&quot;);   &#125;    else    &#123;  document.write(&quot;JS功能强大，要学习噢!&quot;);   &#125;&lt;/script&gt;</code></pre><h2 id="promot对话框"><a href="#promot对话框" class="headerlink" title="promot对话框"></a>promot对话框</h2><p>prompt弹出消息对话框,通常用于询问一些需要与用户交互的信息。弹出消息对话框（包含一个确定按钮、取消按钮与一个文本输入框）。</p><p><strong>语法:</strong></p><p>prompt(str1, str2);<br>参数说明：</p><p>str1: 要显示在消息对话框中的文本，不可修改<br>str2：文本框中的内容，可以修改<br>返回值:</p><ol><li>点击确定按钮，文本框中的内容将作为函数返回值</li><li>点击取消按钮，将返回null<br>看看下面代码:</li></ol><pre><code>var myname=prompt(&quot;请输入你的姓名:&quot;);if(myname!=null)  &#123;   alert(&quot;你好&quot;+myname); &#125;else  &#123;  alert(&quot;你好 my friend.&quot;);  &#125;</code></pre><h2 id="打开新窗口"><a href="#打开新窗口" class="headerlink" title="打开新窗口"></a>打开新窗口</h2><p>open() 方法可以查找一个已经存在或者新建的浏览器窗口。</p><p><strong>语法：</strong></p><pre><code>window.open([URL], [窗口名称], [参数字符串])</code></pre><p><strong>参数说明:</strong></p><p>URL：可选参数，在窗口中要显示网页的网址或路径。如果省略这个参数，或者它的值是空字符串，那么窗口就不显示任何文档。<br>窗口名称：可选参数，被打开窗口的名称。<br>    1.该名称由字母、数字和下划线字符组成。<br>    2.”_top”、”_blank”、”_self”具有特殊意义的名称。<br>       _blank：在新窗口显示目标网页<br>       _self：在当前窗口显示目标网页<br>       _top：框架网页中在上部窗口中显示目标网页<br>    3.相同 name 的窗口只能创建一个，要想创建多个窗口则 name 不能相同。<br>   4.name 不能包含有空格。<br>参数字符串：可选参数，设置窗口参数，各参数用逗号隔开。</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/31/9621f92441c7d561518f086bf718c5e9.png" alt=""></p><h2 id="关闭窗口"><a href="#关闭窗口" class="headerlink" title="关闭窗口"></a>关闭窗口</h2><p>close()关闭窗口</p><p>用法：</p><p>window.close();   //关闭本窗口<br>或</p><p>&lt;窗口对象&gt;.close();   //关闭指定的窗口<br>例如:关闭新建的窗口。</p><pre><code>&lt;script type=&quot;text/javascript&quot;&gt;   var mywin=window.open(&#39;http://fazzie-key.cool&#39;); //将新打的窗口对象，存储在变量mywin中   mywin.close();&lt;/script&gt;</code></pre><p>注意:上面代码在打开新窗口的同时，关闭该窗口，看不到被打开的窗口。</p><h1 id="DOM"><a href="#DOM" class="headerlink" title="DOM"></a>DOM</h1><p>文档对象模型DOM（Document Object Model）定义访问和处理HTML文档的标准方法。DOM 将HTML文档呈现为带有元素、属性和文本的树结构（节点树）。</p><p>HTML文档可以说由节点构成的集合，三种常见的DOM节点:</p><ol><li><p>元素节点：上图中<html>、<body>、<p>等都是元素节点，即标签。</p></li><li><p>文本节点:向用户展示的内容，如<code>&lt;li&gt;...&lt;/li&gt;</code>中的JavaScript、DOM、CSS等文本。</p></li><li><p>属性节点:元素属性，如<a>标签的链接属性href=”<a href="http://fazzie-key.cool&quot;。">http://fazzie-key.cool&quot;。</a></p></li></ol><h2 id="通过ID获取元素"><a href="#通过ID获取元素" class="headerlink" title="通过ID获取元素"></a>通过ID获取元素</h2><p>网页由标签将信息组织起来，而标签的id属性值是唯一的，就像是每人有一个身份证号一样，只要通过身份证号就可以找到相对应的人。那么在网页中，我们通过id先找到标签，然后进行操作。</p><p><strong>语法:</strong></p><pre><code> document.getElementById(&quot;id&quot;) </code></pre><blockquote><p>获取的元素是一个对象，如想对元素进行操作，我们要通过它的属性或方法。</p></blockquote><pre><code>&lt; !DOCTYPE HTML&gt;c&lt;html&gt;o &lt;head&gt;&lt;meta http-equiv=”Content-Type”content=&quot; text/html; charset=gb2312” /&gt;&lt;title&gt;获取元素&lt;/title&gt;&lt;script type=&quot; text/ javascript&quot; &gt;var mye = document. getElementById(&quot;con&quot;);//获取元素存储在变量mye中document. write (mye) ;//输出变量mye&lt;/ script&gt;&lt;/head&gt; &lt;body&gt;&lt;h3&gt;Hello&lt;/h3&gt;&lt;p id=&quot;con&quot;&gt;I love JavaScript&lt;/p&gt;&lt;/ body&gt;&lt;/html&gt;</code></pre><h2 id="innerHTML属性"><a href="#innerHTML属性" class="headerlink" title="innerHTML属性"></a>innerHTML属性</h2><p>innerHTML 属性用于获取或替换 HTML 元素的内容。</p><p>语法:</p><p>Object.innerHTML<br>注意:</p><p>1.Object是获取的元素对象，如通过document.getElementById(“ID”)获取的元素。</p><p>2.注意书写，innerHTML区分大小写。</p><p>我们通过id=”con”获取<p> 元素，并将元素的内容输出和改变元素内容，代码如下:</p><pre><code>&lt; !DOCTYPE HTML&gt;(html&gt;&lt;head&gt;&lt;title&gt; innerHTML&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p id=&quot;con&quot;&gt;Hello World!&lt;/p&gt; &lt;script&gt;var mycon= document.getElementById(&quot; con&quot;) ;document. write(&quot;p标签原始内容:”+ mycon. innerHTML+&quot;&lt;br&gt;&quot;);//输入元素内容mycon. innerHTML =&quot;New text!&quot;; //修改p元素内容document. write(&quot; p标签修改后内容:&quot;+ mycon. innerHTML) ;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;</code></pre><h2 id="改变样式"><a href="#改变样式" class="headerlink" title="改变样式"></a>改变样式</h2><p>HTML DOM 允许 JavaScript 改变 HTML 元素的样式。如何改变 HTML 元素的样式呢？</p><p>语法:</p><pre><code>Object.style.property=new style;</code></pre><blockquote><p>Object是获取的元素对象，如通过document.getElementById(“id”)获取的元素。</p></blockquote><ul><li>backgroundColor:背景颜色</li><li>height:高度</li><li>width：宽度</li><li>cplor：文本颜色</li><li>font：字体属性</li><li>fontFamily：字体系列</li><li>fontsize：字体大小</li></ul><blockquote><p>该表只是一小部分CSS样式属性，其它样式也可以通过该方法设置和修改。</p></blockquote><p>看看下面的代码:</p><p>改变 <code>&lt;p&gt;</code> 元素的样式，将颜色改为红色，字号改为20,背景颜色改为蓝：</p><pre><code>&lt;p id=&quot;pcon&quot;&gt;Hello World!&lt;/p&gt;&lt;script&gt;   var mychar = document.getElementById(&quot;pcon&quot;);   mychar.style.color=&quot;red&quot;;   mychar.style.fontSize=&quot;20&quot;;   mychar.style.backgroundColor =&quot;blue&quot;;&lt;/script&gt;</code></pre><h2 id="隐藏和显示"><a href="#隐藏和显示" class="headerlink" title="隐藏和显示"></a>隐藏和显示</h2><p>网页中经常会看到显示和隐藏的效果，可通过display属性来设置。</p><p><strong>语法：</strong></p><pre><code>Object.style.display = value</code></pre><blockquote><p>Object是获取的元素对象，如通过document.getElementById(“id”)获取的元素。</p></blockquote><div class="table-container"><table><thead><tr><th>值</th><th>描述</th></tr></thead><tbody><tr><td>none</td><td>隐藏</td></tr><tr><td>block</td><td>显示</td></tr></tbody></table></div><pre><code>&lt; !DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=&quot; Content- Type&quot; content=&quot; text/html; charset=gb2312&quot;&gt;&lt;title&gt;display&lt;/title&gt;&lt;script type=&quot; text/ javascript&quot; &gt;function hidetext ()&#123;document. getElementById(&quot; con&quot;). style. display=”none&#39; ;&#125;function showtext ()&#123;document. getElementById( &#39;con&#39; ). style. display=&quot;block&quot; ;&#125;&lt;/script&gt;&lt;/ head&gt;&lt;body&gt;&lt;h1&gt; JavaScript&lt;/h1&gt;&lt;p id=&quot; con&quot;&gt;做为一个Web开发师来说，如果你想提供漂亮的网页、令用户满意的上网体验，JavaScript是 必不可少的工具。&lt;/p&gt;&lt; form&gt;&lt;input type=&quot;button&quot; onclick=&quot;hidetext()&quot; value=&quot;不显示段落内容”/&gt;&lt;input type=&quot; button”onclick=&quot; showtext()”value=&quot;显示 段落内容”/&gt;&lt;/ form&gt;&lt;/body&gt;&lt;/html&gt;</code></pre><h2 id="控制类名"><a href="#控制类名" class="headerlink" title="控制类名"></a>控制类名</h2><p>className 属性设置或返回元素的class 属性。</p><p><strong>语法：</strong></p><pre><code>object.className = classname</code></pre><p><strong>作用:</strong></p><p>1.获取元素的class 属性</p><ol><li>为网页内的某个元素指定一个css样式来更改该元素的外观</li></ol><p>看看下面代码，获得 <code>&lt;p&gt;</code> 元素的 class 属性和改变className：</p><pre><code>&lt; !DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=&quot; Content- -Type&quot; content=&quot; text/html; charset=gb2312&quot;&gt;&lt;title&gt;className属性&lt;/title&gt;&lt;style type=&quot; text/css&quot; &gt;input&#123;font-size: 10px;&#125;.one&#123;width: 200px ;background-color :#CCC;&#125;.two &#123;font-size: 18px;color:#F00;&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;p id=&quot; con”class=&quot; one&quot;&gt;JavaScript&lt;/p&gt;&lt;form&gt;&lt;input type=&quot; button” value=&quot;点击更改”onclick=&#39; modifyclass()”/&gt;&lt;/ form&gt;&lt;script type=&quot; text/ javascript&#39; &gt;var mychar= document. getElementById( con&#39; ) ;document. write(&quot; p元素Class值为:”+ mychar. className+&quot;&lt;br&gt;&quot;);//输出p元素Class属性function modifyclass() &#123;mychar. className=&quot; two”; / /改变className &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 《栈溢出工程师》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 前端 </tag>
            
            <tag> JavaScript </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CSS</title>
      <link href="2020/07/28/CSS/"/>
      <url>2020/07/28/CSS/</url>
      
        <content type="html"><![CDATA[<h1 id="CSS基础"><a href="#CSS基础" class="headerlink" title="CSS基础"></a>CSS基础</h1><p>CSS全称为“层叠样式表 (Cascading Style Sheets)”，它主要是用于定义HTML内容在浏览器内的显示样式，如文字大小、颜色、字体加粗等。</p><span id="more"></span><p>如下列代码：</p><pre><code>p&#123;   font-size:12px;   color:red;   font-weight:bold;&#125;</code></pre><p>使用CSS样式的一个好处是通过定义某个样式，可以让不同网页位置的文字有着统一的字体、字号或者颜色等。</p><h2 id="CSS语法"><a href="#CSS语法" class="headerlink" title="CSS语法"></a>CSS语法</h2><p>css 样式由选择符和声明组成，而声明又由属性和值组成。</p><p>选择符：又称选择器，指明网页中要应用样式规则的元素，如本例中是网页中所有的段（p）的文字将变成蓝色，而其他的元素（如ol）不会受到影响。</p><p>声明：在英文大括号“｛｝”中的的就是声明，属性和值之间用英文冒号“：”分隔。当有多条声明时，中间可以英文分号“;”分隔，如下所示：</p><pre><code>p&#123;font-size:12px;color:red;&#125;</code></pre><p>注意：</p><p>1、最后一条声明可以没有分号，但是为了以后修改方便，一般也加上分号。</p><p>2、为了使用样式更加容易阅读，可以将每条代码写在一个新行内，如下所示：</p><pre><code>p&#123;   font-size:12px;   color:red;&#125;</code></pre><h3 id="CSS注释"><a href="#CSS注释" class="headerlink" title="CSS注释"></a>CSS注释</h3><p><code>/*注释语句*/</code></p><h2 id="CSS的插入方式"><a href="#CSS的插入方式" class="headerlink" title="CSS的插入方式"></a>CSS的插入方式</h2><p>？从CSS 样式代码插入的形式来看基本可以分为以下3种：内联式、嵌入式和外部式三种。</p><h3 id="内联式"><a href="#内联式" class="headerlink" title="内联式"></a>内联式</h3><p>内联式css样式表就是把css代码直接写在现有的HTML标签中，如下面代码：</p><pre><code>&lt;p style=&quot;color:red&quot;&gt;这里文字是红色。&lt;/p&gt;</code></pre><p>注意要写在元素的开始标签里，下面这种写法是错误的：</p><pre><code>&lt;p&gt;这里文字是红色。&lt;/p style=&quot;color:red&quot;&gt;</code></pre><p>并且css样式代码要写在style=””双引号中，如果有多条css样式代码设置可以写在一起，中间用分号隔开。如下代码：</p><pre><code>&lt;p style=&quot;color:red;font-size:12px&quot;&gt;这里文字是红色。&lt;/p&gt;</code></pre><p><strong>效果</strong></p><p style="color:red;font-size:12px">这里文字是红色。</p><h3 id="嵌入式"><a href="#嵌入式" class="headerlink" title="嵌入式"></a>嵌入式</h3><p>嵌入式css样式，就是可以把css样式代码写在<code>&lt;style type=&quot;text/css&quot;&gt;&lt;/style&gt;</code>标签之间。如下面代码实现把三个<code>&lt;span&gt;</code>标签中的文字设置为红色：</p><pre><code>&lt;style type=&quot;text/css&quot;&gt;span&#123;color:red;&#125;&lt;/style&gt;</code></pre><p>嵌入式css样式必须写在<code>&lt;style&gt;&lt;/style&gt;</code>之间，并且一般情况下嵌入式css样式写在<code>&lt;head&gt;&lt;/head&gt;</code>之间。如右边编辑器中的代码。</p><h3 id="外部式"><a href="#外部式" class="headerlink" title="外部式"></a>外部式</h3><p>外部式css样式(也可称为外联式)就是把css代码写一个单独的外部文件中，这个css样式文件以“.css”为扩展名，在<code>&lt;head&gt;</code>内（不是在<code>&lt;style&gt;</code>标签内）使用<code>&lt;link&gt;</code>标签将css样式文件链接到HTML文件内，如下面代码：</p><pre><code>&lt;link href=&quot;base.css&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot; /&gt;</code></pre><p>注意：</p><p>1、css样式文件名称以有意义的英文字母命名，如 main.css。</p><p>2、rel=”stylesheet” type=”text/css” 是固定写法不可修改。</p><p>3、<code>&lt;link&gt;</code>标签位置一般写在<code>&lt;head&gt;</code>标签之内。</p><h3 id="三种方式的优先级"><a href="#三种方式的优先级" class="headerlink" title="三种方式的优先级"></a>三种方式的优先级</h3><p>如果有一种情况：对于同一个元素我们同时用了三种方法设置css样式，那么哪种方法真正有效呢？</p><p>1、使用内联式CSS设置文字为粉色。</p><p>2、然后使用嵌入式CSS来设置文字为红色。</p><p>3、最后又使用外部式设置文字为蓝色（style.css文件中设置）。</p><p>但最终你可以观察到文本被设置为了粉色。因为这三种样式是有优先级的，记住他们的优先级：<strong>内联式 &gt; 嵌入式 &gt; 外部式</strong></p><p>但是嵌入式&gt;外部式有一个前提：嵌入式css样式的位置一定在外部式的后面。如<code>&lt;link href=&quot;style.css&quot; ...&gt;代码在&lt;style type=&quot;text/css&quot;&gt;...&lt;/style&gt;</code></p><p>其实总结来说，就是—就近原则（离被设置元素越近优先级别越高。</p><h1 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h1><h2 id="什么是选择器？"><a href="#什么是选择器？" class="headerlink" title="什么是选择器？"></a>什么是选择器？</h2><p>每一条css样式声明（定义）由两部分组成，形式如下：</p><pre><code>选择器&#123;    样式;&#125;</code></pre><p>在{}之前的部分就是“选择器”，“选择器”指明了{}中的“样式”的作用对象，也就是“样式”作用于网页中的哪些元素。</p><p>标签选择器其实就是html代码中的标签。如右侧代码编辑器中的<code>&lt;html&gt;、&lt;body&gt;、&lt;h1&gt;、&lt;p&gt;、&lt;img&gt;</code>。例如下面代码：</p><pre><code>p&#123;font-size:12px;line-height:1.6em;&#125;</code></pre><p>上面的css样式代码的作用：为p标签设置12px字号，行间距设置1.6em的样式。</p><h2 id="类选择器"><a href="#类选择器" class="headerlink" title="类选择器"></a>类选择器</h2><p>类选择器在css样式编码中是最常用到的，如右侧代码编辑器中的代码:可以实现为“胆小如鼠”、“勇气”字体设置为红色。</p><p><strong>语法</strong>：</p><pre><code>.类选器名称&#123;css样式代码;&#125;</code></pre><p>注意：</p><p>1、英文圆点开头</p><p>2、其中类选器名称可以任意起名（但不要起中文噢）</p><p>使用方法：</p><p>第一步：使用合适的标签把要修饰的内容标记起来，如下：</p><pre><code>&lt;span&gt;胆小如鼠&lt;/span&gt;</code></pre><p>第二步：使用class=”类选择器名称”为标签设置一个类，如下：</p><pre><code>&lt;span class=&quot;stress&quot;&gt;胆小如鼠&lt;/span&gt;</code></pre><p>第三步：设置类选器css样式，如下：</p><pre><code>.stress&#123;color:red;&#125;/*类前面要加入一个英文圆点*/</code></pre><h2 id="ID选择器"><a href="#ID选择器" class="headerlink" title="ID选择器"></a>ID选择器</h2><p>1、使用ID选择器，必须给标签添加上id属性，为标签设置id=”ID名称”，而不是class=”类名称”。</p><p>2、ID选择符的前面是井号（#）号，而不是英文圆点（.）。</p><p>3、id属性的值既为当前标签的id，尽量见名思意，语义化。</p><h2 id="ID和类选择器的区别"><a href="#ID和类选择器的区别" class="headerlink" title="ID和类选择器的区别"></a>ID和类选择器的区别</h2><p><strong>相同点</strong>：可以应用于任何元素</p><p><strong>不同点</strong>：</p><p>1、ID选择器只能在文档中使用一次。与类选择器不同，在一个HTML文档中，ID选择器只能使用一次，而且仅一次。而类选择器可以使用多次。</p><p>下面代码是正确的：</p><pre><code> &lt;p&gt;三年级时，我还是一个&lt;span class=&quot;stress&quot;&gt;胆小如鼠&lt;/span&gt;的小女孩， 上课从来不敢回答老师提出的问题，生怕回答错了老师会批评我。就一直没有这个 &lt;span class=&quot;stress&quot;&gt;勇气&lt;/span&gt;来回答老师提出的问题。&lt;/p&gt;</code></pre><p>而下面代码是错误的：</p><pre><code> &lt;p&gt;三年级时，我还是一个&lt;span id=&quot;stress&quot;&gt;胆小如鼠&lt;/span&gt;的小女孩， 上课从来不敢回答老师提出的问题，生怕回答错了老师会批评我。就一直没有这个&lt;span  id=&quot;stress&quot;&gt;勇气&lt;/span&gt;来回答老师提出的问题。&lt;/p&gt;</code></pre><p>2、可以使用类选择器词列表方法为一个元素同时设置多个样式。我们可以为一个元素同时设多个样式，但只可以用类选择器的方法实现，ID选择器是不可以的（不能使用 ID 词列表）。</p><p>下面的代码是正确的</p><pre><code>.stress&#123;    color:red;&#125;.bigsize&#123;    font-size:25px;&#125;&lt;p&gt;到了&lt;span class=&quot;stress bigsize&quot;&gt;三年级&lt;/span&gt;下学期时，我们班上了一节公开课...&lt;/p&gt;</code></pre><p>上面代码的作用是为“三年级”三个文字设置文本颜色为红色并且字号为25px。</p><p>下面的代码是不正确的</p><pre><code>#stressid&#123;    color:red;&#125;#bigsizeid&#123;    font-size:25px;&#125;&lt;p&gt;到了&lt;span id=&quot;stressid bigsizeid&quot;&gt;三年级&lt;/span&gt;下学期时，我们班上了一节公开课...&lt;/p&gt;</code></pre><p>上面代码不可以实现为“三年级”三个文字设置文本颜色为红色并且字号为25px的作用。</p><h2 id="子选择器"><a href="#子选择器" class="headerlink" title="子选择器"></a>子选择器</h2><p>还有一个比较有用的选择器子选择器，即大于符号(&gt;),用于选择指定标签元素的第一代子元素。如右侧代码编辑器中的代码：</p><pre><code>.food&gt;li&#123;border:1px solid red;&#125;</code></pre><p>这行代码会使class名为food下的子元素li（水果、蔬菜）加入红色实线边框。</p><h2 id="后代选择器"><a href="#后代选择器" class="headerlink" title="后代选择器"></a>后代选择器</h2><p>包含选择器，即加入空格,用于选择指定标签元素下的后辈元素。</p><pre><code>.first  span&#123;color:red;&#125;</code></pre><p>请注意这个选择器与子选择器的区别，子选择器（child selector）仅是指它的直接后代，或者你可以理解为作用于子元素的第一代后代。而后代选择器是作用于所有子后代元素。后代选择器通过空格来进行选择，而子选择器是通过“&gt;”进行选择。</p><blockquote><p>总结：&gt;作用于元素的第一代后代，空格作用于元素的所有后代。</p></blockquote><h2 id="通用选择器"><a href="#通用选择器" class="headerlink" title="通用选择器"></a>通用选择器</h2><p>通用选择器是功能最强大的选择器，它使用一个（*）号指定，它的作用是匹配html中所有标签元素，如下使用下面代码使用html中任意标签元素字体颜色全部设置为红色：</p><pre><code>* &#123;color:red;&#125;</code></pre><h2 id="伪选择器"><a href="#伪选择器" class="headerlink" title="伪选择器"></a>伪选择器</h2><p>更有趣的是伪类选择符，为什么叫做伪类选择符，它允许给html不存在的标签（标签的某种状态）设置样式，比如说我们给html中一个标签元素的鼠标滑过的状态来设置字体颜色：</p><pre><code>a:hover&#123;color:red;&#125;</code></pre><p>上面一行代码就是为 a 标签鼠标滑过的状态设置字体颜色变红。这样就会使第一段文字内容中的“胆小如鼠”文字加入鼠标滑过字体颜色变为红色特效。</p><h2 id="分组选择器"><a href="#分组选择器" class="headerlink" title="分组选择器"></a>分组选择器</h2><p>当你想为html中多个标签元素设置同一个样式时，可以使用分组选择符（，），如下代码为右侧代码编辑器中的h1、span标签同时设置字体颜色为红色：</p><pre><code>h1,span&#123;color:red;&#125;</code></pre><p>它相当于下面两行代码：</p><pre><code>h1&#123;color:red;&#125;span&#123;color:red;&#125;</code></pre><h2 id="CSS的继承"><a href="#CSS的继承" class="headerlink" title="CSS的继承"></a>CSS的继承</h2><p>CSS的某些样式是具有继承性的，那么什么是继承呢？继承是一种规则，它允许样式不仅应用于某个特定html标签元素，而且应用于其后代。比如下面代码：如某种颜色应用于p标签，这个颜色设置不仅应用p标签，还应用于p标签中的所有子元素文本，这里子元素为span标签。</p><pre><code>p&#123;color:red;&#125;&lt;p&gt;三年级时，我还是一个&lt;span&gt;胆小如鼠&lt;/span&gt;的小女孩。&lt;/p&gt;</code></pre><p>可见右侧结果窗口中p中的文本与span中的文本都设置为了红色。但注意有一些css样式是不具有继承性的。如border:1px solid red;</p><pre><code>p&#123;border:1px solid red;&#125;&lt;p&gt;三年级时，我还是一个&lt;span&gt;胆小如鼠&lt;/span&gt;的小女孩。&lt;/p&gt;</code></pre><p>在上面例子中它代码的作用只是给p标签设置了边框为1像素、红色、实心边框线，而对于子元素span是没用起到作用的。</p><h2 id="选择器优先级"><a href="#选择器优先级" class="headerlink" title="选择器优先级"></a>选择器优先级</h2><p>1、如果一个元素使用了多个选择器,则会按照选择器的优先级来给定样式。</p><p>2、选择器的优先级依次是: 内联样式 &gt; id选择器 &gt; 类选择器 &gt; 标签选择器 &gt; 通配符选择器</p><h2 id="标签权值"><a href="#标签权值" class="headerlink" title="标签权值"></a>标签权值</h2><p>有的时候我们为同一个元素设置了不同的CSS样式代码，那么元素会启用哪一个CSS样式呢？下面我们一起来看一下代码：</p><pre><code>p&#123;color:red;&#125;.first&#123;color:green;&#125;&lt;p class=&quot;first&quot;&gt;三年级时，我还是一个&lt;span&gt;胆小如鼠&lt;/span&gt;的小女孩。&lt;/p&gt;</code></pre><p>p和.first都匹配到了p这个标签上，那么会显示哪种颜色呢？green是正确的颜色，那么为什么呢？是因为浏览器是根据权值来判断使用哪种css样式的，权值高的就使用哪种css样式。</p><p>下面是权值的规则：</p><p>标签的权值为1，类选择符的权值为10，ID选择符的权值最高为100。例如下面的代码：</p><pre><code>p&#123;color:red;&#125; /*权值为1*/p span&#123;color:green;&#125; /*权值为1+1=2*/.warning&#123;color:white;&#125; /*权值为10*/p span.warning&#123;color:purple;&#125; /*权值为1+1+10=12*/#footer .note p&#123;color:yellow;&#125; /*权值为100+10+1=111*/</code></pre><p>注意：还有一个权值比较特殊—继承也有权值但很低，有的文献提出它只有0.1，所以可以理解为继承的权值最低。</p><h2 id="最高优先级"><a href="#最高优先级" class="headerlink" title="最高优先级"></a>最高优先级</h2><p>我们在做网页代码的时，有些特殊的情况需要为某些样式设置具有最高权值，怎么办？这时候我们可以使用!important来解决。</p><p>如下代码：</p><pre><code>p&#123;color:red!important;&#125;p&#123;color:green;&#125;&lt;p class=&quot;first&quot;&gt;三年级时，我还是一个&lt;span&gt;胆小如鼠&lt;/span&gt;的小女孩。&lt;/p&gt;这时 p 段落中的文本会显示的red红色。</code></pre><p>注意：<strong>!important</strong>要写在分号的前面</p><p>这里注意当网页制作者不设置css样式时，浏览器会按照自己的一套样式来显示网页。并且用户也可以在浏览器中设置自己习惯的样式，比如有的用户习惯把字号设置为大一些，使其查看网页的文本更加清楚。这时注意样式优先级为：<strong>浏览器默认的样式 &lt; 网页制作者样式 &lt; 用户自己设置的样式</strong>，但记住!important优先级样式是个例外，权值高于用户自己设置的样式。</p><h1 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h1><h2 id="更换字体"><a href="#更换字体" class="headerlink" title="更换字体"></a>更换字体</h2><p>我们可以使用css样式为网页中的文字设置字体、字号、颜色等样式属性。下面我们来看一个例子，下面代码实现：为网页中的文字设置字体为宋体。</p><pre><code>body&#123;font-family:&quot;宋体&quot;;&#125;</code></pre><p>这里注意不要设置不常用的字体，因为如果用户本地电脑上如果没有安装你设置的字体，就会显示浏览器默认的字体。（因为用户是否可以看到你设置的字体样式取决于用户本地电脑上是否安装你设置的字体。）<br>现在一般网页喜欢设置“微软雅黑”，如下代码：</p><pre><code>body&#123;font-family:&quot;Microsoft Yahei&quot;;&#125;</code></pre><p>或</p><pre><code>body&#123;font-family:&quot;微软雅黑&quot;;&#125;</code></pre><p>注意：第一种方法比第二种方法兼容性更好一些。</p><h2 id="字体大小、加粗、倾斜"><a href="#字体大小、加粗、倾斜" class="headerlink" title="字体大小、加粗、倾斜"></a>字体大小、加粗、倾斜</h2><p>可以使用下面代码设置网页中文字的字号为12像素：</p><pre><code>body&#123;font-size:12px;&#125;</code></pre><p>我们还可以使用css样式来改变文字的样式：粗体、斜体、下划线、删除线，可以使用下面代码实现设置文字以粗体样式显示出来。</p><pre><code>p span&#123;font-weight:bold;&#125;</code></pre><p>1、font-style可以设置字体样式，并且有种3设置方式。</p><p>2、正常字体为normal,也是font-style的默认值。</p><p>3、italic为设置字体为斜体，用于字体本身就有倾斜的样式。</p><p>4、oblique为设置倾斜的字体，强制将字体倾斜。</p><pre><code>p&#123;font-style:italic&#125;</code></pre><h2 id="字体颜色"><a href="#字体颜色" class="headerlink" title="字体颜色"></a>字体颜色</h2><p>技术点的解释：</p><p>1、color属性可以设置字体颜色。</p><p>2、color的值有3种设置方式：</p><p>英文命令颜色</p><pre><code>p&#123;color:red;&#125;</code></pre><p>RGB颜色<br>这个与 photoshop 中的 RGB 颜色是一致的，由 R(red)、G(green)、B(blue) 三种颜色的比例来配色。</p><pre><code>p&#123;color:rgb(133,45,200);&#125;</code></pre><p>每一项的值可以是 0~255 之间的整数，也可以是 0%~100% 的百分数。如：</p><pre><code>p&#123;color:rgb(20%,33%,25%);&#125;</code></pre><p>十六进制颜色<br>这种颜色设置方法是现在比较普遍使用的方法，其原理其实也是 RGB 设置，但是其每一项的值由 0-255 变成了十六进制 00-ff。</p><pre><code>p&#123;color:#00ffff;&#125;</code></pre><h2 id="font字体简写"><a href="#font字体简写" class="headerlink" title="font字体简写"></a>font字体简写</h2><p>网页中的字体css样式代码也有他自己的缩写方式，下面是给网页设置字体的代码：</p><pre><code>body&#123;    font-style:italic;    font-weight:bold;     font-size:12px;     line-height:1.5em;     font-family:&quot;宋体&quot;,sans-serif;&#125;</code></pre><p>这么多行的代码其实可以缩写为一句：</p><pre><code>body&#123;    font:italic  bold  12px/1.5em  &quot;宋体&quot;,sans-serif;&#125;</code></pre><p>注意：</p><p>1、使用这一简写方式你至少要指定 font-size 和 font-family 属性，其他的属性(如 font-weight、font-style、font-variant、line-height)如未指定将自动使用默认值。</p><p>2、在缩写时 font-size 与 line-height 中间要加入“/”斜扛。</p><p>一般情况下因为对于中文网站，英文还是比较少的，所以下面缩写代码比较常用：</p><pre><code>body&#123;    font:12px/1.5em  &quot;宋体&quot;,sans-serif;&#125;</code></pre><p>只是有字号、行间距、中文字体、英文字体设置。</p><h1 id="文本样式"><a href="#文本样式" class="headerlink" title="文本样式"></a>文本样式</h1><h2 id="划线"><a href="#划线" class="headerlink" title="划线"></a>划线</h2><p>1、text-decoration可以设置添加到文本的修饰。</p><p>2、text-decoration默认值为none, 定义标准的文本。</p><p>3、text-decoration的值为underline为定义文本下的一条线。</p><p>4、text-decoration的值为overline为定义文本上的一条线。</p><p>5、text-decoration的值为line-through为定义穿过文本下的一条线，一般用于商品折扣价。</p><h2 id="首行缩进"><a href="#首行缩进" class="headerlink" title="首行缩进"></a>首行缩进</h2><p><strong>语法</strong></p><pre><code>p&#123;text-indent:2em;&#125;</code></pre><h2 id="行间距"><a href="#行间距" class="headerlink" title="行间距"></a>行间距</h2><pre><code>p&#123;line-height:1.5em;&#125;</code></pre><h2 id="字符间距"><a href="#字符间距" class="headerlink" title="字符间距"></a>字符间距</h2><p>中文字间隔、字母间隔设置：</p><p>如果想在网页排版中设置文字间隔或者字母间隔就可以使用    letter-spacing 来实现，如下面代码：</p><pre><code>h1&#123;    letter-spacing:50px;&#125;...&lt;h1&gt;了不起的盖茨比&lt;/h1&gt;</code></pre><p>注意：这个样式使用在英文单词时，是设置字母与字母之间的间距。</p><p>单词间距设置：</p><p>如果我想设置英文单词之间的间距呢？可以使用 word-spacing 来实现。如下代码：</p><pre><code>h1&#123;    word-spacing:50px;&#125;...&lt;h1&gt;welcome to imooc!&lt;/h1&gt;</code></pre><h2 id="排列方式"><a href="#排列方式" class="headerlink" title="排列方式"></a>排列方式</h2><p>想为块状元素中的文本、图片设置居中样式，可以使用text-align样式代码，如下代码可实现文本居中显示。</p><pre><code>h1&#123;    text-align:center;&#125;&lt;h1&gt;了不起的盖茨比&lt;/h1&gt;</code></pre><p>同样可以设置居左：</p><pre><code>h1&#123;    text-align:left;&#125;&lt;h1&gt;了不起的盖茨比&lt;/h1&gt;</code></pre><p>还可以设置居右：</p><pre><code>h1&#123;    text-align:right;&#125;&lt;h1&gt;了不起的盖茨比&lt;/h1&gt;</code></pre><h2 id="长度值"><a href="#长度值" class="headerlink" title="长度值"></a>长度值</h2><p>长度单位总结一下，目前比较常用到px（像素）、em、% 百分比，要注意其实这三种单位都是相对单位。</p><p>1、像素</p><p>像素为什么是相对单位呢？因为像素指的是显示器上的小点（CSS规范中假设“90像素=1英寸”）。实际情况是浏览器会使用显示器的实际像素值有关，在目前大多数的设计者都倾向于使用像素（px）作为单位。</p><p>2、em</p><p>就是本元素给定字体的 font-size 值，如果元素的 font-size 为 14px ，那么 1em = 14px；如果 font-size 为 18px，那么 1em = 18px。如下代码：</p><pre><code>p&#123;font-size:12px;text-indent:2em;&#125;</code></pre><p>上面代码就是可以实现段落首行缩进 24px（也就是两个字体大小的距离）。</p><p>下面注意一个特殊情况：</p><p>但当给 font-size 设置单位为 em 时，此时计算的标准以 p 的父元素的 font-size 为基础。如下代码：</p><p>html:</p><pre><code>&lt;p&gt;以这个&lt;span&gt;例子&lt;/span&gt;为例。&lt;/p&gt;</code></pre><p>css:</p><pre><code>p&#123;font-size:14px&#125;span&#123;font-size:0.8em;&#125;</code></pre><p>结果 span 中的字体“例子”字体大小就为 11.2px（14 * 0.8 = 11.2px）。</p><p>3、百分比</p><pre><code>p&#123;font-size:12px;line-height:130%&#125;</code></pre><p>设置行高（行间距）为字体的130%（12 * 1.3 = 15.6px）。</p><h1 id="CSS盒模型"><a href="#CSS盒模型" class="headerlink" title="CSS盒模型"></a>CSS盒模型</h1><h2 id="元素分类"><a href="#元素分类" class="headerlink" title="元素分类"></a>元素分类</h2><p>在CSS中，html中的标签元素大体被分为三种不同的类型：块状元素、内联元素(又叫行内元素)和内联块状元素。</p><p>常用的块状元素有：</p><p><code>&lt;div&gt;、&lt;p&gt;、&lt;h1&gt;...&lt;h6&gt;、&lt;ol&gt;、&lt;ul&gt;、&lt;dl&gt;、&lt;table&gt;、&lt;address&gt;、&lt;blockquote&gt; 、&lt;form&gt;</code></p><p>常用的内联元素有：</p><p><code>&lt;a&gt;、&lt;span&gt;、&lt;br&gt;、&lt;i&gt;、&lt;em&gt;、&lt;strong&gt;、&lt;label&gt;、&lt;q&gt;、&lt;var&gt;、&lt;cite&gt;、&lt;code&gt;</code></p><p>常用的内联块状元素有：</p><p><code>&lt;img&gt;、&lt;input&gt;</code></p><h2 id="块元素"><a href="#块元素" class="headerlink" title="块元素"></a>块元素</h2><p>什么是块级元素？在html中<code>&lt;div&gt;、 &lt;p&gt;、&lt;h1&gt;、&lt;form&gt;、&lt;ul&gt;</code>和 <code>&lt;li&gt;</code>就是块级元素。设置display:block就是将元素显示为块级元素。如下代码就是将内联元素a转换为块状元素，从而使a元素具有块状元素特点。</p><p><code>a&#123;display:block;&#125;</code><br>块级元素特点：</p><p>1、每个块级元素都从新的一行开始，并且其后的元素也另起一行。</p><p>2、元素的高度、宽度、行高以及顶和底边距都可设置。</p><p>3、元素宽度在不设置的情况下，是它本身父容器的100%（和父元素的宽度一致），除非设定一个宽度。</p><h2 id="内联元素"><a href="#内联元素" class="headerlink" title="内联元素"></a>内联元素</h2><p>在html中，<code>&lt;span&gt;、&lt;a&gt;、&lt;label&gt;、 &lt;strong&gt;</code> 和<code>&lt;em&gt;</code>就是典型的内联元素（行内元素）（inline）元素。当然块状元素也可以通过代码display:inline将元素设置为内联元素。如下代码就是将块状元素div转换为内联元素，从而使 div 元素具有内联元素特点。</p><pre><code> div&#123;     display:inline; &#125;......&lt;div&gt;我要变成内联元素&lt;/div&gt;内联元素特点：</code></pre><p>1、和其他元素都在一行上；</p><p>2、元素的高度、宽度及顶部和底部边距不可设置；</p><p>3、元素的宽度就是它包含的文字或图片的宽度，不可改变。</p><h2 id="内联块状元素"><a href="#内联块状元素" class="headerlink" title="内联块状元素"></a>内联块状元素</h2><p>内联块状元素（inline-block）就是同时具备内联元素、块状元素的特点，代码display:inline-block就是将元素设置为内联块状元素。(css2.1新增)，<code>&lt;img&gt;、&lt;input&gt;</code>标签就是这种内联块状标签。</p><p>inline-block 元素特点：</p><p>1、和其他元素都在一行上；</p><p>2、元素的高度、宽度、行高以及顶和底边距都可设置。</p><h2 id="元素隐藏"><a href="#元素隐藏" class="headerlink" title="元素隐藏"></a>元素隐藏</h2><p>none设置此元素不会被显示，当想要元素隐藏的时候可以使用此值。</p><h2 id="盒子模型长宽"><a href="#盒子模型长宽" class="headerlink" title="盒子模型长宽"></a>盒子模型长宽</h2><p>盒模型宽度和高度和我们平常所说的物体的宽度和高度理解是不一样的，css内定义的宽（width）和高（height），指的是填充以里的内容范围。</p><p>因此一个元素实际宽度（盒子的宽度）=左边界+左边框+左填充+内容宽度+右填充+右边框+右边界。<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/28/50582b4bd6adf5aa7daf256cb59cb258.png" alt=""></p><p>元素的高度也是同理。</p><p>比如：</p><p>css代码：</p><pre><code>div&#123;    width:200px;    padding:20px;    border:1px solid red;    margin:10px;    &#125;</code></pre><p>html代码：</p><pre><code>&lt;body&gt;   &lt;div&gt;文本内容&lt;/div&gt;&lt;/body&gt;</code></pre><p>元素的实际长度为：10px+1px+20px+200px+20px+1px+10px=262px。在chrome浏览器下可查看元素盒模型，如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/28/443afb8d1153a2a42ea2552f6f477c0b.png" alt=""></p><h2 id="设置背景色"><a href="#设置背景色" class="headerlink" title="设置背景色"></a>设置背景色</h2><p>网页中的标签不论是行内元素还是块状元素都可以给它设置一个背景色。</p><p>为标签设置背景颜色可以使background-color:颜色值来实现。</p><p>例子如下：</p><pre><code>div&#123;background-color:red;&#125;//为块状元素设置a&#123;background-color:green;&#125;//为行内元素设置</code></pre><h2 id="盒子边框"><a href="#盒子边框" class="headerlink" title="盒子边框"></a>盒子边框</h2><p>盒子模型的边框就是围绕着内容及补白的线，这条线你可以设置它的粗细、样式和颜色(边框三个属性)。</p><p>如下面代码为 div 来设置边框粗细为 2px、样式为实心的、颜色为红色的边框：</p><pre><code>div&#123;    border:2px  solid  red;&#125;</code></pre><p>上面是 border 代码的缩写形式，可以分开写：</p><pre><code>div&#123;    border-width:2px;    border-style:solid;    border-color:red;&#125;</code></pre><p>注意：</p><p>1、border-style（边框样式）常见样式有：</p><p>dashed（虚线）| dotted（点线）| solid（实线）。</p><p>2、border-color（边框颜色）中的颜色可设置为十六进制颜色，如:</p><pre><code>border-color:#888;//前面的井号不要忘掉。</code></pre><p>3、border-width（边框宽度）中的宽度也可以设置为：</p><p>thin | medium | thick（但不是很常用），最常还是用像素（px）。</p><h3 id="特定边框"><a href="#特定边框" class="headerlink" title="特定边框"></a>特定边框</h3><p>现在有一个问题，如果有想为 p 标签单独设置下边框，而其它三边都不设置边框样式怎么办呢？css 样式中允许只为一个方向的边框设置样式：</p><pre><code>div&#123;border-bottom:1px solid red;&#125;</code></pre><p>同样可以使用下面代码实现其它三边(上、右、左)边框的设置：</p><pre><code>border-top:1px solid red;border-right:1px solid red; border-left:1px solid red;</code></pre><h2 id="加入圆角"><a href="#加入圆角" class="headerlink" title="加入圆角"></a>加入圆角</h2><p>元素边框的圆角效果可以使用border-radius属性来设置。圆角可分为左上、右上、右下、左下。如下代码：</p><pre><code> div&#123;border-radius: 20px 10px 15px 30px;&#125;</code></pre><p>也可以分开写：</p><pre><code>div&#123;    border-top-left-radius: 20px;   border-top-right-radius: 10px;   border-bottom-right-radius: 15px;   border-bottom-left-radius: 30px;&#125;</code></pre><p>如果四个圆角都为10px;可以这么写：</p><pre><code>div&#123; border-radius:10px;&#125;</code></pre><p>如果左上角和右下角圆角效果一样为10px，右上角和左下角圆角一样为20px，可以这么写：</p><pre><code>div&#123; border-radius:10px 20px;&#125;</code></pre><p>需要特别注意的：一个正方形，当设置圆角效果值为元素宽度一半时，显示效果为圆形。例如：</p><pre><code> div &#123;        width: 200px;        height: 200px;        border: 5px solid red;        border-radius: 100px;    &#125;</code></pre><p>也可以写为百分比50%</p><pre><code> div &#123;        width: 200px;        height: 200px;        border: 5px solid red;        border-radius: 100px;    &#125;</code></pre><h2 id="设置内边距"><a href="#设置内边距" class="headerlink" title="设置内边距"></a>设置内边距</h2><p>元素内容与边框之间是可以设置距离的，称之为“内边距（填充）”。填充也可分为上、右、下、左(顺时针)。如下代码：</p><pre><code>div&#123;padding:20px 10px 15px 30px;&#125;</code></pre><p>顺序一定不要搞混。可以分开写上面代码：</p><pre><code>div&#123;   padding-top:20px;   padding-right:10px;   padding-bottom:15px;   padding-left:30px;&#125;</code></pre><p>如果上、右、下、左的填充都为10px;可以这么写</p><pre><code>div&#123;padding:10px;&#125;</code></pre><p>如果上下填充一样为10px，左右一样为20px，可以这么写：</p><pre><code>div&#123;padding:10px 20px;&#125;</code></pre><h2 id="外边距"><a href="#外边距" class="headerlink" title="外边距"></a>外边距</h2><p>元素与其它元素之间的距离可以使用边界（margin）来设置。边界也是可分为上、右、下、左。如下代码：</p><pre><code>div&#123;margin:20px 10px 15px 30px;&#125;</code></pre><p>也可以分开写：</p><pre><code>div&#123;   margin-top:20px;   margin-right:10px;   margin-bottom:15px;   margin-left:30px;&#125;</code></pre><p>如果上右下左的边界都为10px;可以这么写：</p><pre><code>div&#123; margin:10px;&#125;</code></pre><p>如果上下边界一样为10px，左右一样为20px，可以这么写：</p><pre><code>div&#123; margin:10px 20px;&#125;</code></pre><p>总结一下：padding和margin的区别，padding在边框里，margin在边框外。</p><h1 id="CSS布局模型"><a href="#CSS布局模型" class="headerlink" title="CSS布局模型"></a>CSS布局模型</h1><p>布局模型与盒模型一样都是 CSS3 最基本、 最核心的概念。 但布局模型是建立在盒模型基础之上，又不同于我们常说的 CSS3 布局样式或 CSS3 布局模板。如果说布局模型是本，那么 CSS3 布局模板就是末了，是外在的表现形式。<br>CSS3包含3种基本的布局模型，用英文概括为：Flow、Layer 和 Float。<br>在网页中，元素有三种布局模型：</p><p>1、流动模型（Flow）</p><p>2、浮动模型 (Float)</p><p>3、层模型（Layer）</p><h2 id="流动模型"><a href="#流动模型" class="headerlink" title="流动模型"></a>流动模型</h2><p>流动（Flow）是默认的网页布局模式。也就是说网页在默认状态下的 HTML 网页元素都是根据流动模型来分布网页内容的。</p><p>流动布局模型具有2个比较典型的特征：</p><p>第一点，块状元素都会在所处的包含元素内自上而下按顺序垂直延伸分布，因为在默认状态下，块状元素的宽度都为100%。实际上，块状元素都会以行的形式占据位置。如右侧代码编辑器中三个块状元素标签(div，h1，p)宽度显示为100%。</p><p>第二点，在流动模型下，内联元素都会在所处的包含元素内从左到右水平分布显示。</p><h2 id="浮动模型"><a href="#浮动模型" class="headerlink" title="浮动模型"></a>浮动模型</h2><p>任何元素在默认情况下是不能浮动的，但可以用 CSS 定义为浮动，如 div、p、table、img 等元素都可以被定义为浮动。如下代码可以实现两个 div 元素一行显示。</p><pre><code>div&#123;    width:200px;    height:200px;    border:2px red solid;    float:left;&#125;</code></pre><p>当然你也可以同时设置两个元素右浮动也可以实现一行显示。</p><pre><code>div&#123;    width:200px;    height:200px;    border:2px red solid;    float:right;&#125;</code></pre><p>两个元素一左一右可以实现一行显示吗？</p><pre><code>div&#123;    width:200px;    height:200px;    border:2px red solid;&#125;</code></pre><h2 id="层模型"><a href="#层模型" class="headerlink" title="层模型"></a>层模型</h2><p>什么是层布局模型？层布局模型就像是图像软件PhotoShop中非常流行的图层编辑功能一样，每个图层能够精确定位操作，但在网页设计领域，由于网页大小的活动性，层布局没能受到热捧。但是在网页上局部使用层布局还是有其方便之处的。</p><p>如何让html元素在网页中精确定位，就像图像软件PhotoShop中的图层一样可以对每个图层能够精确定位操作。CSS定义了一组定位（positioning）属性来支持层布局模型。</p><p>层模型有三种形式：</p><p>1、绝对定位(position: absolute)</p><p>2、相对定位(position: relative)</p><p>3、固定定位(position: fixed)</p><h3 id="绝对定位"><a href="#绝对定位" class="headerlink" title="绝对定位"></a>绝对定位</h3><p>如果想为元素设置层模型中的绝对定位，需要设置position:absolute(表示绝对定位)，这条语句的作用将元素从文档流中拖出来，然后使用left、right、top、bottom属性相对于其最接近的一个具有定位属性的父包含块进行绝对定位。如果不存在这样的包含块，则相对于body元素，即相对于浏览器窗口。</p><p>如下面代码可以实现div元素相对于浏览器窗口向右移动100px，向下移动50px。</p><pre><code>div&#123;    width:200px;    height:200px;    border:2px red solid;    position:absolute;    left:100px;    top:50px;&#125;&lt;div id=&quot;div1&quot;&gt;&lt;/div&gt;</code></pre><h2 id="相对定位"><a href="#相对定位" class="headerlink" title="相对定位"></a>相对定位</h2><pre><code>如果想为元素设置层模型中的相对定位，需要设置position:relative（表示相对定位），它通过left、right、top、bottom属性确定元素在正常文档流中的偏移位置。相对定位完成的过程是首先按static(float)方式生成一个元素(并且元素像层一样浮动了起来)，然后相对于以前的位置移动，移动的方向和幅度由left、right、top、bottom属性确定，偏移前的位置保留不动。</code></pre><p>如下代码实现相对于以前位置向下移动50px，向右移动100px;</p><pre><code>#div1&#123;    width:200px;    height:200px;    border:2px red solid;    position:relative;    left:100px;    top:50px;&#125;&lt;div id=&quot;div1&quot;&gt;&lt;/div&gt;</code></pre><h3 id="固定定位"><a href="#固定定位" class="headerlink" title="固定定位"></a>固定定位</h3><p>fixed：表示固定定位，与absolute定位类型类似，但它的相对移动的坐标是视图（屏幕内的网页窗口）本身。由于视图本身是固定的，它不会随浏览器窗口的滚动条滚动而变化，除非你在屏幕中移动浏览器窗口的屏幕位置，或改变浏览器窗口的显示大小，因此固定定位的元素会始终位于浏览器窗口内视图的某个位置，不会受文档流动影响，这与background-attachment:fixed;属性功能相同。以下代码可以实现相对于浏览器视图向右移动100px，向下移动50px。并且拖动滚动条时位置固定不变。</p><pre><code>#div1&#123;    width:200px;    height:200px;    border:2px red solid;    position:fixed;    left:100px;    top:50px;&#125;</code></pre><h2 id="Relative与Absolute组合使用"><a href="#Relative与Absolute组合使用" class="headerlink" title="Relative与Absolute组合使用"></a>Relative与Absolute组合使用</h2><p>1、参照定位的元素必须是相对定位元素的前辈元素：</p><pre><code>&lt;div id=&quot;box1&quot;&gt;&lt;!--参照定位的元素--&gt;    &lt;div id=&quot;box2&quot;&gt;相对参照元素进行定位&lt;/div&gt;&lt;!--相对定位元素--&gt;&lt;/div&gt;</code></pre><p>从上面代码可以看出box1是box2的父元素（父元素当然也是前辈元素了）。</p><p>2、参照定位的元素必须加入position:relative;</p><pre><code>#box1&#123;    width:200px;    height:200px;    position:relative;        &#125;</code></pre><p>3、定位元素加入position:absolute，便可以使用top、bottom、left、right来进行偏移定位了。</p><pre><code>#box2&#123;    position:absolute;    top:20px;    left:30px;         &#125;</code></pre><p>这样box2就可以相对于父元素box1定位了（这里注意参照物就可以不是浏览器了，而可以自由设置了）。</p><h1 id="弹性盒模型"><a href="#弹性盒模型" class="headerlink" title="弹性盒模型"></a>弹性盒模型</h1><pre><code>&lt;style type=&quot;text/css&quot;&gt;    .box &#123;        background: blue;        display: flex;    &#125;    .box div &#123;        width: 200px;        height: 200px;    &#125;    .box1 &#123;        background: red;    &#125;    .box2 &#123;        background: orange;    &#125;    .box3 &#123;        background: green;    &#125;    &lt;/style&gt;&lt;/head&gt;&lt;body&gt;    &lt;div class=&quot;box&quot;&gt;        &lt;div class=&quot;box1&quot;&gt;&lt;/div&gt;        &lt;div class=&quot;box2&quot;&gt;&lt;/div&gt;        &lt;div class=&quot;box3&quot;&gt;&lt;/div&gt;    &lt;/div&gt;&lt;/body&gt;</code></pre><p>上面的代码：</p><p>三个块元素设置大小以及背景色，在父容器中添加flex。</p><p>技术点的解释：</p><p>1、设置display: flex属性可以把块级元素在一排显示。</p><p>2、flex需要添加在父元素上，改变子元素的排列顺序。</p><p>3、默认为从左往右依次排列,且和父元素左边没有间隙。</p><h2 id="横轴排列"><a href="#横轴排列" class="headerlink" title="横轴排列"></a>横轴排列</h2><p>justify-content属性，本属性定义了项目在主轴上的对齐方式。结合上一节的布局例子进行理解，属性值分别为：</p><pre><code> justify-content: flex-start | flex-end | center | space-between | space-around;</code></pre><p>flex-start：交叉轴的起点对齐</p><pre><code> .box &#123;        background: blue;        display: flex;        justify-content: flex-start;    &#125;</code></pre><p>flex-end：右对齐</p><pre><code> .box &#123;        background: blue;        display: flex;        justify-content: flex-end;    &#125;</code></pre><p>center： 居中</p><pre><code> .box &#123;        background: blue;        display: flex;        justify-content: center;    &#125;</code></pre><p>space-between：两端对齐，项目之间的间隔都相等。</p><pre><code> .box &#123;        background: blue;        display: flex;        justify-content: space-between;    &#125;</code></pre><p>space-around：每个项目两侧的间隔相等。所以，项目之间的间隔比项目与边框的间隔大一倍。</p><pre><code>.box &#123;        background: blue;        display: flex;        justify-content: space-around;    &#125;</code></pre><h2 id="竖轴"><a href="#竖轴" class="headerlink" title="竖轴"></a>竖轴</h2><p>align-items属性定义了项目在交叉轴上的对齐方式。属性值分别为：</p><p>align-items: flex-start | flex-end | center | baseline | stretch;<br>结合右侧编辑器中的布局以及下面的样式设置进行理解：</p><p>flex-start：默认值，左对齐</p><pre><code>   .box &#123;        height: 700px;        background: blue;        display: flex;        align-items: flex-start;    &#125;</code></pre><p>flex-end：交叉轴的终点对齐</p><pre><code> .box &#123;        height: 700px;        background: blue;        display: flex;        align-items: flex-end;    &#125;</code></pre><p>center： 交叉轴的中点对齐</p><pre><code>.box &#123;        height: 700px;        background: blue;        display: flex;        align-items: center;    &#125;</code></pre><p>baseline：项目的第一行文字的基线对齐。</p><pre><code>.box &#123;        height: 700px;        background: blue;        display: flex;        align-items: baseline;    &#125;</code></pre><p>三个盒子中设置不同的字体大小，可以参考右侧编辑器中的代码进行测试。</p><p>stretch（默认值）：如果项目未设置高度或设为auto，将占满整个容器的高度。</p><pre><code> .box &#123;        height: 300px;        background: blue;        display: flex;        align-items: stretch;    &#125;.box div &#123;    /*不设置高度，元素在垂直方向上铺满父容器*/    width: 200px;&#125;</code></pre><h2 id="子元素占比"><a href="#子元素占比" class="headerlink" title="子元素占比"></a>子元素占比</h2><p>1、给子元素设置flex属性,可以设置子元素相对于父元素的占比。</p><p>2、flex属性的值只能是正整数,表示占比多少。</p><p>3、给子元素设置了flex之后,其宽度属性会失效。</p>]]></content>
      
      
      <categories>
          
          <category> 《栈溢出工程师》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CSS </tag>
            
            <tag> 前端 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HTML</title>
      <link href="2020/07/28/HTML/"/>
      <url>2020/07/28/HTML/</url>
      
        <content type="html"><![CDATA[<h1 id="HTML基础"><a href="#HTML基础" class="headerlink" title="HTML基础"></a>HTML基础</h1><h2 id="HTML和CSS关系"><a href="#HTML和CSS关系" class="headerlink" title="HTML和CSS关系"></a>HTML和CSS关系</h2><ul><li>css是用来修饰html样式的</li><li>html本身是有一些默认样式,如果我们想改变html标签的样式,就需要借助css</li><li>html+css构成了我们网页的基本页面结构和样式</li></ul><span id="more"></span><h2 id="标签的使用"><a href="#标签的使用" class="headerlink" title="标签的使用"></a>标签的使用</h2><ul><li>标签由英文尖括号&lt;和&gt;括起来，如<html>就是一个标签。</li><li>html中的标签一般都是成对出现的，分开始标签和结束标签。结束标签比开始标签多了一个/。</li></ul><p>如：</p><ul><li><p><code>&lt;p&gt;&lt;/p&gt;</code></p></li><li><p><code>&lt;div&gt;&lt;/div&gt;</code></p></li><li><p><code>&lt;span&gt;&lt;/span&gt;</code></p></li><li>标签与标签之间是可以嵌套的，但先后顺序必须保持一致，如：<code>&lt;div&gt;</code>里嵌套<code>&lt;p&gt;</code>，那么<code>&lt;/p&gt;</code>必须放在<code>&lt;/div&gt;</code>的前面。</li><li>HTML标签不区分大小写，<code>&lt;h1&gt;</code>和<code>&lt;H1&gt;</code>是一样的，但建议小写，因为大部分程序员都以小写为准。</li></ul><h2 id="HTML基本结构"><a href="#HTML基本结构" class="headerlink" title="HTML基本结构"></a>HTML基本结构</h2><p>技术点的解释：</p><ol><li><p><code>&lt;!DOCTYPE html&gt;</code>:文档类型声明，表示该文件为 HTML5文件。<code>&lt;!DOCTYPE&gt;</code> 声明必须是 HTML 文档的第一行，位于 <code>&lt;html&gt;</code> 标签之前</p></li><li><p><code>&lt;html&gt;&lt;/html&gt;</code>标签对：<code>&lt;html&gt;</code>标签位于HTML文档的最前面，用来标识HTML文档的开始；<code>&lt;/html&gt;</code>标签位于HTML文档的最后面，用来标识HTML 文档的结束；这两个标签对成对存在，中间的部分是文档的头部和主题。</p></li></ol><p>3.<code>&lt;head&gt;&lt;/head&gt;</code>标签对：标签包含有关HTML文档的信息，可以包含一些辅助性标签。如<code>&lt;title&gt;&lt;/title&gt;，&lt;link /&gt;&lt;meta /&gt;，&lt;style&gt;&lt;/style&gt;，&lt;script&gt;&lt;/script&gt;</code>等，但是浏览器除了会在标题栏显示<code>&lt;title&gt;</code>元素的内容外，不会向用户显示head元素内的其他任何内容。</p><p>4.<code>&lt;body&gt;&lt;/body&gt;</code>标签对：它是HTML文档的主体部分，在此标签中可以包含<code>&lt;p&gt;&lt;h1&gt;&lt;br&gt;</code>等众多标签，<code>&lt;body&gt;</code>标签出现在<code>&lt;/head&gt;</code>标签之后，且必须在闭标签<code>&lt;/html&gt;</code>之前闭合。</p><h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><pre><code>&lt;!DOCTYPE html&gt;&lt;html&gt;    &lt;head&gt;        &lt;meta charset=&quot;UTF-8&quot;&gt;        &lt;title&gt;html文件基本结构&lt;/title&gt;    &lt;/head&gt;    &lt;body&gt;        &lt;h1&gt;Hello world&lt;/h1&gt;    &lt;/body&gt;&lt;/html&gt;</code></pre><p><strong>效果</strong></p><h2 id="Hello-world"><a href="#Hello-world" class="headerlink" title="Hello world"></a><h1>Hello world</h1></h2><h2 id="head标签"><a href="#head标签" class="headerlink" title="head标签"></a>head标签</h2><p>文档的头部描述了文档的各种属性和信息，包括文档的标题等，绝大多数文档头部包含的数据都不会真正作为内容显示给读者。</p><p>下面这些标签可用在 head 部分：</p><p>1、head标签为双标签，有尾标签，<code>&lt;head&gt;&lt;/head&gt;</code>。</p><p>2、head标签表示头部标签,通常用来嵌套meta、title、style等标签。</p><p>3、<code>&lt;title&gt;</code>标签：在<code>&lt;title&gt;</code>和<code>&lt;/title&gt;</code>标签之间的文字内容是网页的标题信息，它会出现在浏览器的标题栏中。网页的title标签用于告诉用户和搜索引擎这个网页的主要内容是什么，搜索引擎可以通过网页标题，迅速的判断出网页的主题。每个网页的内容都是不同的，每个网页都应该有一个独一无二的title。</p><p>4、<code>&lt;meta charset=&quot;UTF-8&quot;&gt;</code>设置当前文件字符编码</p><p>5、style标签：双标签中设置当前文件样式</p><p>例如title标签：</p><pre><code>&lt;head&gt;    &lt;title&gt;hello world&lt;/title&gt;&lt;/head&gt;&lt;title&gt;</code></pre><p>标签的内容“hello world”会在浏览器中的标题栏上显示出来.</p><h2 id="body标签"><a href="#body标签" class="headerlink" title="body标签"></a>body标签</h2><h3 id="案例-1"><a href="#案例-1" class="headerlink" title="案例"></a>案例</h3><pre><code>&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;    &lt;meta charset=&quot;UTF-8&quot;&gt;    &lt;title&gt;了不起的盖茨比&lt;/title&gt;&lt;/head&gt;&lt;body&gt;    &lt;!-- 标题标签 --&gt;    &lt;h1&gt;了不起的盖茨比&lt;/h1&gt;    &lt;!-- 段落标签 --&gt;    &lt;p&gt;1922年的春天，一个想要成名名叫尼克•卡拉威（托比•马奎尔Tobey Maguire 饰）的作家，离开了美国中西部，来到了纽约。那是一个道德感渐失，爵士乐流行，走私为王，股票飞涨的时代。为了追寻他的&lt;span&gt;美国梦&lt;/span&gt;，他搬入纽约附近一海湾居住。&lt;/p&gt;    &lt;!-- 段落标签 --&gt;    &lt;p&gt;菲茨杰拉德，二十世纪美国文学巨擘之一，兼具作家和编剧双重身份。他以诗人的敏感和戏剧家的想象为&quot;爵士乐时代&quot;吟唱华丽挽歌，其诗人和梦想家的气质亦为那个奢靡年代的不二注解。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</code></pre><h2 id="HTML代码注释"><a href="#HTML代码注释" class="headerlink" title="HTML代码注释"></a>HTML代码注释</h2><p><code>&lt;!--注释文字--&gt;</code></p><h1 id="HTML5语义化标签"><a href="#HTML5语义化标签" class="headerlink" title="HTML5语义化标签"></a>HTML5语义化标签</h1><p><strong>标签的用途</strong></p><p>我们学习网页制作时，常常会听到一个词，语义化。那么什么叫做语义化呢，说的通俗点就是：明白每个标签的用途（在什么情况下使用此标签合理）比如，网页上的文章的标题就可以用标题标签，网页上的各个栏目的栏目名称也可以使用标题标签。文章中内容的段落就得放在段落标签中等等</p><p><strong>语义化的好处</strong></p><ol><li><p>更容易被搜索引擎收录。</p></li><li><p>更容易让屏幕阅读器读出网页内容。</p></li></ol><h2 id="常用标签"><a href="#常用标签" class="headerlink" title="常用标签"></a>常用标签</h2><div class="table-container"><table><thead><tr><th><code>&lt;p&gt;</code></th><th>段落标签</th></tr></thead><tbody><tr><td><code>&lt;span&gt;</code></td><td>自定义文字样式</td></tr><tr><td><code>&lt;hx&gt;</code></td><td>标题标签</td></tr><tr><td><code>&lt;div&gt;</code></td><td>分块标签</td></tr><tr><td><code>&lt;header&gt;</code></td><td>头部标签</td></tr><tr><td><code>&lt;footer&gt;</code></td><td>底部标签</td></tr><tr><td><code>&lt;section&gt;</code></td><td>区段标签</td></tr><tr><td><code>&lt;aside&gt;</code></td><td>侧边栏</td></tr><tr><td><code>&lt;br&gt;</code></td><td>换行</td></tr><tr><td><code>&amp;nbsp</code></td><td>空格</td></tr><tr><td><code>&lt;hr&gt;</code></td><td>分隔线标签</td></tr><tr><td><code>&lt;ul&gt;,&lt;li&gt;</code></td><td>列表标签</td></tr><tr><td><code>&lt;ol&gt;,&lt;li&gt;</code></td><td>有序列表</td></tr></tbody></table></div><h2 id="段落标签"><a href="#段落标签" class="headerlink" title="段落标签"></a>段落标签</h2><p>语法：</p><p><code>&lt;p&gt;段落文本&lt;/p&gt;</code></p><p> 注意一段文字一个<code>&lt;p&gt;</code>标签，如在一篇新闻文章中有3段文字，就要把这3个段落分别放到3个<code>&lt;p&gt;</code>标签中。</p><h2 id="diy文字：-lt-span-gt"><a href="#diy文字：-lt-span-gt" class="headerlink" title="diy文字：&lt;span&gt;"></a>diy文字：<code>&lt;span&gt;</code></h2><p> <code>&lt;span&gt;</code>标签是没有语义的，它的作用就是为了设置单独的样式用 的。</p><h2 id="lt-div-gt-自定义块"><a href="#lt-div-gt-自定义块" class="headerlink" title="&lt;div&gt;自定义块"></a><code>&lt;div&gt;</code>自定义块</h2><p>在网页制作过程过中，可以把一些独立的逻辑部分划分出来，放在一个<code>&lt;div&gt;</code>标签中，这个<code>&lt;div&gt;</code>标签的作用就相当于一个容器。</p><p>语法：</p><p><code>&lt;div&gt;…&lt;/div&gt;</code></p><h2 id="列表标签"><a href="#列表标签" class="headerlink" title="列表标签"></a>列表标签</h2><p><strong>语法</strong></p><pre><code>&lt;ul&gt;  &lt;li&gt;信息&lt;/li&gt;  &lt;li&gt;信息&lt;/li&gt;   ......&lt;/ul&gt;</code></pre><p><strong>效果</strong>  </p><ul>  <li>信息</li>  <li>信息</li>   ......</ul><h1 id="插入图片链接"><a href="#插入图片链接" class="headerlink" title="插入图片链接"></a>插入图片链接</h1><h2 id="插入图片"><a href="#插入图片" class="headerlink" title="插入图片"></a>插入图片</h2><p><strong>语法：</strong></p><pre><code>&lt;img src=&quot;图片地址&quot; alt=&quot;下载失败时的替换文本&quot; title = &quot;提示文本&quot;&gt;</code></pre><p><strong>举例：</strong></p><pre><code>&lt;img src = &quot;myimage.gif&quot; alt = &quot;My Image&quot; title = &quot;My Image&quot; /&gt;</code></pre><p>1、src：标识图像的位置；</p><p>2、alt：指定图像的描述性文本，当图像不可见时（下载不成功时），可看到该属性指定的文本；</p><p>3、title：提供在图像可见时对图像的描述(鼠标滑过图片时显示的文本)；</p><p>4、图像可以是GIF，PNG，JPEG格式的图像文件。</p><h2 id="插入超链接"><a href="#插入超链接" class="headerlink" title="插入超链接"></a>插入超链接</h2><p>使用<code>&lt;a&gt;</code>标签可实现超链接，它在网页制作中可以说是无处不在，只要有链接的地方，就会有这个标签。</p><p>语法：</p><p><code>&lt;a  href=&quot;目标网址&quot;  title=&quot;鼠标滑过显示的文本&quot;&gt;</code>链接显示的文本<code>&lt;/a&gt;</code></p><p><strong>例如：</strong></p><pre><code>&lt;a  href=&quot;http://www.fazzie-key.cool&quot;  title=&quot;点击进入慕课网&quot;&gt;click here!&lt;/a&gt;</code></pre><p>上面例子作用是单击click here!文字，网页链接到<a href="http://www.fazzie-key.cool这个网页。">http://www.fazzie-key.cool这个网页。</a></p><p><strong>效果</strong></p><p>  <a  href="http://www.fazzie-key.cool"  title="点击进入慕课网">click here!</a></p><blockquote><p>title属性的作用，鼠标滑过链接文字时会显示这个属性的文本内容。这个属性在实际网页开发中作用很大，主要方便搜索引擎了解链接地址的内容（语义化更友好），）。</p></blockquote><h3 id="打开新窗口"><a href="#打开新窗口" class="headerlink" title="打开新窗口"></a>打开新窗口</h3><p>a标签有的target属性，代表打开网页的方式。可选值为”_self和_blank”，默认值为_self，代表在当前页面打开链接，_blank代表在新窗口打开链接。</p><h2 id="插入表格"><a href="#插入表格" class="headerlink" title="插入表格"></a>插入表格</h2><p>创建表格的四个元素：table、tr、th、td</p><p>1、<table>…</table>：整个表格以<table>标记开始、</table>标记结束。</p><p>2、<tr>…</tr>：表格的一行，所以有几对tr 表格就有几行。</p><p>3、<td>…</td>：表格的一个单元格，一行中包含几对<td>...</td>，说明一行中就有几列。</p><p>4、<th>…</th>：表格的头部的一个单元格，表格表头。</p><p>5、表格中列的个数，取决于一行中数据单元格的个数。</p><p>6、border属性可以为表格添加边框，属性值为数字。</p><p><strong>注意：</strong></p><p>1、table标签用来定义整个表格，为双标签，必须有结束标签。</p><p>2、table标签里面可以放caption标签和tr标签。</p><p>3、caption标签用来定义表格的标题。</p><p>4、tr标签用来设置表格的行，tr里面只能放th或者td标签，一组tr标签代表一行。</p><p>5、th用来设置表格的标题，会加粗居中显示。也就是th标签中的文本默认为粗体并且居中显示。</p><p>6、td同来设置表格的列，一组td标签代表一列。</p><p>7、table表格在没有添加border属性之前, 在浏览器中显示是没有表格线的。</p><p><strong>案例：</strong></p><pre><code>&lt;table border =&quot;1&quot;&gt;    &lt;caption&gt; 前端三件套&lt;/caption&gt;    &lt;tr&gt;        &lt;th&gt;语言&lt;/th&gt;        &lt;th&gt;难度&lt;/th&gt;    &lt;/tr&gt;     &lt;tr&gt;        &lt;td&gt;HTML&lt;/td&gt;        &lt;td&gt;7&lt;/td&gt;    &lt;/tr&gt;      &lt;tr&gt;        &lt;td&gt;CSS&lt;/td&gt;        &lt;td&gt;8&lt;/td&gt;    &lt;/tr&gt;      &lt;tr&gt;        &lt;td&gt;JavaScript&lt;/td&gt;        &lt;td&gt;9&lt;/td&gt;    &lt;/tr&gt;&lt;/table&gt;</code></pre><p><strong>效果</strong></p><table border ="1">    <caption> 前端三件套</caption>    <tr>        <th>语言</th>        <th>难度</th>    </tr>     <tr>        <td>HTML</td>        <td>7</td>    </tr>      <tr>        <td>CSS</td>        <td>8</td>    </tr>      <tr>        <td>JavaScript</td>        <td>9</td>    </tr></table><p>1、<code>&lt;thead&gt;</code> 标签定义表格的表头。该标签用于组合 HTML 表格的表头内容。</p><p>2、<code>&lt;tbody&gt;…&lt;/tbody&gt;</code>：如果不加<code>&lt;thead&gt;&lt;tbody&gt;&lt;tfooter&gt;</code> , table表格加载完后才显示。加上这些表格结构， tbody包含行的内容下载完优先显示，不必等待表格结束后在显示，同时如果表格很长，用tbody分段，可以一部分一部分地显示。（通俗理解table 可以按结构一块块的显示，不在等整个表格加载完后显示。）</p><p>3、<code>&lt;tfoot&gt;</code> 元素用于对 HTML 表格中的表注（页脚）内容进行分组。</p><p>4、thead、tfoot 以及 tbody 元素使您有能力对表格中的行进行分组。当您创建某个表格时，您也许希望拥有一个标题行，一些带有数据的行，以及位于底部的一个总计行。这种划分使浏览器有能力支持独立于表格标题和页脚的表格正文滚动。当长的表格被打印时，表格的表头和页脚可被打印在包含表格数据的每张页面上。</p><h1 id="网站与用户交互"><a href="#网站与用户交互" class="headerlink" title="网站与用户交互"></a>网站与用户交互</h1><p>网站怎样与用户进行交互？答案是使用HTML表单(form)。表单是可以把浏览者输入的数据传送到服务器端，这样服务器端程序就可以处理表单传过来的数据。</p><p><strong>语法：</strong></p><p><code>&lt;form   method=&quot;传送方式&quot;   action=&quot;服务器文件&quot;&gt;</code><br>讲解：</p><p>1.<code>&lt;form&gt;</code> ：<code>&lt;form&gt;</code>标签是成对出现的，以<code>&lt;form&gt;</code>开始，以<code>&lt;/form&gt;</code>结束。</p><p>2.action ：浏览者输入的数据被传送到的地方,比如一个PHP页面(save.php)。</p><p>3.method ： 数据传送的方式（get/post）。</p><pre><code>&lt;form    method=&quot;post&quot;   action=&quot;save.php&quot;&gt;        &lt;label for=&quot;username&quot;&gt;用户名:&lt;/label&gt;        &lt;input type=&quot;text&quot; name=&quot;username&quot; /&gt;        &lt;label for=&quot;pass&quot;&gt;密码:&lt;/label&gt;        &lt;input type=&quot;password&quot; name=&quot;pass&quot; /&gt;&lt;/form&gt;</code></pre><p><strong>效果</strong></p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/28/632bd87adb5745d95e801801a74e9b49.png" alt=""></p><h2 id="输入方式"><a href="#输入方式" class="headerlink" title="输入方式"></a>输入方式</h2><p>当用户要在表单中键入字母、数字等内容时，就会用到文本输入框。文本框也可以转化为密码输入框。<br><strong><br>语法：</strong></p><pre><code>&lt;form&gt;   &lt;input type=&quot;text/password&quot; name=&quot;名称&quot; value=&quot;文本&quot; /&gt;&lt;/form&gt;</code></pre><p><strong>1、type：</strong></p><p>   当type=”text”时，输入框为文本输入框;<br>   当type=”password”时, 输入框为密码输入框。</p><p><strong>2、name</strong>：</p><p>为文本框命名，以备后台程序ASP 、PHP使用。</p><p><strong>3、value</strong>：</p><p>为文本输入框设置默认值。(一般起到提示作用)</p><p>举例：</p><pre><code>&lt;form&gt;  姓名：  &lt;input type=&quot;text&quot; name=&quot;myName&quot;&gt;  &lt;br/&gt;  密码：  &lt;input type=&quot;password&quot; name=&quot;pass&quot;&gt;&lt;/form&gt;</code></pre><h2 id="placeholder属性"><a href="#placeholder属性" class="headerlink" title="placeholder属性"></a>placeholder属性</h2><p>input标签中占位符placeholder,属性，有时候需要提示用户输入框需要输入框的内容，</p><pre><code>&lt;input type=&quot;password&quot; placeholder=&quot;请输入密码&quot; name=&quot;pass&quot;&gt;</code></pre><h2 id="数字输入框"><a href="#数字输入框" class="headerlink" title="数字输入框"></a>数字输入框</h2><p>1、input的type属性设置为number,则表示该输入框的类型为数字。</p><p>2、数字框只能输入数字，输入其他字符无效。</p><p>3、数字框最右侧会有一个加减符号,可以调整输入数字的大小,不同浏览器表现不一致。</p><pre><code>&lt;input type=&quot;number&quot;&gt;</code></pre><h2 id="网址输入框"><a href="#网址输入框" class="headerlink" title="网址输入框"></a>网址输入框</h2><p>1、input的type属性设置为url,则表示该输入框的类型为网址。</p><p>2、数字框的值需以<a href="http://或者https://开头,且后面必须有内容,否则表单提交的时候会报错误提示。">http://或者https://开头,且后面必须有内容,否则表单提交的时候会报错误提示。</a></p><pre><code>&lt;input type=&quot;url&quot;&gt;</code></pre><h2 id="邮箱输入框"><a href="#邮箱输入框" class="headerlink" title="邮箱输入框"></a>邮箱输入框</h2><p>1、Input的type属性设置为email,则表示该输入框的类型为邮箱。</p><p>2、数字框的值必须包含@。</p><p>3、数字框的值@之后必须有内容,否则会报错误提示。</p><pre><code> &lt;input type=&quot;email&quot;&gt;</code></pre><h2 id="文本框（评论系统）"><a href="#文本框（评论系统）" class="headerlink" title="文本框（评论系统）"></a>文本框（评论系统）</h2><p>当用户需要在表单中输入大段文字时，需要用到文本输入域。</p><p>语法：</p><p><code>&lt;textarea  rows=&quot;行数&quot; cols=&quot;列数&quot;&gt;</code>文本<code>&lt;/textarea&gt;</code><br>1、<code>&lt;textarea&gt;</code>标签是成对出现的，以<code>&lt;textarea&gt;</code>开始，以<code>&lt;/textarea&gt;</code>结束。</p><p>2、cols ：多行输入域的列数。</p><p>3、rows ：多行输入域的行数。</p><p>4、在<code>&lt;textarea&gt;&lt;/textarea&gt;</code>标签之间可以输入默认值。</p><p>举例：</p><pre><code>&lt;form  method=&quot;post&quot; action=&quot;save.php&quot;&gt;        &lt;label&gt;联系我们&lt;/label&gt;        &lt;textarea cols=&quot;50&quot; rows=&quot;10&quot; &gt;在这里输入内容...&lt;/textarea&gt;&lt;/form&gt;</code></pre><h2 id="label标签"><a href="#label标签" class="headerlink" title="label标签"></a>label标签</h2><p>label标签不会向用户呈现任何特殊效果，它的作用是为鼠标用户改进了可用性。如果你在 label 标签内点击文本，就会触发此控件。就是说，当用户单击选中该label标签时，浏览器就会自动将焦点转到和标签相关的表单控件上（就自动选中和该label标签相关连的表单控件上）。</p><p><strong>语法：</strong></p><p><code>&lt;label for=&quot;控件id名称&quot;&gt;</code><br>注意：标签的 for 属性中的值应当与相关控件的 id 属性值一定要相同。</p><p><strong>例子：</strong></p><pre><code>&lt;form  &lt;label for=&quot;email&quot;&gt;输入你的邮箱地址&lt;/label&gt;  &lt;input type=&quot;email&quot; id=&quot;email&quot; placeholder=&quot;Enter email&quot;&gt;&lt;/form&gt;</code></pre><h2 id="选框"><a href="#选框" class="headerlink" title="选框"></a>选框</h2><p>在使用表单设计调查表时，为了减少用户的操作，使用选择框是一个好主意，html中有两种选择框，即单选框和复选框，两者的区别是单选框中的选项用户只能选择一项，而复选框中用户可以任意选择多项，甚至全选。请看下面的例子:</p><p><strong>语法：</strong></p><pre><code>&lt;input   type=&quot;radio/checkbox&quot;   value=&quot;值&quot;    name=&quot;名称&quot;   checked=&quot;checked&quot;/&gt;</code></pre><p>1、<strong>type</strong>:</p><p>   当 type=”radio” 时，控件为单选框</p><p>   当 type=”checkbox” 时，控件为复选框</p><p>2、<strong>value</strong>：提交数据到服务器的值（后台程序PHP使用）</p><p>3、<strong>name</strong>：为控件命名，以备后台程序 ASP、PHP 使用</p><p>4、<strong>checked</strong>：当设置 checked=”checked” 时，该选项被默认选中</p><p>如下面代码：</p><pre><code>&lt;form action=&quot;save.php&quot; method=&quot;post&quot;&gt;        &lt;label&gt;性别:&lt;/label&gt;        &lt;label&gt;男&lt;/label&gt;        &lt;input type=&quot;radio&quot; value=&quot;1&quot; name=&quot;gender&quot; /&gt;        &lt;label&gt;女&lt;/label&gt;        &lt;input type=&quot;radio&quot; value=&quot;2&quot; name=&quot;gender&quot; /&gt;    &lt;/form&gt;</code></pre><h2 id="下拉框"><a href="#下拉框" class="headerlink" title="下拉框"></a>下拉框</h2><p>下拉列表在网页中也常会用到，它可以有效的节省网页空间。既可以单选、又可以多选。如下代码：</p><pre><code>&lt;form&gt;        &lt;select&gt;            &lt;option value=&quot;看书&quot;&gt;看书&lt;/option&gt;            &lt;option value=&quot;旅游&quot;&gt;旅游&lt;/option&gt;            &lt;option value=&quot;运动&quot;&gt;运动&lt;/option&gt;            &lt;option value=&quot;购物&quot;&gt;购物&lt;/option&gt;        &lt;/select&gt;    &lt;/form&gt;</code></pre><h2 id="提交按钮"><a href="#提交按钮" class="headerlink" title="提交按钮"></a>提交按钮</h2><p>在表单中有两种按钮可以使用，分别为：提交按钮、重置。这一小节讲解提交按钮：当用户需要提交表单信息到服务器时，需要用到提交按钮。</p><p><strong>语法：</strong></p><p><code>&lt;input   type=&quot;submit&quot;   value=&quot;提交&quot;&gt;</code></p><p><strong>type</strong>：只有当type值设置为submit时，按钮才有提交作用</p><p><strong>value</strong>：按钮上显示的文字</p><pre><code>    &lt;form method=&quot;post&quot; action=&quot;save.php&quot;&gt;        &lt;label for=&quot;myName&quot;&gt;姓名：&lt;/label&gt;        &lt;input type=&quot;text&quot; value=&quot; &quot; name=&quot;myName &quot; /&gt;        &lt;input type=&quot;submit&quot; value=&quot;提交&quot; name=&quot;submitBtn&quot; /&gt;    &lt;/form&gt;</code></pre><h2 id="重置"><a href="#重置" class="headerlink" title="重置"></a>重置</h2><p>当用户需要重置表单信息到初始时的状态时，比如用户输入“用户名”后，发现书写有误，可以使用重置按钮使输入框恢复到初始状态。只需要把type设置为”reset”就可以。</p><p>语法：</p><pre><code>&lt;input type=&quot;reset&quot; value=&quot;重置&quot;&gt;</code></pre><p><strong>type</strong>：只有当type值设置为reset时，按钮才有重置作用</p><p><strong>value</strong>：按钮上显示的文字</p>]]></content>
      
      
      <categories>
          
          <category> 《栈溢出工程师》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 前端 </tag>
            
            <tag> HTML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL</title>
      <link href="2020/07/25/MySQL/"/>
      <url>2020/07/25/MySQL/</url>
      
        <content type="html"><![CDATA[<h2 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h2><p>命令行cd到目录</p><p><code>C:\Program Files\MySQL\MySQL Server 8.0\bin</code></p><pre><code>mysql -u root -p</code></pre><span id="more"></span><h2 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h2><h3 id="添加删除数据"><a href="#添加删除数据" class="headerlink" title="添加删除数据"></a>添加删除数据</h3><h4 id="查看DB"><a href="#查看DB" class="headerlink" title="查看DB"></a>查看DB</h4><pre><code>show databases</code></pre><h4 id="添加DB"><a href="#添加DB" class="headerlink" title="添加DB"></a>添加DB</h4><pre><code>create database gc</code></pre><h4 id="删除DB"><a href="#删除DB" class="headerlink" title="删除DB"></a>删除DB</h4><pre><code>drop database gc</code></pre><blockquote><p>gc 为数据库名</p></blockquote><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h3 id="文本类型"><a href="#文本类型" class="headerlink" title="文本类型"></a>文本类型</h3><div class="table-container"><table><thead><tr><th>数据类型</th><th>描述</th></tr></thead><tbody><tr><td>CHAR(size)</td><td>保存固定长度的字符串(可包含字母、数字以及特殊字符)●在括号中指定字符串的长度。最多255个字符。</td></tr><tr><td>VARCHAR(size)</td><td>保存可变长度的字符串(可包含字母、数字以及特殊字符)。在括号中指定字符串的最大长度。最多255个字符。注释:如果值的长度大于255，则被转换为TEXT类型。</td></tr><tr><td>TINYTEXT</td><td>存放最大长度为255个字符的字符串。</td></tr><tr><td>TEXT</td><td>存放最大长度为65,535个字符的字符串。</td></tr><tr><td>BLOB</td><td>用于BLOBs (Binary Large OBjects).存放最多65,535字节的数据。</td></tr><tr><td>MEDIUMTEXT</td><td>存放最大长度为16,777,215 个字符的字符串。</td></tr><tr><td>MEDIUMBLOB</td><td>用于BLOBs (Binary Large OBjects)。存放最多16,777,215 字节的数据。</td></tr><tr><td>LONGTEXT</td><td>存放最大长度为4,294,967,295个字符的字符串。</td></tr><tr><td>LONGBLOB</td><td>用于BLOBs (Binary Large OBjects).存放最多4,294,967,295字节的数据。</td></tr><tr><td>ENUM(x,y,z,etc.)</td><td>允许你输入可能值的列表。可以在ENUM列表中列出最大65535个值。如果列表中不存在插入的值，则插入空值。注释:这些值是按照你输入的顺序存储的。可以按照此格式输入可能的值: ENUM(‘X’,”Y’,;Z’)</td></tr><tr><td>SET</td><td>与ENUM类似，SET最多只能包含64个列表项，不过SET可存储一个以上的值。</td></tr></tbody></table></div><h3 id="数字类型"><a href="#数字类型" class="headerlink" title="数字类型"></a>数字类型</h3><div class="table-container"><table><thead><tr><th>数据类型</th><th>描述</th></tr></thead><tbody><tr><td>TINYINT(size)</td><td>-128到127常规。0到255无符号*。在括号中规定最大位数。</td></tr><tr><td>SMALLINT(size)</td><td>-32768到32767常规。0到65535无符号*。在括号中规定最大位数。</td></tr><tr><td>MEDIUMINT(size)</td><td>-8388608到8388607普通。0 to 16777215无符号*。在括号中规定最大位数。</td></tr><tr><td>INT(size)</td><td>-2147483648到2147483647常规。0到4294967295无符号*。在括号中规定最大位数。</td></tr><tr><td>BIGINT(size)</td><td>-9223372036854775808到9223372036854775807常规。0到18446744073709551615无符号*。在括号中规定最大位数。</td></tr><tr><td>FLOAT(size,d)</td><td>带有浮动小数点的小数字。在括号中规定最大位数。在d参数中规定小数点右侧的最大位数。</td></tr><tr><td>DOUBLE(size,d)</td><td>带有浮动小数点的大数字。在括号中规定最大位数。在d参数中规定小数点右侧的最大位数。</td></tr><tr><td>DECIMAL(size,d)</td><td>作为字符串存储的DOUBLE类型，允许固定的小数点。</td></tr></tbody></table></div><h3 id="日期类型"><a href="#日期类型" class="headerlink" title="日期类型"></a>日期类型</h3><div class="table-container"><table><thead><tr><th>数据类型</th><th>描述</th></tr></thead><tbody><tr><td>DATE()</td><td>日期。格式: YYYY-MM-DD注释:支持的范围是从*1000-01-01’到’9999-12-31’</td></tr><tr><td>DATETIME()</td><td>*日期和时间的组合。格式: YYYY-MM-DD HH:MM:SS注释:支持的范围是从’1000-01-01 00:00:00’到’9999-12-31 23:59:59’</td></tr><tr><td>TIMESTAMP()</td><td><em>时间戳。TIMESTAMP值使用Unix纪元(‘1970-01-01 00:00:00 UTC)至今的描述来存储。格式: YYYY-MM-DD HH:MM:SS注释:支持的范围是从 </em>1970-01-0100:00:01 UTC到’2038-01 -0903:14:07’ utC</td></tr><tr><td>TIME()</td><td>时间。格式: HH:MM:SS注释:支持的范围是从’-838:59:59’到’838:59:59’</td></tr><tr><td>YEAR()</td><td>2位或4位格式的年。注释: 4位格式所允许的值: 1901到2155。2位格式所允许的值: 70到69，表示从1970到2069.</td></tr></tbody></table></div><h2 id="创建数据表"><a href="#创建数据表" class="headerlink" title="创建数据表"></a>创建数据表</h2><pre><code>create table table_ name (colum_ name data_ _type,colum_ name data_ type,.colum_ name data_type);</code></pre><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><pre><code>CREATE TABLE account (id bigint(20)create Time datetime,ip varchar(255),mobile varchar(255),nickname varchar(255),passwd varchar(255)，username varchar(255),avatar varchar(255),brief text,job varchar(255),location varchar(255),qq varchar(255), gender int(11)，city varchar(255),province varchar(255));</code></pre><h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><pre><code>use gc;#使用数据库show tables;#展示数据表create account(colum type);#创建数据表describe account#查看数据表drop account;#删除数据表</code></pre><h2 id="增加删除行列"><a href="#增加删除行列" class="headerlink" title="增加删除行列"></a>增加删除行列</h2><h3 id="增加"><a href="#增加" class="headerlink" title="增加"></a>增加</h3><p><code>alter table [ table_ name ] add [ column_ name][ data_ _type ][not null] [default ]</code></p><p>例子:</p><pre><code>alter table account add c1 int(11) not null default 1;</code></pre><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><p><code>alter table[ table_ name ] drop[ column_ name ]</code></p><p>例子:</p><pre><code>alter table account drop c1;</code></pre><h2 id="修改列信息"><a href="#修改列信息" class="headerlink" title="修改列信息"></a>修改列信息</h2><p><code>alter table[ table_ name ] change [ old_ column_ name ] [ new_ column_ name ][data_ type ]</code></p><p>1.只改列名:<br>data<em> type和原来一样，old</em> <em>column</em> name != new<em> column</em> name</p><p>2.只改数据类型:<br>old<em> column</em> name == new<em> column</em> name, data_ type改变</p><p>3.列名和数据类型都改了</p><h2 id="插入和查看表的数据"><a href="#插入和查看表的数据" class="headerlink" title="插入和查看表的数据"></a>插入和查看表的数据</h2><h3 id="查看"><a href="#查看" class="headerlink" title="查看"></a>查看</h3><p><code>select * from table_ name;</code></p><p><code>select col_ name 1 ,col_ name2, ... from table_ name;</code></p><h3 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h3><p><code>insert into[ tabte_ name ] values(值1, 值....</code></p><p><code>insert into [table_ name] (列1， 列2...) values(值1,值...</code></p><h2 id="where-条件"><a href="#where-条件" class="headerlink" title="where 条件"></a>where 条件</h2><p><code>select * from table_ name where col_ name运算符值</code></p><p>例子:</p><pre><code>select * from book where title = = t’ ;</code></pre><h3 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h3><div class="table-container"><table><thead><tr><th>运算符</th><th>描述</th></tr></thead><tbody><tr><td>=</td><td>等于</td></tr><tr><td>!=</td><td>不等于</td></tr><tr><td><code>&gt;</code></td><td>大于</td></tr><tr><td>&lt;</td><td>小于.</td></tr><tr><td>&gt;=</td><td>大于等于</td></tr><tr><td>&lt;=</td><td>小于等于</td></tr><tr><td>between</td><td>在两个值范围内</td></tr><tr><td>like</td><td>按某个模式查找</td></tr></tbody></table></div><h3 id="组合条件"><a href="#组合条件" class="headerlink" title="组合条件"></a>组合条件</h3><p>where后面可以通过and与or运算符组合多个条件筛选</p><p>语法:</p><p><code>select * from table_ namewhere toll=XXXandcol2=XXorcol3&gt;XX</code></p><h2 id="null字段的判断"><a href="#null字段的判断" class="headerlink" title="null字段的判断"></a>null字段的判断</h2><p>NULL字段无法直接查询</p><ul><li>正确 </li></ul><p>select * from book where id is null</p><ul><li>错误 </li></ul><p>select * from book where id = null</p><h2 id="distinct语句"><a href="#distinct语句" class="headerlink" title="distinct语句"></a>distinct语句</h2><p><code>select distinct col_ name from table_ name ;</code></p><p>例子:</p><pre><code>select distinct title from book;</code></pre><blockquote><p>select 语句可以去除重复的语句</p></blockquote><h2 id="order-by对查询结果进行排序"><a href="#order-by对查询结果进行排序" class="headerlink" title="order by对查询结果进行排序"></a>order by对查询结果进行排序</h2><p><strong>1.按单一列名排序:</strong></p><p>select * from table<em> name [where子句] order by col</em> name [asc/desc]</p><p><strong>2.按多列排序:</strong></p><p>select * from table_ name [where子句] order by coll [asc/desc]，col2<br>[asc,desc]</p><blockquote><p>asc升序<br>desc降序</p></blockquote><h2 id="limit截取查询结果"><a href="#limit截取查询结果" class="headerlink" title="limit截取查询结果"></a>limit截取查询结果</h2><p><code>select * from table_ name [where子句] [order by子句] limit [offset,] rowCount</code></p><p>offset:查询结果的起始位置，第一 条记录的其实是0<br>rowCount:从offset位置开始，获取的记录条数</p><p>注意<br>limit rowCount = limit 0,rowCount</p><blockquote><p>作用：截取页面</p></blockquote><h2 id="插入命令和查询命令select的组合使用"><a href="#插入命令和查询命令select的组合使用" class="headerlink" title="插入命令和查询命令select的组合使用"></a>插入命令和查询命令select的组合使用</h2><p><strong>一般用法:</strong></p><p><code>insert into [表名] values(值1, 值.....</code></p><p><code>insert into[表名] (列1 ，列2...) values(值1,值....</code></p><p><strong>Insert into与select的组合用法</strong></p><p><code>insert into [表名1] select 列1，列2 from [表名2]</code></p><p><code>insert into [表名1](列1，列2)select列3，列4 from[表名2]</code></p><blockquote><p>作用：数据迁移</p></blockquote><h2 id="更新表的数据"><a href="#更新表的数据" class="headerlink" title="更新表的数据"></a>更新表的数据</h2><p><strong>修改单列:</strong></p><p><code>update表名set列名= XXX [where字句]</code></p><p><strong>修改多列:</strong></p><p><code>update表名set列名1 = xxx,列名2 = xxX... [where 字句]</code></p><h2 id="where语句中in-like-between的使用"><a href="#where语句中in-like-between的使用" class="headerlink" title="where语句中in,like,between的使用"></a>where语句中in,like,between的使用</h2><h3 id="in"><a href="#in" class="headerlink" title="in"></a>in</h3><p><code>select * from表名where列名in (value1 ,value2..)</code></p><p><code>select * from表名where列名in (select 列名from表名)</code></p><p>例子</p><pre><code>select * from book where title in(&#39;sun&#39;,&#39;color&#39;)</code></pre><h3 id="betweeen"><a href="#betweeen" class="headerlink" title="betweeen"></a>betweeen</h3><p><code>select * from表名where列名between值1 and值2</code></p><p><code>select * from表名where列名not between值1 and值2</code></p><h3 id="like"><a href="#like" class="headerlink" title="like"></a>like</h3><p><strong>作用：字符模糊匹配</strong></p><p><code>select * from表名where列名[not] like pattern</code></p><p><strong>pattern:匹配模式</strong></p><p>比如<br><code>&#39;abc&#39;%abc’,‘abc%’,&#39;%abc%&#39;</code> ;<br>“<code>%</code>” 是一个通配符，理解_上可以把他当成任何字符串</p><p><strong>例子:</strong></p><p><code>&#39;%abc&#39;</code><br>能匹配<br><code>&#39;erttsabc&#39;</code></p>]]></content>
      
      
      <categories>
          
          <category> 《栈溢出工程师》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>note of deeplearning </title>
      <link href="2020/07/18/note-of-deeplearning/"/>
      <url>2020/07/18/note-of-deeplearning/</url>
      
        <content type="html"><![CDATA[<h1 id="深度学习笔记"><a href="#深度学习笔记" class="headerlink" title="深度学习笔记"></a>深度学习笔记</h1><p><a href="https://Fazziekey.github.io/2020/07/18/RNN/">RNN(循环神经网络)</a></p><p><a href="https://fazzie-key.cool/2020/07/18/CNN%E2%80%9C/">CNN(卷积神经网络)</a></p>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN</title>
      <link href="2020/07/18/RNN/"/>
      <url>2020/07/18/RNN/</url>
      
        <content type="html"><![CDATA[<h1 id="循环序列模型"><a href="#循环序列模型" class="headerlink" title="循环序列模型"></a>循环序列模型</h1><h2 id="为什么选择序列模型"><a href="#为什么选择序列模型" class="headerlink" title="为什么选择序列模型"></a>为什么选择序列模型</h2><p><img src="http://www.ai-start.com/dl2017/images/ae2970d80a119cd341ef31c684bfac49.png" alt=""><br><span id="more"></span></p><h2 id="数学符号"><a href="#数学符号" class="headerlink" title="数学符号"></a>数学符号</h2><ul><li>输入$x^{<t>}$</li><li>输出$y^{<t>}$</li><li>序列长度$T_x$</li><li>词向量one-hot编码<br><img src="http://www.ai-start.com/dl2017/images/8deca8a84f06466155d2d8d53d26e05d.png" alt=""></li></ul><h2 id="RNN模型"><a href="#RNN模型" class="headerlink" title="RNN模型"></a>RNN模型</h2><h3 id="用普通神经网络的问题"><a href="#用普通神经网络的问题" class="headerlink" title="用普通神经网络的问题"></a>用普通神经网络的问题</h3><ul><li>输出和输入维度不对应</li><li>序列前后不存在联系</li></ul><h3 id="解决，引入RNN"><a href="#解决，引入RNN" class="headerlink" title="解决，引入RNN"></a>解决，引入RNN</h3><p><img src="http://www.ai-start.com/dl2017/images/cb041c33b65e17600842ebf87174c4f2.png" alt=""></p><script type="math/tex; mode=display">a^{<1>}=g_1(w_{aa}a^{<0>}+w_{<ax>x^{1}}+b_a)</script><script type="math/tex; mode=display">\hat{y}=g_2(w_{ya}a^{<1>}+b_y)</script><h3 id="更一般情况"><a href="#更一般情况" class="headerlink" title="更一般情况"></a>更一般情况</h3><script type="math/tex; mode=display">a^{<t>}=g_1(w_{aa}a^{<t-1>}+w_{<ax>x^{t}}+b_a)</script><script type="math/tex; mode=display">\hat{y}=g_2(w_{ya}a^{<t>}+b_y)</script><h3 id="进一步化简"><a href="#进一步化简" class="headerlink" title="进一步化简"></a>进一步化简</h3><script type="math/tex; mode=display">a^{<t>}=g_1(w_{a}[a^{<t-1>},x^{t}]+b_a)</script><script type="math/tex; mode=display">\hat{y}=g_2(w_{ya}a^{<t>}+b_y)</script><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p><img src="http://www.ai-start.com/dl2017/images/rnn-f.png" alt=""></p><h3 id="RNN常用激活函数"><a href="#RNN常用激活函数" class="headerlink" title="RNN常用激活函数"></a>RNN常用激活函数</h3><ul><li>tanh</li><li>ReLU</li></ul><h2 id="通过时间的反向传播"><a href="#通过时间的反向传播" class="headerlink" title="通过时间的反向传播"></a>通过时间的反向传播</h2><h3 id="单个单元Loss-Function"><a href="#单个单元Loss-Function" class="headerlink" title="单个单元Loss Function"></a>单个单元Loss Function</h3><script type="math/tex; mode=display">L^{<t>}(\hat{y}^{<t>},y^{<t>})=-y^{<t>}log\hat{y}^{<t>}-(1-y^{<t>})log(1-\hat{y}^{<t>})</script><h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><script type="math/tex; mode=display">L(\hat{y}^{<t>},y^{<t>})=\sum_{t=1}^{T_n}L^{<t>}(\hat{y}^{<t>},y^{<t>})</script><h3 id="RNN反向传播计算"><a href="#RNN反向传播计算" class="headerlink" title="RNN反向传播计算"></a>RNN反向传播计算</h3><p><img src="http://www.ai-start.com/dl2017/images/rnn_cell_backprop.png" alt=""></p><h2 id="不同类型的RNN结构"><a href="#不同类型的RNN结构" class="headerlink" title="不同类型的RNN结构"></a>不同类型的RNN结构</h2><p><img src="http://www.ai-start.com/dl2017/images/329b0748f7282efc206ea8de6a709833.png" alt=""></p><h3 id="多对一"><a href="#多对一" class="headerlink" title="多对一"></a>多对一</h3><p><img src="http://www.ai-start.com/dl2017/images/14e1df0a7a8cdd1584b2e92e87e23aa7.png" alt=""></p><h3 id="一对一"><a href="#一对一" class="headerlink" title="一对一"></a>一对一</h3><p><img src="http://www.ai-start.com/dl2017/images/14e1df0a7a8cdd1584b2e92e87e23aa7.png" alt=""></p><h3 id="一对多"><a href="#一对多" class="headerlink" title="一对多"></a>一对多</h3><p><img src="http://www.ai-start.com/dl2017/images/db580f1dfd6095d672fc62cce74ce5e2.png" alt=""></p><h2 id="语言模型和序列生成"><a href="#语言模型和序列生成" class="headerlink" title="语言模型和序列生成"></a>语言模型和序列生成</h2><p>语言模型所做的就是，它会告诉你某个特定的句子它出现的概率是多少</p><h3 id="语言模型建立步骤"><a href="#语言模型建立步骤" class="headerlink" title="语言模型建立步骤"></a>语言模型建立步骤</h3><ul><li>获取大量数据集</li><li>句子标记化</li><li>构建RNN</li></ul><h2 id="RNN梯度消失的问题"><a href="#RNN梯度消失的问题" class="headerlink" title="RNN梯度消失的问题"></a>RNN梯度消失的问题</h2><p><img src="http://www.ai-start.com/dl2017/images/8fb1c4afe30b7a0ede26522b355068ba.png" alt=""></p><h3 id="RNN存在问题"><a href="#RNN存在问题" class="headerlink" title="RNN存在问题"></a>RNN存在问题</h3><ul><li>缺乏长期记忆，如cat和were</li><li>梯度爆炸</li></ul><h3 id="如何解决？"><a href="#如何解决？" class="headerlink" title="如何解决？"></a>如何解决？</h3><ul><li>GRU</li><li>LSTM</li></ul><h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><h3 id="cell单元"><a href="#cell单元" class="headerlink" title="cell单元"></a>cell单元</h3><ul><li>功能：记忆<script type="math/tex; mode=display">\hat{c}^{<t>}=tanh(W_c[c^{<t-1>},x^{<t>}]+b_c)</script><h3 id="记忆门"><a href="#记忆门" class="headerlink" title="记忆门"></a>记忆门</h3><script type="math/tex; mode=display">\Gamma_u=\sigma(W_u[c^{<t-1>},x^{<t>}]+b_u)</script></li></ul><h3 id="GRU简化模型"><a href="#GRU简化模型" class="headerlink" title="GRU简化模型"></a>GRU简化模型</h3><script type="math/tex; mode=display">c^{<t>}=\Gamma \hat{c}^{<t>}+(1-\Gamma_u)c^{<t-1>}</script><h3 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h3><script type="math/tex; mode=display">\Gamma_r=\sigma(W_r[c^{<t-1>},x^{<t>}]+b_r)</script><script type="math/tex; mode=display">\hat{c}^{<t>}=tanh(W_c[\Gamma_r*c^{<t-1>},x^{<t>}]+b_c)</script><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><h3 id="LSTM模型图"><a href="#LSTM模型图" class="headerlink" title="LSTM模型图"></a>LSTM模型图</h3><p><img src="http://www.ai-start.com/dl2017/images/LSTM.png" alt=""></p><h3 id="LSTM单元"><a href="#LSTM单元" class="headerlink" title="LSTM单元"></a>LSTM单元</h3><ul><li>更新门</li><li>遗忘门</li><li>输出门</li><li>激活函数</li><li>相关性门</li></ul><h3 id="LSTM的前向传播"><a href="#LSTM的前向传播" class="headerlink" title="LSTM的前向传播"></a>LSTM的前向传播</h3><p><img src="http://www.ai-start.com/dl2017/images/LSTM_rnn.png" alt=""></p><h3 id="LSTM反向传播"><a href="#LSTM反向传播" class="headerlink" title="LSTM反向传播"></a>LSTM反向传播</h3><h4 id="门求偏导"><a href="#门求偏导" class="headerlink" title="门求偏导"></a>门求偏导</h4><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200712162521.png" alt=""></p><h4 id="参数求偏导"><a href="#参数求偏导" class="headerlink" title="参数求偏导"></a>参数求偏导</h4><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200712162520.png" alt=""></p><h4 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h4><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200712162522.png" alt=""></p><h2 id="双向RNN"><a href="#双向RNN" class="headerlink" title="双向RNN"></a>双向RNN</h2><p><img src="http://www.ai-start.com/dl2017/images/48c787912f7f8daee638dd311583d6cf.png" alt=""></p><h2 id="深层RNN"><a href="#深层RNN" class="headerlink" title="深层RNN"></a>深层RNN</h2><p><img src="http://www.ai-start.com/dl2017/images/8378c2bfe73e1ac9f85d6aa79b71b5eb.png" alt=""></p><hr><h1 id="自然语言处理和词嵌入"><a href="#自然语言处理和词嵌入" class="headerlink" title="自然语言处理和词嵌入"></a>自然语言处理和词嵌入</h1><p><img src="http://www.ai-start.com/dl2017/images/68d7c930146724f39782cb57d33161e9.png" alt=""></p><h2 id="词汇表征"><a href="#词汇表征" class="headerlink" title="词汇表征"></a>词汇表征</h2><h3 id="one-hot编码的缺点"><a href="#one-hot编码的缺点" class="headerlink" title="one-hot编码的缺点"></a>one-hot编码的缺点</h3><ul><li>每个词孤立，泛化能力不强</li><li>数据量太大</li></ul><h3 id="如何改进？"><a href="#如何改进？" class="headerlink" title="如何改进？"></a>如何改进？</h3><h4 id="应用特征向量"><a href="#应用特征向量" class="headerlink" title="应用特征向量"></a>应用特征向量</h4><p><img src="http://www.ai-start.com/dl2017/images/ce30c9ae7912bdb3562199bf85eca1cd.png" alt=""></p><h3 id="t-SNE-算法"><a href="#t-SNE-算法" class="headerlink" title="t-SNE 算法"></a>t-SNE 算法</h3><p>将一维词向量转化为二维可视化<br><img src="http://www.ai-start.com/dl2017/images/59fb45cfdf7faa53571ec7b921b78358.png" alt=""></p><h2 id="使用词嵌入"><a href="#使用词嵌入" class="headerlink" title="使用词嵌入"></a>使用词嵌入</h2><p><img src="http://www.ai-start.com/dl2017/images/b4bf4b0cdcef0c9d021707c47d5aecda.png" alt=""></p><h3 id="词嵌入做迁移学习的方法"><a href="#词嵌入做迁移学习的方法" class="headerlink" title="词嵌入做迁移学习的方法"></a>词嵌入做迁移学习的方法</h3><ul><li>先从大量的文本集中学习词嵌入。一个非常大的文本集，或者下载预训练好的词嵌入模型</li><li><p>用这些词嵌入模型把它迁移到新的只有少量标注训练集的任务中，比如说用这个300维的词嵌入来表示的单词。这样做的一个好处就是可以用更低维度的特征向量代替原来的10000维的one-hot向量，现在你可以用一个300维更加紧凑的向量。尽管one-hot向量很快计算，而学到的用于词嵌入的300维的向量会更加紧凑。</p></li><li><p>在你的任务上训练模型时，在你的命名实体识别任务上，只有少量的标记数据集上，你可以自己选择要不要继续微调，用新的数据调整词嵌入。实际中，只有这个第二步中有很大的数据集你才会这样做，如果你标记的数据集不是很大，通常我不会在微调词嵌入上费力气。</p></li></ul><h2 id="词嵌入的特性"><a href="#词嵌入的特性" class="headerlink" title="词嵌入的特性"></a>词嵌入的特性</h2><ul><li>类比推理</li></ul><script type="math/tex; mode=display">e_{man}-e_{women}=\begin{bmatrix}-1\\0.01\\0.03\\0.09\end{bmatrix}-\begin{bmatrix}-1\\0.02\\0.02\\0.01\end{bmatrix}=\begin{bmatrix}-2\\-0.01\\0.01\\0.08\end{bmatrix}\approx\begin{bmatrix}-2\\0\\0\\0\end{bmatrix}</script><script type="math/tex; mode=display">e_{king}-e_{queen}=\begin{bmatrix}-0.95\\0.093\\0.70\\0.09\end{bmatrix}-\begin{bmatrix}0.97\\0.95\\0.69\\0.01\end{bmatrix}=\begin{bmatrix}-1.92\\-0.02\\0.01\\0.01\end{bmatrix}\approx\begin{bmatrix}-2\\0\\0\\0\end{bmatrix}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/16/db12137b26fb4e84e5261b2dcb49ddf9.png" alt=""></p><script type="math/tex; mode=display">e_{man}-e_{woman}\approx e_{king}-e_{queen}</script><h3 id="如何反映相似度？"><a href="#如何反映相似度？" class="headerlink" title="如何反映相似度？"></a>如何反映相似度？</h3><p><strong>余弦相似度</strong></p><script type="math/tex; mode=display">sim(u,v)=\frac{u^Tv}{|u|_2|v|_2}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/16/b71d0cddd12a41ab5c9212b24497a92c.png" alt=""></p><h2 id="嵌入矩阵"><a href="#嵌入矩阵" class="headerlink" title="嵌入矩阵"></a>嵌入矩阵</h2><p>学习词嵌入其实是学习一个嵌入矩阵<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/16/f319dc21af8e248c2e7418883f160097.png" alt=""></p><script type="math/tex; mode=display">E\cdot o_i=e_{i}</script><h2 id="学习词嵌入"><a href="#学习词嵌入" class="headerlink" title="学习词嵌入"></a>学习词嵌入</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/16/b6b25908f6ab1c94d6d102191377160f.png" alt=""></p><h3 id="skip-gram"><a href="#skip-gram" class="headerlink" title="skip-gram"></a>skip-gram</h3><p><strong>输入：预测的词汇前n个单词，上下文</strong></p><p><strong>输出：softmax的rank</strong></p><ul><li>随机生成一个嵌入矩阵</li><li>学习嵌入矩阵参数</li></ul><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>学习映射关系</p><script type="math/tex; mode=display">o_c \to E \to e_c\to o_{softmax}\to \hat{y}</script><ul><li>随机选择目标单词上下文的另一个单词</li><li>输入one-hot向量</li><li>监督学习</li></ul><blockquote><p>softmax:</p></blockquote><script type="math/tex; mode=display">p(t|c)=\frac{e^{\theta_t^T}e_c}{\sum_{j=1}^{10000}e^{\theta_t^T}e_c}</script><h3 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h3><p>softmax运算量太大</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><ul><li>分级</li><li>负采样</li></ul><h3 id="CBOM和skip-gram"><a href="#CBOM和skip-gram" class="headerlink" title="CBOM和skip-gram"></a>CBOM和skip-gram</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/16/774dd0cfce8313c273f7155aa064869a.png" alt=""></p><h2 id="负采样（Negative-Sampling）"><a href="#负采样（Negative-Sampling）" class="headerlink" title="负采样（Negative Sampling）"></a>负采样（Negative Sampling）</h2><ul><li>给定一对单词和target</li><li>使用逻辑回归</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/16/1b1dd8a7cbddd05425e6b728cbbac957.png" alt=""></p><script type="math/tex; mode=display">P(y=1|c,t)=\sigma(\theta_t^Te_c)</script><ul><li>输入one-hot向量</li><li>传递给嵌入矩阵</li><li>得到10000个逻辑回归问题</li><li>只选取其中K个进行训练</li><li>转化为K个二分类问题</li></ul><h2 id="Glove-global-vectors-for-word-represengtation"><a href="#Glove-global-vectors-for-word-represengtation" class="headerlink" title="Glove(global vectors for word represengtation)"></a>Glove(global vectors for word represengtation)</h2><ul><li>$x_{ij}$:单词i和单词j在上下文出现的次数</li></ul><script type="math/tex; mode=display">minimize\sum_{i=1}^{10000}\sum_{j=1}^{10000}f(x_{ij})(\theta_i^Te_j+b_i-b_j+\log x_{ij})^2</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/17/c649b0538387f6bac50010d96b8ca019.png" alt=""></p><h2 id="情感分类"><a href="#情感分类" class="headerlink" title="情感分类"></a>情感分类</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/17/af7d6b80483a8a5ad04a78ebaabeca7d.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/17/134af70c48588b247e437f8a5cab420e.png" alt=""></p><h2 id="词嵌入除偏（Debiasing-Word-Embeddings）"><a href="#词嵌入除偏（Debiasing-Word-Embeddings）" class="headerlink" title="词嵌入除偏（Debiasing Word Embeddings）"></a>词嵌入除偏（Debiasing Word Embeddings）</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/17/7640e02d6663dd97ec7cf1e152041e1e.png" alt=""></p><ul><li>求平均</li><li>中和</li><li>均衡步</li></ul><h1 id="序列模型和注意力机制（Sequence-models-amp-Attention-mechanism"><a href="#序列模型和注意力机制（Sequence-models-amp-Attention-mechanism" class="headerlink" title="序列模型和注意力机制（Sequence models &amp; Attention mechanism"></a>序列模型和注意力机制（Sequence models &amp; Attention mechanism</h1><h2 id="基础模型"><a href="#基础模型" class="headerlink" title="基础模型"></a>基础模型</h2><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><ul><li>机器翻译</li><li>语言识别</li></ul><h3 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/4e15947a9ec31763d8545715e20e707c.png" alt=""></p><ul><li>输入法语，输出英语</li><li>网络结构：LSTM或GR</li></ul><h3 id="图片描述"><a href="#图片描述" class="headerlink" title="图片描述"></a>图片描述</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/2683ae07c70d727208561dc32a64d43a.png" alt=""></p><ul><li>输入图片</li><li>进入卷积神经网络如AlexNet</li><li>把softmax单元更换为RNN结构</li><li>输出描述图片的序列</li></ul><h2 id="选择最可能的句子（Picking-the-most-likely-sentence）"><a href="#选择最可能的句子（Picking-the-most-likely-sentence）" class="headerlink" title="选择最可能的句子（Picking the most likely sentence）"></a>选择最可能的句子（Picking the most likely sentence）</h2><h3 id="语言模型和机器翻译模型的区别"><a href="#语言模型和机器翻译模型的区别" class="headerlink" title="语言模型和机器翻译模型的区别"></a>语言模型和机器翻译模型的区别</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/d6b739666d9d7af497ce06ea4a3b14fc.png" alt=""></p><ul><li>语言模型以零向量开始，翻译模型从句子开始</li><li>翻译模型为条件语言模型</li></ul><p><strong>最大化以下式子</strong></p><script type="math/tex; mode=display">P(y^{<1>},...,t^{<T_y>}|x)</script><h3 id="为什么不用Greed-search"><a href="#为什么不用Greed-search" class="headerlink" title="为什么不用Greed search"></a>为什么不用Greed search</h3><ul><li>我们需要一次性输出整个句子而不是一个单词</li><li>运算量太大</li></ul><blockquote><p>贪心搜索是一种来自计算机科学的算法，生成第一个词的分布以后，它将会根据你的条件语言模型挑选出最有可能的第一个词进入你的机器翻译模型中，在挑选出第一个词之后它将会继续挑选出最有可能的第二个词，然后继续挑选第三个最有可能的词，这种算法就叫做贪心搜索。</p></blockquote><h2 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h2><ul><li>beam width$B=3$,选择softmax前三个最大值</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/a620354d4ceafdca5ca0f9ee8b5b658b.png" alt=""></p><ul><li>输入句子，如“Jane visite l’Afrique en Septembre”</li><li>选择第一个单词，保留前三的可能性</li><li>确认单词对的最大可能性</li><li>继续重复该步骤</li></ul><h2 id="Beam-search-改进"><a href="#Beam-search-改进" class="headerlink" title="Beam search 改进"></a>Beam search 改进</h2><h3 id="Length-normalization"><a href="#Length-normalization" class="headerlink" title="Length normalization"></a>Length normalization</h3><script type="math/tex; mode=display">arg max\prod_{t=1}^{T_y}P(y^{<t>}|x,y^{<1>},...,y^{<t-1>})</script><p>加入log函数</p><script type="math/tex; mode=display">arg max\sum_{t=1}^{T_y}\log P(y^{<t>}|x,y^{<1>},...,y^{<t-1>})</script><p>加入参数$\alpha$</p><script type="math/tex; mode=display">\frac{1}{T_y^{\alpha}}arg max\sum_{t=1}^{T_y}\log P(y^{<t>}|x,y^{<1>},...,y^{<t-1>})</script><h3 id="how-to-choose-Beam-width-B"><a href="#how-to-choose-Beam-width-B" class="headerlink" title="how to choose Beam width B"></a>how to choose Beam width B</h3><ul><li>B=1贪心算法</li><li>B过大，计算量大，效果没有显著提升</li></ul><h2 id="Beam-search-误差分析"><a href="#Beam-search-误差分析" class="headerlink" title="Beam search 误差分析"></a>Beam search 误差分析</h2><ul><li>$P(y^*|x)&gt;P(\hat{y}|x)$束搜索算法出错</li><li>$P(y^*|x)&lt;P(\hat{y}|x)$RNN模型出错</li></ul><blockquote><p>RNN实际上是一个编码器合解码器</p></blockquote><h2 id="如何评价一个翻译系统的好坏"><a href="#如何评价一个翻译系统的好坏" class="headerlink" title="如何评价一个翻译系统的好坏"></a>如何评价一个翻译系统的好坏</h2><ul><li>与人工翻译进行对比</li><li>观察输出是否在参考中</li><li>每个单词加入计分上限</li></ul><h3 id="Blue-score"><a href="#Blue-score" class="headerlink" title="Blue score"></a>Blue score</h3><p>考虑词组得分</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/143d162f91004d79ff47338ad85be5d7.png" alt=""></p><h2 id="注意力模型（Attention-Model）"><a href="#注意力模型（Attention-Model）" class="headerlink" title="注意力模型（Attention Model）"></a>注意力模型（Attention Model）</h2><h3 id="长句子的问题"><a href="#长句子的问题" class="headerlink" title="长句子的问题"></a>长句子的问题</h3><p>blue score降低<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/d1ab68289592565fa9abf0df235039e3.png" alt=""></p><h3 id="注意力权重-alpha-lt-t-t’-gt"><a href="#注意力权重-alpha-lt-t-t’-gt" class="headerlink" title="注意力权重$\alpha&lt;t,t’&gt;$"></a>注意力权重$\alpha&lt;t,t’&gt;$</h3><p>第t个输出单词对应第t‘个输入单词的权重<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/c629b0432b83a9875e5a80ebb2558484.png" alt=""></p><script type="math/tex; mode=display">c^{<t>}=\sum_{t'}\alpha^{<t,t'>}a^{<t'>}</script><script type="math/tex; mode=display">\alpha^{<t,t'>}=\frac{exp(e^{<t,t'>})}{\sum_{t'=1}^{T_x}exp(e^{<t,t'>})}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/b6ff6fb462ce466873c06b366060a817.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/4560eb03dfac3d28c6e1e831295d64e7.png" alt=""></p><h3 id="网络结构梳理"><a href="#网络结构梳理" class="headerlink" title="网络结构梳理"></a>网络结构梳理</h3><script type="math/tex; mode=display">activate：x^t \to a^t</script><script type="math/tex; mode=display">a^t,s^{t-1}\to e^t</script><script type="math/tex; mode=display">softmax:e \to \alpha</script><script type="math/tex; mode=display">\alpha,a\to c</script><script type="math/tex; mode=display">activate：c \to s</script><script type="math/tex; mode=display">s^t\to y^t</script><h2 id="语音识别"><a href="#语音识别" class="headerlink" title="语音识别"></a>语音识别</h2><ul><li>输入音频</li><li>输出文字序列</li></ul><h3 id="phonemes方法"><a href="#phonemes方法" class="headerlink" title="phonemes方法"></a>phonemes方法</h3><p>将声音分解为最基本的音位</p><h3 id="CTC-cost（Connectionist-temporal-classification）"><a href="#CTC-cost（Connectionist-temporal-classification）" class="headerlink" title="CTC cost（Connectionist temporal classification）"></a>CTC cost（Connectionist temporal classification）</h3><ul><li>折叠重复字符<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/f16abb639f18e3ab3674c1db485590c8.png" alt=""></li></ul><h2 id="触发字检测（Trigger-Word-Detection）"><a href="#触发字检测（Trigger-Word-Detection）" class="headerlink" title="触发字检测（Trigger Word Detection）"></a>触发字检测（Trigger Word Detection）</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed@master/2020/07/18/16d66c2dd1d9b048e93013002beef77e.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN“</title>
      <link href="2020/07/18/CNN%E2%80%9C/"/>
      <url>2020/07/18/CNN%E2%80%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h2 id="Computer-Version"><a href="#Computer-Version" class="headerlink" title="Computer Version"></a>Computer Version</h2><p>传统的机器视觉的特征提取算法大都基于梯度实现，卷积神经网络为解决视觉问题提供了一个新的方法<br><span id="more"></span></p><h2 id="边缘检测，特征提取"><a href="#边缘检测，特征提取" class="headerlink" title="边缘检测，特征提取"></a>边缘检测，特征提取</h2><p>卷积运算是卷积神经网络的基本组成部分。下面以边缘检测的例子来介绍卷积运算。</p><p>所谓边缘检测，在下面的图中，分别通过垂直边缘检测和水平边缘检测得到不同的结果：<br><img src="https://pic4.zhimg.com/80/v2-2ee2dc2433c755b8997fa73d52b19b27_720w.jpg" alt=""></p><p>假设对于一个 $6\times6$ 大小的图片（以数字表示），以及一个 $3\times3$大小的 filter（卷积核） 进行卷积运算，以<code>*</code> 符号表示。图片和垂直边缘检测器分别如左和中矩阵所示：<br><img src="https://pic3.zhimg.com/80/v2-ef33cdc3a41a9f684b7ff94145a78112_720w.jpg" alt="image"></p><h3 id="不同的边缘检测"><a href="#不同的边缘检测" class="headerlink" title="不同的边缘检测"></a>不同的边缘检测</h3><h3 id="水平和垂直"><a href="#水平和垂直" class="headerlink" title="水平和垂直"></a>水平和垂直</h3><p><img src="https://pic4.zhimg.com/80/v2-13d7c463c6122598c80c48e804fdf9fb_720w.jpg" alt=""></p><h3 id="其他Filter"><a href="#其他Filter" class="headerlink" title="其他Filter"></a>其他Filter</h3><p><img src="https://pic4.zhimg.com/80/v2-85fc99b475104f2906a2d4a4694ece0f_720w.jpg" alt=""></p><p>对于复杂的图片，我们可以直接将 filter 中的数字直接看作是需要学习的参数，其可以学习到对于图片检测相比上面filter更好的更复杂的 filter ，如相对于水平和垂直检测器，我们训练的 filter 参数也许可以知道不同角度的边缘。</p><p>通过卷积运算，在卷积神经网络中通过反向传播算法，可以学习到相应于目标结果的 filter，将其应用于整个图片，输出其提取到的所有有用的特征。</p><p><img src="https://pic1.zhimg.com/80/v2-d8cef747fcb56e5dea88a143ab79a83c_720w.jpg" alt=""></p><h2 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h2><h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><p>在每次卷积运算中图片会缩小且边缘的特征信息损失</p><script type="math/tex; mode=display">n\times n \to (n+2p-f+1)\times(n+2p-f+1)</script><h2 id="Stride卷积步长"><a href="#Stride卷积步长" class="headerlink" title="Stride卷积步长"></a>Stride卷积步长</h2><p>卷积的步长是构建卷积神经网络的一个基本的操作。</p><p>如前面的例子中，我们使用的 stride=1，每次的卷积运算以1个步长进行移动。下面是 stride=2 时对图片进行卷积的结果：<br><img src="https://pic3.zhimg.com/80/v2-6ae842e349ed01f48b5343ce9b5b386a_720w.jpg" alt=""></p><script type="math/tex; mode=display">n\times n \to (\frac{n+2p-f}{s}+1)\times(\frac{n+2p-f}{s}+1)</script><blockquote><p>注意，在当 $padding =\not 1$ 时，若移动的窗口落在图片外面，则不要再进行相乘的操作，丢弃边缘的数值信息，所以输出图片的最终维度为向下取整。</p></blockquote><h2 id="三维卷积"><a href="#三维卷积" class="headerlink" title="三维卷积"></a>三维卷积</h2><p>对于有RGB三个通道的图像</p><p><img src="https://pic2.zhimg.com/80/v2-94b43f2cf7f532acc039eedc2e36b811_720w.jpg" alt=""></p><p>通过卷积得到二维的矩阵</p><h3 id="多核卷积"><a href="#多核卷积" class="headerlink" title="多核卷积"></a>多核卷积</h3><p>单个卷积核应用于图片时，提取图片特定的特征，不同的卷积核提取不同的特征。如两个大小均为 $3\times3\times3$ 的卷积核分别提取图片的垂直边缘和水平边缘。</p><p><img src="https://pic1.zhimg.com/80/v2-14d2d62dd293798e516be6387bbbbadc_720w.jpg" alt=""></p><blockquote><p>在卷积神经网络中，卷积核可以看做神经元，通过学习更新每个卷积核中的参数</p></blockquote><h2 id="卷积神经网络（CNN）"><a href="#卷积神经网络（CNN）" class="headerlink" title="卷积神经网络（CNN）"></a>卷积神经网络（CNN）</h2><h3 id="单层卷积神经网络"><a href="#单层卷积神经网络" class="headerlink" title="单层卷积神经网络"></a>单层卷积神经网络</h3><p>和普通的神经网络单层前向传播的过程类似，卷积神经网络也是一个先由输入和权重及偏置做线性运算，然后得到的结果输入一个激活函数中，得到最终的输出：</p><script type="math/tex; mode=display">z^{[1]}=w^{[1]}a^{[0]}+b^{[1]}</script><script type="math/tex; mode=display">a^{[1]}=g(z^{[1]})</script><p>不同点是：在卷积神经网络中，权重和输入进行的是卷积运算。</p><h3 id="网络参数-parameter"><a href="#网络参数-parameter" class="headerlink" title="网络参数(parameter)"></a>网络参数(parameter)</h3><p>10个$3\times3\times3$的卷积核+每个核的偏置</p><p><strong>参数数量</strong></p><script type="math/tex; mode=display">(3\times3\times3+1)\times10=280</script><h3 id="标记总结"><a href="#标记总结" class="headerlink" title="标记总结"></a>标记总结</h3><p>如果<code>l</code>表示一个卷积层</p><ul><li>$f^{[l]}$:filter的维度</li><li>$p^{[l]}$:padding</li><li>$s^{[l]}$:stride</li><li>$n_C^{[l]}$:卷积核个数</li><li>$f^{[l]}\times f^{[l]}\times n_c^{[l-1]}$:filter大小</li><li>$a^{[l]} \to n_W^{[l]}\times n_H^{[l]}\times n_c^{[l]}$:Activation激活值</li><li>$f^{[l]}\times f^{[l]}\times n_c^{[l-1]}\times n_c^{[l]}$:Weight权重</li><li>$n_C^{[l]} —(1,1,1,n_C{[l]})$:bias</li><li>$ n_W^{[l-1]}\times n_H^{[l-1]}\times n_C^{[l-1]}$:input</li><li>$ n_W^{[l]}\times n_H^{[l]}\times n_C^{[l]}$:output</li><li>维度变化</li></ul><script type="math/tex; mode=display">n_H^{[l]}=[\frac{n_H^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1]</script><script type="math/tex; mode=display">n_W^{[l]}=[\frac{n_W^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1]</script><h2 id="卷积神经网络类型"><a href="#卷积神经网络类型" class="headerlink" title="卷积神经网络类型"></a>卷积神经网络类型</h2><ul><li>卷积层(Convolution)</li><li>池化层(Pooling)</li><li>全连接层(Fully Connected)</li></ul><h2 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h2><h3 id="最大池化（Max-pool）"><a href="#最大池化（Max-pool）" class="headerlink" title="最大池化（Max pool）"></a>最大池化（Max pool）</h3><p><img src="https://pic3.zhimg.com/80/v2-516cf3ce448be6a529256afd344842b6_720w.jpg" alt=""></p><h3 id="平均池化（Average-Pooling）"><a href="#平均池化（Average-Pooling）" class="headerlink" title="平均池化（Average Pooling）"></a>平均池化（Average Pooling）</h3><p><img src="https://pic2.zhimg.com/80/v2-15c3b97ce2a9cc354b2affdf0ed0d391_720w.jpg" alt=""></p><p><strong>维度变化</strong></p><script type="math/tex; mode=display">n\times n \to (\frac{n+2p-f}{s}+1)\times(\frac{n+2p-f}{s}+1)</script><p><strong>参数</strong></p><ul><li>f：filter的大小</li><li>s：stride</li><li>p：padding，很少使用</li></ul><blockquote><p>池化池没有要学习的参数，是对卷积层结果的压缩得到更加重要的特征，同时还能有效控制过拟合。</p></blockquote><h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><ul><li>把二维特征图像转化为一维向量</li><li>分类</li></ul><p>比如对于五个卷积核输出的$30\times 30 \times5$的特征图像，用一个$30\times 30 \times5\times 120$的卷积核去卷积，120为全连接层的神经元数量</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWM0LnpoaW1nLmNvbS84MC92Mi1kZTRiYTRiYWM2YWJlZDUzMDI1MDI2Zjg3N2ZkODBkMV9oZC5qcGc?x-oss-process=image/format,png" alt=""></p><h2 id="卷积神经网络案例"><a href="#卷积神经网络案例" class="headerlink" title="卷积神经网络案例"></a>卷积神经网络案例</h2><p><img src="https://pic1.zhimg.com/80/v2-875949201832bc5062806de09a3bb258_720w.jpg" alt=""></p><p>构建深度卷积的模式：</p><ul><li>随着网络的深入，提取的特征图片大小将会逐渐减小，但同时通道数量应随之增加；</li><li>Conv—Pool—Conv—Pool—Fc—Fc—Fc—softmax</li></ul><p><strong>参数</strong></p><div class="table-container"><table><thead><tr><th>layer</th><th>Activation shape</th><th>Activation size</th><th>pamameter</th></tr></thead><tbody><tr><td>Input</td><td>(32,32,3)</td><td>3027</td><td>0</td></tr><tr><td>Conv1(f=5,s=1,n=8)</td><td>(28,28,8)</td><td>6272</td><td>208</td></tr><tr><td>Pool1</td><td>(14,14,8)</td><td>1568</td><td>0</td></tr><tr><td>Conv2(f=5,s=1,n=8)</td><td>(10,10,16)</td><td>1600</td><td>416</td></tr><tr><td>Pool2</td><td>(5,5,16)</td><td>400</td><td>0</td></tr><tr><td>FC1</td><td>(120,1)</td><td>120</td><td>48001</td></tr><tr><td>FC2</td><td>(84,1)</td><td>84</td><td>10081</td><td></td></tr><tr><td>Softmax</td><td>(10,1)</td><td>10</td><td>841</td></tr></tbody></table></div><ul><li>在卷积层，仅有少量的参数；</li><li>在池化层，没有参数；</li><li>在全连接层，存在大量的参数。</li></ul><h1 id="常见卷积神经网络"><a href="#常见卷积神经网络" class="headerlink" title="常见卷积神经网络"></a>常见卷积神经网络</h1><h2 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h2><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><p>针对灰度图像<br><img src="https://pic2.zhimg.com/80/v2-47ce686eadb7eca361cb8e30e5ce44a9_720w.jpg" alt=""></p><ul><li>随着网络的深度增加，图像的大小在缩小，与此同时，通道的数量却在增加；</li><li>每个卷积层后面接一个池化层。</li></ul><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>针对彩色图像<br><img src="https://pic1.zhimg.com/80/v2-40cf21fdb32b5c45267dc8454b1c305c_720w.jpg" alt=""></p><ul><li>与LeNet相似，但网络结构更大，参数更多，表现更加出色；</li><li>使用了Relu；</li><li>使用了多个GPUs；</li></ul><h2 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h2><p>VGG卷积层和池化层均具有相同的卷积核大小，都使用 $3\times 3,stride=1,SAME$ 的卷积和$2\times2,stride=1$ 的池化。其结构如下：<br><img src="https://pic4.zhimg.com/80/v2-ace8d57233d927540b09dd62f3a02767_720w.jpg" alt=""></p><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><h3 id="残差快"><a href="#残差快" class="headerlink" title="残差快"></a>残差快</h3><p>对于一个以下结构的网路<br><img src="https://pic4.zhimg.com/80/v2-9af76e289bd4a48aa986560767108833_720w.jpg" alt=""></p><p><strong>前向传播</strong></p><script type="math/tex; mode=display">Linear:z^{[l+1]}=W^{[l+1]}a^{[l]}+b^{[l+1]}</script><script type="math/tex; mode=display">Relu:a^{[l+1]}=g(z^{[l+1]})</script><script type="math/tex; mode=display">Linear:z^{[l+2]}=W^{[l+2]}a^{[l+1]}+b^{[l+2]}</script><script type="math/tex; mode=display">Relu:a^{[l+2]}=g(z^{[l+2]})</script><hr><p>而ResNet块则将其传播过程增加了一个从$a^{[l]}$直接到$a^{[l+2]}$ 的连接，将其称之为“short cut”或者“skip connection”</p><p>也就是</p><script type="math/tex; mode=display">a^{[l+2]}=g(z^{[l+2]}+a^{[l]})</script><p><img src="https://pic3.zhimg.com/80/v2-f798208c0ea57bfc0991c280c71f5bf2_720w.jpg" alt=""></p><p><img src="https://pic2.zhimg.com/80/v2-879e090bfb52fae7112ad9896f316ccd_720w.jpg" alt=""></p><h3 id="普通神经网络和ResNet的对比"><a href="#普通神经网络和ResNet的对比" class="headerlink" title="普通神经网络和ResNet的对比"></a>普通神经网络和ResNet的对比</h3><p><img src="https://pic4.zhimg.com/80/v2-bcaf9f135b7a957a6498020c11f055b7_720w.jpg" alt=""></p><ul><li>在没有残差的普通神经网络中，训练的误差实际上是随着网络层数的加深，先减小再增加；</li><li>在有残差的ResNet中，即使网络再深，训练误差都会随着网络层数的加深逐渐减小。</li><li>ResNet对于中间的激活函数来说，有助于能够达到更深的网络，解决梯度消失和梯度爆炸的问题。</li></ul><h3 id="为什么ResNet性能更好？"><a href="#为什么ResNet性能更好？" class="headerlink" title="为什么ResNet性能更好？"></a>为什么ResNet性能更好？</h3><script type="math/tex; mode=display">a^{[l+2]}=g(z^{[l+2]}+a^{[l]})=g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})</script><p>初始化时$W^{[l+2]}=0,b^{[l+2]}=0$<br>则</p><script type="math/tex; mode=display">a^{[l+2]}=g(z^{[l+2]}+a^{[l]})=g(a^{[l]})=relu(a^{[l]})=a^{[l]}</script><p>在这里W和b可以看做开关，在不用的时候为0，这样更深结构的网络不会影响网络的性能</p><h3 id="将普通神经网络转化为ResNet"><a href="#将普通神经网络转化为ResNet" class="headerlink" title="将普通神经网络转化为ResNet"></a>将普通神经网络转化为ResNet</h3><p><img src="https://pic1.zhimg.com/80/v2-3d285acc6d2b01fc635f021303cd9c2c_720w.jpg" alt=""></p><h2 id="1-times-1卷积"><a href="#1-times-1卷积" class="headerlink" title="1$\times$1卷积"></a>1$\times$1卷积</h2><p><strong>作用</strong></p><ul><li>维度压缩：使用目标维度的 的卷积核个数。</li><li>增加非线性：保持与原维度相同的 的卷积核个数。</li></ul><p><img src="https://pic1.zhimg.com/80/v2-d277d0d16f71b796a5c5f6ee5bc4b034_720w.jpg" alt=""></p><h2 id="inception-Network"><a href="#inception-Network" class="headerlink" title="inception Network"></a>inception Network</h2><p><strong>网络结构</strong></p><p>通过padding保证维度相同</p><p><img src="https://pic1.zhimg.com/80/v2-9baabd75b8b165539fabbf2d29ef97c8_720w.jpg" alt=""></p><p>在上面的Inception结构中，应用了不同的卷积核，以及带padding的池化层。在保持输入图片大小不变的情况下，通过不同运算结果的叠加，增加了通道的数量。</p><p><img src="https://pic1.zhimg.com/80/v2-1c0d40f4f581a53d8f9c0c10ebd3e588_720w.jpg" alt=""></p><p><img src="https://pic4.zhimg.com/80/v2-8adfd8721d6ea9da0f214c406007f293_720w.jpg" alt=""></p><h3 id="使用0ne-by-one降低计算成本"><a href="#使用0ne-by-one降低计算成本" class="headerlink" title="使用0ne by one降低计算成本"></a>使用0ne by one降低计算成本</h3><p><img src="https://www.zhihu.com/equation?tex=28%5Ctimes28%5Ctimes16%5Ctimes1%5Ctimes1%5Ctimes192%3D2.4M" alt=""></p><p><img src="https://www.zhihu.com/equation?tex=28%5Ctimes28%5Ctimes32%5Ctimes5%5Ctimes5%5Ctimes16%3D10.0M" alt=""></p><h3 id="总计算成本"><a href="#总计算成本" class="headerlink" title="总计算成本"></a>总计算成本</h3><p><img src="https://www.zhihu.com/equation?tex=2.4M%2B10.0M%3D12.4M" alt=""></p><p><img src="https://pic4.zhimg.com/80/v2-5922b4756827c373937fe83c2f192607_720w.jpg" alt=""></p><h2 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h2><p>如今在深度学习领域，许多研究者都会将他们的工作共享到网络上。在我们实施自己的工作的时候，比如说做某种物体的识别分类，但是只有少量的数据集，对于从头开始训练一个深度网络结构是远远不够的。</p><p>但是我们可以应用迁移学习，应用其他研究者建立的模型和参数，用少量的数据仅训练最后自定义的softmax网络。从而能够在小数据集上达到很好的效果。</p><h2 id="数据扩充"><a href="#数据扩充" class="headerlink" title="数据扩充"></a>数据扩充</h2><ul><li>镜像翻转（Mirroring）；</li><li>随机剪裁（Random Cropping）；</li><li>色彩转换（Color shifting）：</li></ul><h1 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h1><ul><li>分类问题：判断图中是否为汽车；</li><li>目标定位：判断是否为汽车，并确定具体位置；</li><li>目标检测：检测不同物体并定位。</li></ul><p><img src="https://pic3.zhimg.com/80/v2-8bbaccec33b79c571936e6f8540baf8e_720w.jpg" alt=""></p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="https://pic2.zhimg.com/80/v2-9741b5618f05011318b2e491f0305ce9_720w.jpg" alt=""><br>输出：包含图片中存在的对象及定位框</p><ul><li>行人，0 or 1；</li><li>汽车，0 or 1；</li><li>摩托车，0 or 1；</li><li>图片背景，0 or 1；</li><li>定位框：$b_x,b_y,b_h,b_w$</li></ul><blockquote><p>其中,$b_x,b_y$表示汽车中点， $b_h,b_w$分别表示定位框的高和宽。以图片左上角为(0,0)，以右下角为(1,1)，这些数字均为位置或长度所在图片的比例大小。</p></blockquote><h3 id="Softmax层的输出"><a href="#Softmax层的输出" class="headerlink" title="Softmax层的输出"></a>Softmax层的输出</h3><script type="math/tex; mode=display">y=\begin{bmatrix}P_c&是否有目标\\b_x\\b_y\\b_h\\b_w\\c_1&行人\\c_2&汽车\\c_3&摩托\end{bmatrix}</script><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><script type="math/tex; mode=display">L(\hat{y},y)=\begin{cases}(\hat{y_1}-y_1)^2+(\hat{y_2}-y_2)^2...+(\hat{y_8}-y_8)^2 & if  y_1=1 \\ (\hat{y_1}-y_1)^2& if y_1=0\end{cases}</script><p>在实际的目标定位应用中，我们可以使用更好的方式是：</p><ul><li>softmax使用对数似然损失函数；</li><li>对边界框的四个值应用平方误差或者类似的方法；</li><li>对P_c应用logistic regression损失函数，或者平方预测误差。</li></ul><h2 id="特征检测"><a href="#特征检测" class="headerlink" title="特征检测"></a>特征检测</h2><p>输出图片特征点来进行定位<br><img src="https://pic4.zhimg.com/80/v2-6e55c8d64e63fc7a8eb79bf33bbda57b_720w.jpg" alt=""></p><h2 id="目标检测-1"><a href="#目标检测-1" class="headerlink" title="目标检测"></a>目标检测</h2><p>目标框滑动检测</p><p><img src="https://pic1.zhimg.com/80/v2-53d92d8debf3e6eddd847e3bcac3dbfc_720w.jpg" alt=""></p><ul><li>首先选定一个特定大小的窗口，将窗口内的图片输入到模型中进行预测；</li><li>以固定步幅滑动该窗口，遍历图像的每个区域，对窗内的各个小图不断输入模型进行预测；</li><li>继续选取一个更大的窗口，再次遍历图像的每个区域，对区域内是否有车进行预测；</li><li>遍历整个图像，可以保证在每个位置都能检测到是否有车。</li></ul><p><strong>缺点</strong>：计算成本巨大，每个窗口的小图都要进行卷积运算，（但在神经网络兴起之前，使用的是线性分类器，所以滑动窗口算法的计算成本较低）。</p><h3 id="如何解决？"><a href="#如何解决？" class="headerlink" title="如何解决？"></a>如何解决？</h3><p>将全连接层转化为卷积层</p><p><img src="https://pic3.zhimg.com/80/v2-e7eba2a4221dc203855463b3173afc22_720w.jpg" alt=""></p><h3 id="滑动窗口的卷积实现"><a href="#滑动窗口的卷积实现" class="headerlink" title="滑动窗口的卷积实现"></a>滑动窗口的卷积实现</h3><p><img src="https://pic3.zhimg.com/80/v2-6449d6e63fe143c92dfe7c5db9dcaf9a_720w.jpg" alt=""></p><p>我们以上面训练好的模型，输入一个 $16\times16\times3$大小的整幅图片，图中蓝色部分代表滑动窗口的大小。我们以2为大小的步幅滑动窗口，分别与卷积核进行卷积运算，最后得到4幅$10\times10\times16$大小的特征图，然而因为在滑动窗口的操作时，输入部分有大量的重叠，也就是有很多重复的运算，导致在下一层中的特征图值也存在大量的重叠，所以最后得到的第二层激活值（特征图）构成一副$12\times12\times16$大小的特征图。对于后面的池化层和全连接层也是同样的过程。</p><p>那么由此可知，滑动窗口在整幅图片上进行滑动卷积的操作过程，就等同于在该图片上直接进行卷积运算的过程。所以卷积层实现滑动窗口的这个过程，我们不需要把输入图片分割成四个子集分别执行前向传播，而是把他们作为一张图片输入到卷积神经网络中进行计算，其中的重叠部分（公共区域）可以共享大量的计算。</p><p><img src="https://pic1.zhimg.com/80/v2-79dd54e97a7dc03eb192b73161aabd48_720w.jpg" alt=""></p><p>利用卷积的方式实现滑动窗口算法的方法，提高了整体的计算效率。</p><h2 id="Bounding-Box"><a href="#Bounding-Box" class="headerlink" title="Bounding Box"></a>Bounding Box</h2><h3 id="作用-1"><a href="#作用-1" class="headerlink" title="作用"></a>作用</h3><p>解决非正方形的边框</p><p><img src="https://pic2.zhimg.com/80/v2-054b6ee470945e1237787a09d2a0cb21_720w.jpg" alt=""></p><h3 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h3><p><img src="https://pic2.zhimg.com/80/v2-ea55629d01beef467d9273eb7b39d29d_720w.jpg" alt=""></p><ul><li>在整幅图片上加上较为精细的网格，将图片分割成$n\times n$个小的图片；</li><li>采用图像分类和定位算法，分别应用在图像的$n\times n$个格子中。</li><li>定义训练标签：（对于每个网格，定义如前面的向量$y_i$）</li><li>将 $n\times n$ 个格子标签合并在一起，最终的目标输出Y的大小为:$n\times n \times 8$ （这里8是因为例子中的目标值有8个）。 </li></ul><p>通过这样的训练集训练得到目标探测的卷积网络模型。我们利用训练好的模型，将与模型输入相同大小的图片输入到训练好的网络中，得到大小为 $n\times n\times8$的预测输出。通过观察 $n\times n$不同位置的输出值，我们就能知道这些位置中是否存在目标物体，然后也能由存在物体的输出向量得到目标物体的更加精准的边界框。</p><h3 id="YOLO-notation："><a href="#YOLO-notation：" class="headerlink" title="YOLO notation："></a>YOLO notation：</h3><ul><li>将对象分配到一个格子的过程是：观察对象的中点，将该对象分配到其中点所在的格子中，（即使对象横跨多个格子，也只分配到中点所在的格子中，其他格子记为无该对象，即标记为“0”）；</li><li>YOLO显式地输出边界框，使得其可以具有任意宽高比，并且能输出更精确的坐标，不受滑动窗口算法滑动步幅大小的限制；</li><li>YOLO是一次卷积实现，并不是在 $n\times n$网格上进行$n^2$ 次运算，而是单次卷积实现，算法实现效率高，运行速度快，可以实现实时识别。</li></ul><h3 id="bounding-boxes-细节："><a href="#bounding-boxes-细节：" class="headerlink" title="bounding boxes 细节："></a>bounding boxes 细节：</h3><p>利用YOLO算法实现目标探测的时候，对于存在目标对象的网格中，定义训练标签Y的时候，边界框的指定参数的不同对其预测精度有很大的影响。</p><ul><li>对于每个网格，以左上角为(0,0)，以右下角为(1,1)；</li><li>中点$b_x,b_y$表示坐标值，在0~1之间；</li><li>宽高$b_h,b_w$表示比例值，存在&gt;1的情况。</li></ul><h2 id="交并比IoU"><a href="#交并比IoU" class="headerlink" title="交并比IoU"></a>交并比IoU</h2><p>交并比函数用来评价目标检测算法是否运作良好。<br><img src="https://pic2.zhimg.com/80/v2-5156b1ec84138eb23b879652aba3a865_720w.jpg" alt=""></p><blockquote><p>红框是人为标注的，紫框是训练结果</p></blockquote><p>对于理想边界框和目标探测算法预测得到的边界框，交并比函数计算两个边界框交集和并集之比。</p><script type="math/tex; mode=display">IoU=\frac{交集面积}{并集面积}</script><h2 id="非最大值抑制-non-max-suppression-NMS"><a href="#非最大值抑制-non-max-suppression-NMS" class="headerlink" title="非最大值抑制(non-max suppression ,NMS)"></a>非最大值抑制(non-max suppression ,NMS)</h2><p><strong>作用</strong>：保证对一个对象不做多次检测</p><p><img src="https://pic1.zhimg.com/80/v2-d786896a701bc6b22359f8a678969638_720w.jpg" alt=""><br>在上图中可能会有多个格子内检测出对象</p><h3 id="NMS的基本思想"><a href="#NMS的基本思想" class="headerlink" title="NMS的基本思想"></a>NMS的基本思想</h3><ul><li>在对$n\times n$个网格进行目标检测算法后，每个网格输出的 [公式] 为一个0~1的值，表示有车的概率大小。其中会有多个网格内存在高概率；</li><li>得到对同一个对象的多次检测，也就是在一个对象上有多个具有重叠的不同的边界框；</li><li>非最大值抑制对多种检测结果进行清理：选取最大$P_c$ 的边界框，对所有其他与该边界框具有高交并比或高重叠的边界框进行抑制；</li><li>逐一审视剩下的边界框，寻找最高的$P_c$值边界框，重复上面的步骤。</li><li>非最大值抑制，也就是说抑制那些不是最大值，却比较接近最大值的边界框。</li></ul><h3 id="以单对象检测为例"><a href="#以单对象检测为例" class="headerlink" title="以单对象检测为例"></a>以单对象检测为例</h3><ul><li>对于图片每个网格预测输出矩阵： $y=[P_c,b_x,b_y,b_h,b_w]$，其中 $P_c$表示有对象的概率；</li><li>抛弃$P_c&lt;=0.6$的边界框，也就是低概率的情况；</li><li>对剩余的边界框（while）：<ul><li>选取最大$P_c$ 值的边界框，作为预测输出边界框；</li><li>抛弃和选取的边界框$IoU&gt;=0.5$ 的剩余的边界框。</li></ul></li></ul><blockquote><p>对于多对象检测，输出标签中就会有多个分量。正确的做法是：对每个输出类别分别独立进行一次非最大值抑制。</p></blockquote><h2 id="Anchor-Box"><a href="#Anchor-Box" class="headerlink" title="Anchor Box"></a>Anchor Box</h2><p><strong>作用</strong>：解决两个目标出现在一个框内的情况<br><img src="https://pic1.zhimg.com/80/v2-12c977ce40c692b2238f4e0f98034d3c_720w.jpg" alt=""></p><ul><li><p>先定义几个Anchor Box，目标向量变为</p><script type="math/tex; mode=display">y_i=[P_c,b_x,b_y,b_h,b_w,c_1,c_2,c_3,P_c,b_x,b_y,b_h,b_w,c_1,c_2,c_3...]</script></li><li><p>不使用Anchor box：训练图片中的每个对象，根据对象的中点，分配到对应的格子中。输出大小</p></li><li>使用Anchor box：训练图片的每个对象，根据对象的中点，分配到对应的格子中，同时还分配到一个和对象形状的IoU最高的Anchor box 中。输出大小（例如两个Anchor box）$n\times n \times 16$。</li></ul><h3 id="难点问题："><a href="#难点问题：" class="headerlink" title="难点问题："></a>难点问题：</h3><ul><li>如果我们使用了两个Anchor box，但是同一个格子中却有三个对象的情况，此时只能用一些额外的手段来处理；</li><li>同一个格子中存在两个对象，但它们的Anchor box 形状相同，此时也需要引入一些专门处理该情况的手段。</li></ul><h3 id="Anchor-box-的选择："><a href="#Anchor-box-的选择：" class="headerlink" title="Anchor box 的选择："></a>Anchor box 的选择：</h3><ul><li>一般人工指定Anchor box 的形状，选择5~10个以覆盖到多种不同的形状，可以涵盖我们想要检测的对象的形状；</li><li>高级方法：K-means 算法：将不同对象形状进行聚类，用聚类后的结果来选择一组最具代表性的Anchor box，以此来代表我们想要检测对象的形状。</li></ul><h2 id="YOLO算法"><a href="#YOLO算法" class="headerlink" title="YOLO算法"></a>YOLO算法</h2><h3 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h3><ul><li>构造数据集</li><li>模型训练</li><li>运行NMS</li></ul><h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><p>假设我们要在图片中检测三种目标：行人、汽车和摩托车，同时使用两种不同的Anchor box。</p><h3 id="训练集"><a href="#训练集" class="headerlink" title="训练集"></a>训练集</h3><ul><li>输入X：同样大小的完整图片；</li><li>目标Y：使用$3\times 3$ 网格划分，输出大小$3\times 3\times2\times8$，或者$3\times 3\times16$ </li><li>对不同格子中的小图，定义目标输出向量Y。<br><img src="https://pic3.zhimg.com/80/v2-04baf38d85d19b9574a31b6a794c7d6a_720w.jpg" alt=""></li></ul><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>输入与训练集中相同大小的图片，同时得到每个格子中不同的输出结果： $3\times 3\times2\times8$<br><img src="https://pic4.zhimg.com/80/v2-5fcb40ed001cb3629e0bec69676e3eb7_720w.jpg" alt=""></p><h3 id="运行NMS"><a href="#运行NMS" class="headerlink" title="运行NMS"></a>运行NMS</h3><ul><li>假设使用了2个Anchor box，那么对于每一个网格，我们都会得到预测输出的2个bounding boxes，其中一个Pc比较高；</li></ul><p><img src="https://pic2.zhimg.com/80/v2-22c34225d06de047752ee9752bb741a5_720w.jpg" alt=""></p><ul><li><p>抛弃概率Pc值低的预测bounding boxes；<br><img src="https://pic3.zhimg.com/80/v2-a0355fddb90cc272161244376621c5e2_720w.jpg" alt=""></p></li><li><p>对每个对象（如行人、汽车、摩托车）分别使用NMS算法得到最终的预测边界框</p></li></ul><p><img src="https://pic3.zhimg.com/80/v2-638ccfeaf962131a2bccc15b8c4de816_720w.jpg" alt=""></p><h2 id="候选区域"><a href="#候选区域" class="headerlink" title="候选区域"></a>候选区域</h2><ul><li>R-CNN（Regions with convolutional networks），会在我们的图片中选出一些目标的候选区域，从而避免了传统滑动窗口在大量无对象区域的无用运算。</li><li>所以在使用了R-CNN后，我们不会再针对每个滑动窗口运算检测算法，而是只选择一些候选区域的窗口，在少数的窗口上运行卷积网络。</li><li>具体实现：运用图像分割算法，将图片分割成许多不同颜色的色块，然后在这些色块上放置窗口，将窗口中的内容输入网络，从而减小需要处理的窗口数量。<br><img src="https://pic4.zhimg.com/80/v2-813a1cce626d7c967e3c843a1dd56103_720w.jpg" alt=""></li></ul><h3 id="更快的算法："><a href="#更快的算法：" class="headerlink" title="更快的算法："></a>更快的算法：</h3><ul><li>R-CNN：给出候选区域，不使用滑动窗口，对每个候选区域进行分类识别，输出对象 标签 和 bounding box，从而在确实存在对象的区域得到更精确的边界框，但速度慢；</li><li>Fast R-CNN：给出候选区域，使用滑动窗口的卷积实现去分类所有的候选区域，但得到候选区的聚类步骤仍然非常慢；</li><li>Faster R-CNN：使用卷积网络给出候选区域。</li></ul><h1 id="CNN的其他应用"><a href="#CNN的其他应用" class="headerlink" title="CNN的其他应用"></a>CNN的其他应用</h1><h2 id="人脸识别和人脸验证"><a href="#人脸识别和人脸验证" class="headerlink" title="人脸识别和人脸验证"></a>人脸识别和人脸验证</h2><h3 id="Verification"><a href="#Verification" class="headerlink" title="Verification"></a>Verification</h3><ul><li>input image,nameID</li><li>Output true or flase</li></ul><h3 id="Recognition"><a href="#Recognition" class="headerlink" title="Recognition"></a>Recognition</h3><ul><li>K person in database</li><li>input a image</li><li>out put the id or name of the person</li></ul><h2 id="One-shot学习"><a href="#One-shot学习" class="headerlink" title="One-shot学习"></a>One-shot学习</h2><p>人脸识别的困难点在于一次学习识别，训练样例只有一个</p><h3 id="为什么不用softmax？"><a href="#为什么不用softmax？" class="headerlink" title="为什么不用softmax？"></a>为什么不用softmax？</h3><p>softmax的缺点在于加入新人后需要重新训练网络</p><h3 id="degree-of-difference-between-two-picture"><a href="#degree-of-difference-between-two-picture" class="headerlink" title="degree of difference between two picture"></a>degree of difference between two picture</h3><p>引入Similarity函数</p><script type="math/tex; mode=display">d(img_1,img_2)<\tau</script><script type="math/tex; mode=display">>\tau</script><h2 id="siamese网络"><a href="#siamese网络" class="headerlink" title="siamese网络"></a>siamese网络</h2><p><img src="http://www.ai-start.com/dl2017/images/ecd4f7ca6487b4ccb19c1f5039e9d876.png" alt=""></p><script type="math/tex; mode=display">d(x^{(1)},d^{(2)})=||f(x^{(1)})-f(x^{2})||_2^2</script><h4 id="如何训练？"><a href="#如何训练？" class="headerlink" title="如何训练？"></a>如何训练？</h4><p>定义编码函数$f(x^i)$,输入$x^i$，得到对应编码</p><h2 id="Triplet-损失"><a href="#Triplet-损失" class="headerlink" title="Triplet 损失"></a>Triplet 损失</h2><h3 id="Tripletloss-between-A-N-P"><a href="#Tripletloss-between-A-N-P" class="headerlink" title="Tripletloss between A N P"></a>Tripletloss between A N P</h3><p>定义三个变量</p><ul><li>anchor</li><li>positive正确的</li><li>nagetive错误的<script type="math/tex; mode=display">L(A,P,N)=max(||f(A)-f(P)||^2-||f(A)-f(N)||^2+\alpha)<</script><strong>存在问题</strong></li></ul><p>当A和N差别过大时梯度下降很小，网络并不能学到什么，所以在训练的时候要选取尽可能难区分的数据</p><h2 id="人脸验证与二分类"><a href="#人脸验证与二分类" class="headerlink" title="人脸验证与二分类"></a>人脸验证与二分类</h2><p>用二分类的思想训练网络</p><ul><li>输入两张图片</li><li>输出是否为同一个人<br><img src="http://www.ai-start.com/dl2017/images/c3bf61934da2f20a7d15e183c1d1d2ab.png" alt=""></li></ul><p>逻辑回归单元的处理</p><script type="math/tex; mode=display">\hat{y}=\sigma(\sum_{k=1}^{128}w_i|f(x^{(i)})_k-f(x^{(j)})_k|+b)</script><h2 id="神经风格迁移"><a href="#神经风格迁移" class="headerlink" title="神经风格迁移"></a>神经风格迁移</h2><p><img src="http://www.ai-start.com/dl2017/images/7b75c69ef064be274c82127a970461cf.png" alt=""></p><h2 id="CNN特征可视化"><a href="#CNN特征可视化" class="headerlink" title="CNN特征可视化"></a>CNN特征可视化</h2><p><img src="http://www.ai-start.com/dl2017/images/6d489f040214efb27bf0f109874b3918.png" alt=""></p><ul><li>找出让单元最大激活化的图片快<br><img src="http://www.ai-start.com/dl2017/images/2ccff4b8e125893f330414574cd03af8.png" alt=""></li><li>在越深的层会检测到越复杂的特征</li></ul><h2 id="神经风格迁移代价函数"><a href="#神经风格迁移代价函数" class="headerlink" title="神经风格迁移代价函数"></a>神经风格迁移代价函数</h2><h3 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h3><script type="math/tex; mode=display">J(G)=\alpha J_{content}(C,G)+\beta J_{style}(S,G)</script><h3 id="content-cost"><a href="#content-cost" class="headerlink" title="content cost"></a>content cost</h3><p>利用一个训练好的卷积模型，如VGG</p><ul><li>$a^{<a href="c">l</a>}$第l层的激活值</li><li>$a^{<a href="G">l</a>}$<script type="math/tex; mode=display">J_{content}(C,G)=\frac{1}{2}||a^{[l](c)}-a^{[l](G)}||^2</script></li></ul><h3 id="style-cost"><a href="#style-cost" class="headerlink" title="style cost"></a>style cost</h3><p><img src="http://www.ai-start.com/dl2017/images/e3d74c1ce2393ae4e706a1cc4024f311.png" alt=""></p><ul><li>通道1检测出垂直纹理</li><li>通道2检测出橙色</li></ul><p>现在利用相关系数描述同时出现两种特征的的概率</p><p><strong>定义风格矩阵：style matrix</strong></p><script type="math/tex; mode=display">a^{[l]}_{i,j,k}=activation (h,w,c),</script><script type="math/tex; mode=display">G^{[l](s)} =n_c^{[l]}\times n_c^{[l]}</script><script type="math/tex; mode=display">G_{kk'}^{[l](G)}=\sum_{i=1}^{n_H^{[l]}}\sum_{j=1}^{n_W^{[l]}}a^{[l]}_{i,j,k}a^{[l]}_{i,j,k'}</script><script type="math/tex; mode=display">G_{kk'}^{[l](S)}=\sum_{i=1}^{n_H^{[l]}}\sum_{j=1}^{n_W^{[l]}}a^{[l]}_{i,j,k}a^{[l]}_{i,j,k'}</script><script type="math/tex; mode=display">J^{[l]}_{style}(S,G)=\frac{1}{(2n_H^{[l]}n_W^{[l]}n_c^{[l]})^2}\sum_k\sum_{k'}(G_{kk'}^{[l](S)}-G_{kk'}^{[l](G)})</script>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep learning </tag>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Radiation</title>
      <link href="2020/06/21/Radiation/"/>
      <url>2020/06/21/Radiation/</url>
      
        <content type="html"><![CDATA[<h1 id="5-2-Green’s-Functions格林函数"><a href="#5-2-Green’s-Functions格林函数" class="headerlink" title="5.2 Green’s Functions格林函数"></a>5.2 Green’s Functions格林函数</h1><p>本章将考虑在有源情况下的麦克斯韦方程，先写出频域有源情况下的麦克斯韦方程</p><h2 id="Dyadic-Green’s-Functions"><a href="#Dyadic-Green’s-Functions" class="headerlink" title="Dyadic Green’s Functions"></a>Dyadic Green’s Functions</h2><span id="more"></span><script type="math/tex; mode=display">\begin{cases} \nabla\times\overrightarrow{E}=iw\overrightarrow{B}\\\nabla\times\overrightarrow{H}=-iw\overrightarrow{D}+\overrightarrow{J}\\\nabla\cdot\overrightarrow{B}=0\\\nabla\cdot\overrightarrow{D}=\rho\end{cases}</script><p>得到</p><script type="math/tex; mode=display">\nabla \times \nabla\times\bar E-k^2\bar E=iw\mu\bar J</script><p>为了解出这个方程，我们引入格林函数</p><script type="math/tex; mode=display">\bar E(\bar r)=iw\mu\iiint d\bar r^{'}\bar G(\bar r,\bar r^{'})\cdot\bar J(\bar r^{'})</script><blockquote><p>在数学中，格林函数是一种用来解有初始条件或边界条件的非齐次微分方程的函数。在物理学的多体理论中，格林函数常常指各种关联函数，有时并不符合数学上的定义。</p><p>从物理上看，一个数学物理方程是表示一种特定的”场”和产生这种场的”源”之间的关系。例如，热传导方程表示温度场和热源之间的关系，泊松方程表示静电场和电荷分布的关系，等等。这样，当源被分解成很多点源的叠加时，如果能设法知道点源产生的场，利用叠加原理，我们可以求出同样边界条件下任意源的场，这种求解数学物理方程的方法就叫格林函数法。而点源产生的场就叫做格林函数。</p></blockquote><p> <a href="https://www.zhihu.com/question/58746631/answer/170741359"><strong>格林函数的物理意义是什么——知乎</strong></a></p><p>其中电流J存在关系</p><script type="math/tex; mode=display">\bar J(\bar r)=\iiint d\bar r^{'}\delta(\bar r-\bar r^{'})\bar I\cdot\bar J(\bar r^{'})</script><p>化简得</p><script type="math/tex; mode=display">\nabla \times \nabla\times\bar G(\bar r,\bar r^{'})-k^2\bar G(\bar r,\bar r^{'})=\delta(\bar r-\bar r^{'})\bar I</script><p>令</p><script type="math/tex; mode=display">\bar G(\bar r,\bar r^{'})=[\bar I+\frac{1}{k^2}\nabla\nabla] g(\bar r,\bar r^{'})</script><p>代入化简</p><script type="math/tex; mode=display">\nabla^2g(\bar r,\bar r^{'})+k^2g(\bar r,\bar r^{'})=-\delta(\bar r-\bar r^{'})</script><p>当r’=0,即电流不随位置变化</p><script type="math/tex; mode=display">\nabla^2g(\bar r)+k^2g(\bar r)=-\delta(\bar r)</script><p>得到</p><script type="math/tex; mode=display">g(r)=C\frac{e^{ikr}}{r}</script><script type="math/tex; mode=display">\to g(\bar r,\bar r^{'})=\frac{e^{ik|\bar r-\bar r^{'}|}}{4\pi|\bar r-\bar r^{'}|}</script><script type="math/tex; mode=display">\to \bar E(\bar r)=iw\mu[\bar I+\frac{1}{k^2}\nabla\nabla]\cdot\iiint dr'\frac{e^{ik|\bar r-\bar r^{'}|}}{4\pi|\bar r-\bar r^{'}|}\bar J(\bar r')</script><h2 id="Radiation-Field-Approximation"><a href="#Radiation-Field-Approximation" class="headerlink" title="Radiation Field Approximation"></a>Radiation Field Approximation</h2><p>当r离辐射源很远时可以进行近似</p><script type="math/tex; mode=display">|\bar r-\bar r'|\approx r-\hat{r}\cdot\bar r'kr>>1</script><script type="math/tex; mode=display">\bar E(\bar r)=iw\mu[\bar I+\frac{1}{k^2}\nabla\nabla]\cdot\iiint dr'\frac{e^{ik|\bar r-\bar r^{'}|}}{4\pi|\bar r-\bar r^{'}|}\bar J(\bar r')\approx iw\mu[\bar I+\frac{1}{k^2}\nabla\nabla]\cdot\ \frac{e^{ikr}}{4\pi r}\iiint dr'\bar J(\bar r')e^{-i\bar k\cdot \bar r'}</script><p>定义矢量电流矩</p><script type="math/tex; mode=display">\bar f(\theta,\phi)=\iiint d \bar r'\bar J(\bar r')e^{-i\bar k\cdot \bar r'}</script><p>得到</p><script type="math/tex; mode=display">\bar E(\bar r)=iw\mu\frac{e^{ikr}}{4\pi r}(\hat{\theta}f_\theta+\hat{\phi}f_\phi)\bar H(\bar r)=iw\mu\frac{e^{ikr}}{4\pi r}(\hat{\theta}f_\theta-\hat{\phi}f_\phi)<\bar S>=\hat{r}\frac{1}{2}\sqrt{\frac{\mu}{\epsilon}}(\frac{k}{4\pi r})^2(|f_\theta|^2+|f_\phi|^2)</script><h1 id="5-3Hertzian-Dipoles"><a href="#5-3Hertzian-Dipoles" class="headerlink" title="5.3Hertzian Dipoles"></a>5.3Hertzian Dipoles</h1><h2 id="Hertzian-Electric-Dipole"><a href="#Hertzian-Electric-Dipole" class="headerlink" title="Hertzian Electric Dipole"></a>Hertzian Electric Dipole</h2><p>最简单的辐射情况是赫兹天线</p><p>其电流满足</p><script type="math/tex; mode=display">\bar J(\bar r^{'})=\hat{z}Il\delta(\bar r^{'})</script><p>对应电流矢量矩</p><script type="math/tex; mode=display">\bar f(\theta,\phi)=\hat{z}Il=(\hat{r}cos\theta-\hat{\theta}sin\theta)Il</script><p><img src="https://note.youdao.com/yws/public/resource/d7887d3005300ddc063057fa9bf70e07/xmlnote/C20E125C2113468EBEB26E32D2926873/6259" alt="image"><br><strong>电场和磁场</strong></p><script type="math/tex; mode=display">\bar E(\bar r)=-\hat{\theta}iw\mu Il\frac{e^{ikr}}{4\pi r}sin\theta</script><script type="math/tex; mode=display">\bar H(\bar r)=-\hat{\phi}ik Il\frac{e^{ikr}}{4\pi r}sin\theta</script><script type="math/tex; mode=display"><\bar S>=\hat{r}\frac{1}{2}\sqrt{\frac{\mu}{\epsilon}}(\frac{kIl}{4\pi r})^2sin^2\theta</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621210236.png" alt=""><br><strong>总辐射功率</strong></p><script type="math/tex; mode=display">P_r=\frac{4\pi}{s}\eta[\frac{kIl}{4\pi}]^2</script><p><strong>增益</strong></p><script type="math/tex; mode=display">G(\theta,\phi)=\frac{3}{2}sin^2\theta</script><p>天线方向被定义为最大增益处，垂直于偶极轴</p><script type="math/tex; mode=display">D=G(\theta,\phi)_{max}=\frac{3}{2}</script><h1 id="Linear-Dipole-Arrays多天线阵列"><a href="#Linear-Dipole-Arrays多天线阵列" class="headerlink" title="Linear Dipole Arrays多天线阵列"></a>Linear Dipole Arrays多天线阵列</h1><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621210237.png" alt=""></p><script type="math/tex; mode=display">\bar f(\theta,\phi)=\iiint d \bar r'\bar J(\bar r')e^{-i\bar k\cdot \bar r'}</script><script type="math/tex; mode=display">=\iiint dx^{'}e^{-ikx^{'}}\hat{z}Il(\delta(x^{'})+\delta(x^{'}-d)e^{i\alpha}+\delta(x^{'}-2d)e^{i2\alpha}……+\delta(x^{'}-nd)e^{in\alpha})</script><script type="math/tex; mode=display">=\hat{z}Il(\frac{1-e^{inu}}{1-e^{iu}})</script><p><strong>电场和磁场</strong></p><script type="math/tex; mode=display">\bar E(\bar r)=-\hat{\theta}iw\mu Il\frac{e^{ikr}}{4\pi r}sin\theta(\frac{1-e^{inu}}{1-e^{iu}})</script><script type="math/tex; mode=display">\bar H(\bar r)=-\hat{\phi}ik Il\frac{e^{ikr}}{4\pi r}sin\theta(\frac{1-e^{inu}}{1-e^{iu}})</script><script type="math/tex; mode=display"><\bar S>=\hat{r}\frac{1}{2}\sqrt{\frac{\mu}{\epsilon}}(\frac{kIl}{4\pi r})^2sin^2\theta</script><blockquote><p>阵列因子 $\frac{1-e^{inu}}{1-e^{iu}}$</p></blockquote><script type="math/tex; mode=display">|F(u)|=|\frac{1-e^{inu}}{1-e^{iu}}|=|\frac{sin(nu/2)}{sin(u/2)}|</script><script type="math/tex; mode=display">u=kdcos\psi-\alpha</script><script type="math/tex; mode=display">cos\psi=sin\theta cos\phi</script><p>$\alpha$为相邻天线相位差<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621210631.png" alt=""></p><script type="math/tex; mode=display">u_n=\frac{2n\pi}{N}</script><script type="math/tex; mode=display">n=\pm1,\pm2……</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621210632.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621210633.png" alt=""></p><blockquote><p>n决定主瓣最大强度，n为天线数量</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手撕麦克斯韦 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Wave Guide</title>
      <link href="2020/06/21/Wave-Guide/"/>
      <url>2020/06/21/Wave-Guide/</url>
      
        <content type="html"><![CDATA[<h2 id="Guidance-by-Conducting-Parrallel-Plates-金属波导"><a href="#Guidance-by-Conducting-Parrallel-Plates-金属波导" class="headerlink" title="Guidance by Conducting Parrallel Plates 金属波导"></a>Guidance by Conducting Parrallel Plates 金属波导</h2><h3 id="导引"><a href="#导引" class="headerlink" title="导引"></a>导引</h3><p>对于一个非常非常导电的conducting Media$\epsilon<em>t=\epsilon_g+i\sigma/w,\sigma/w\epsilon_g&gt;&gt;1$<br><span id="more"></span><br>此时TE波的菲涅尔反射系数$P</em>{ot}^{TE}=\infty,R^{TE}=-1,T^{TE}=0$</p><script type="math/tex; mode=display">\bar{E}_1=\bar E_i+\bar E_r=\hat{y}(2isink_{ix}x)e^{ik_zz}</script><script type="math/tex; mode=display">\bar{E}_1(r,t)=-\hat{y}2sink_{ix}xsin(kz-wt)</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621100436.png" alt=""><br>在x方向形成驻波</p><h3 id="模型建立"><a href="#模型建立" class="headerlink" title="模型建立"></a>模型建立</h3><p>在x=0，x=d电场强度为0的位置放置两块平行板</p><p>y方向电场</p><script type="math/tex; mode=display">E_y=Ae^{ik_xx+ik_zz}+e^{-ik_xx+ik_zz}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621100438.png" alt=""></p><p>由上述推导知</p><ul><li>当x=0<script type="math/tex; mode=display">A/B=-1</script></li><li>当x=d<script type="math/tex; mode=display">B/A=-e^{i2k_xd}</script></li></ul><p>得到</p><script type="math/tex; mode=display">e^{i2k_xd}=1=e^{i2m\pi}</script><script type="math/tex; mode=display">\to 2k_xd=2m\pi</script><h3 id="Guidance-Condition导波条件"><a href="#Guidance-Condition导波条件" class="headerlink" title="Guidance Condition导波条件"></a>Guidance Condition导波条件</h3><h3 id="k-x-frac-m-pi-d-frac-m-2d-K-0"><a href="#k-x-frac-m-pi-d-frac-m-2d-K-0" class="headerlink" title="$k_x=\frac{m\pi}{d}=\frac{m}{2d}K_0$"></a><strong>$k_x=\frac{m\pi}{d}=\frac{m}{2d}K_0$</strong></h3><p><strong>TE波m不可以取0，<br>TM波m可以取0</strong></p><blockquote><p>物理意义<br>对于一个频率的波，只有特定方向的波可以传播</p><h1 id="k-2-z-frac-m-pi-d-2-k-2"><a href="#k-2-z-frac-m-pi-d-2-k-2" class="headerlink" title="$k^2_z+(\frac{m\pi}{d})^2=k^2$"></a>$k^2_z+(\frac{m\pi}{d})^2=k^2$</h1><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621100437.png" alt=""></p></blockquote><p>当$k&lt;\frac{m\pi}{d}$时，$k_z$为虚数，波将不断衰减，无法传播</p><script type="math/tex; mode=display">m<\frac{dw\sqrt{\mu_0\epsilon_0}}{\pi}=\frac{dw}{\pi c}</script><h3 id="cutoff-spatial-frequence"><a href="#cutoff-spatial-frequence" class="headerlink" title="cutoff spatial frequence"></a>cutoff spatial frequence</h3><script type="math/tex; mode=display">k_{cm}=\frac{m\pi}{d}=\frac{m}{2d}K_0</script><p>$k&lt;k_{cm}$时波延z方向指数衰减</p><script type="math/tex; mode=display">\lambda<=\lambda=\frac{2\pi}{k_{c1}}=2d</script><blockquote><p>只有波长小于2d的波可以传导</p></blockquote><p>对于$\frac{m}{2d}&lt;k&lt;\frac{(m+1)}{2d}$只有m个TE波的模式和m+1个TM波的模式</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621142928.png" alt=""></p><p>m越小，群速度越大,传递信息量越大，模式越低，传递信息月快</p><script type="math/tex; mode=display">v_p=\frac{c}{\sqrt{1-(k_x/k)^2}}</script><script type="math/tex; mode=display">v_g=c\sqrt{1-(k_x/k)^2}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621142807.png" alt=""></p><h2 id="Excitation-of-Modes-in-Parrllel-Plate-Waveguides平行波导中模的激发"><a href="#Excitation-of-Modes-in-Parrllel-Plate-Waveguides平行波导中模的激发" class="headerlink" title="Excitation of Modes in Parrllel-Plate Waveguides平行波导中模的激发"></a>Excitation of Modes in Parrllel-Plate Waveguides平行波导中模的激发</h2><p>上述推导假设平行板不存在电荷，当波导存在线电流时，$\sigma$不为无穷时，根据边界条件</p><script type="math/tex; mode=display">\bar J_s=\hat{y}J_s(x)</script><script type="math/tex; mode=display">H_x|_{z=0_+}-H_x|_{z=0_-}=J_s(x)</script><p>由欧姆定律</p><script type="math/tex; mode=display">J=\sigma E</script><script type="math/tex; mode=display">\to E_w=\frac{J}{\sigma}</script><script type="math/tex; mode=display">H_w=\frac{1}{\eta}E_w</script><script type="math/tex; mode=display">\eta=\sqrt{\frac{\mu}{\epsilon_m}}</script><script type="math/tex; mode=display">(\epsilon_m=(\epsilon+i\frac{\sigma}{\epsilon w}))</script><script type="math/tex; mode=display">\bar{S}=\bar{E}\times\bar{H}^*</script><p>知金属里出现电场,会有能量向外泄露</p><h2 id="TM-Modes-in-Parrallel-Plate-Waveguide-TM波在波导的传播模式"><a href="#TM-Modes-in-Parrallel-Plate-Waveguide-TM波在波导的传播模式" class="headerlink" title="TM Modes in Parrallel-Plate Waveguide TM波在波导的传播模式"></a>TM Modes in Parrallel-Plate Waveguide TM波在波导的传播模式</h2><script type="math/tex; mode=display">\bar H=\hat{y}\bar H_0(e^{ik_xx+ik_zz}+e^{-ik_xx+ik_zz})</script><script type="math/tex; mode=display">=\hat{y}2H_0cosk_xxe^{ik_zz}</script><script type="math/tex; mode=display">\bar{E}=2H_0\frac{1}{w\epsilon}[\hat{x}k_zcosk_xx-\hat{z}k_xsink_xx]e^{ik_zz}</script><script type="math/tex; mode=display"><S_z>=\frac{k}{2w\epsilon}|H_0|^2</script><p>当kx=0</p><script type="math/tex; mode=display">\bar{H}=\hat{y}H_0e^{ikz}</script><script type="math/tex; mode=display">\bar{E}=\hat{y}\eta H_0e^{ikz}</script><blockquote><p>$\eta$：特性阻抗</p></blockquote><h2 id="Guide-Wave-in-a-Symmetric-Slab-Dielectric-Waveguide介质波导"><a href="#Guide-Wave-in-a-Symmetric-Slab-Dielectric-Waveguide介质波导" class="headerlink" title="Guide Wave in a Symmetric Slab Dielectric Waveguide介质波导"></a>Guide Wave in a Symmetric Slab Dielectric Waveguide介质波导</h2><p>对于TE波</p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621143653.png" alt=""></p><script type="math/tex; mode=display">\bar E_0=\hat{y}e^{ik_{0x}x}e^{ik_{z}z}</script><script type="math/tex; mode=display">\bar E_1=\hat{y}(Ae^{ik_{1x}x}e^{ik_{z}z}+e^{-ik_{1x}x}e^{ik_{z}z})</script><script type="math/tex; mode=display">\bar E_2=\hat{y}e^{ik_{2x}x}e^{ik_{z}z}</script><hr><script type="math/tex; mode=display">p_{10}=\frac{i\mu_1\alpha}{\mu_0k_{0x}}=iP_{10I}</script><script type="math/tex; mode=display">R_{10}=R_{12}=\frac{1-P_{10}}{1+P_{10}}=e^{i2\phi_{10}}</script><blockquote><p>每次反射后都会增加相位</p></blockquote><p>得到导波条件</p><script type="math/tex; mode=display">k_{1x}d+2\phi_{10}=m\pi</script><script type="math/tex; mode=display">\phi=-tan^{-1}(\frac{\mu k_{1xI}}{\mu_1 k_x})</script><script type="math/tex; mode=display">(k_{1x}d)^2+(k_{0x}d)^2=(k_1^2-k_0^2)d^2</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621225140.png" alt=""><br><strong>cut off</strong></p><script type="math/tex; mode=display">k_1=k_{cm}=\frac{m\pi}{d\sqrt{1-\mu_0\epsilon_0/\mu_1\epsilon_1}}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621224502.png" alt=""></p><blockquote><p>光纤通讯的原理就是介质波导</p><h2 id="Cylindrical-Rectangular-Waveguides矩形波导"><a href="#Cylindrical-Rectangular-Waveguides矩形波导" class="headerlink" title="Cylindrical Rectangular Waveguides矩形波导"></a>Cylindrical Rectangular Waveguides矩形波导</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621185840.png" alt=""></p></blockquote><p>为了将问题化简，引入一个在xy平面的向量s</p><script type="math/tex; mode=display">\bar E=\bar E_s+\hat{z}\bar E_z</script><script type="math/tex; mode=display">\bar E_s=\hat{x}\bar E_x+\hat{y}\bar E_y</script><p>定义</p><script type="math/tex; mode=display">\nabla=\nabla_s+\hat{z}\frac{\partial}{\partial z}</script><p>则对于麦克斯韦方程，可以转化为</p><script type="math/tex; mode=display">(\nabla_s+\hat{z}\frac{\partial}{\partial z})\times(\bar E_s+\hat{z}\bar E_z)=iw\mu(\bar H_s+\hat{z}\bar H_z)</script><script type="math/tex; mode=display">(\nabla_s+\hat{z}\frac{\partial}{\partial z})\times(\bar H_s+\hat{z}\bar H_z)=-iw\epsilon(\bar E_s+\hat{z}\bar E_z)</script><script type="math/tex; mode=display">\to \begin{cases}\nabla_s\times\bar E_s=iw\mu\bar H_z\\\nabla_s\times\hat{z}\bar E_z+\hat{z}\frac{\partial}{\partial z}\bar \times E_s=iw\mu\bar H_s\\\nabla_s\times\bar H_s=-iw\mu\bar E_z\\\nabla_s\times\hat{z}\bar H_z+\hat{z}\frac{\partial}{\partial z}\bar \times H_s=-iw\mu\bar E_s\end{cases}</script><script type="math/tex; mode=display">\to\begin{cases}\bar E_s=\frac{1}{w^2\mu\epsilon-k_z^2}[\nabla_s\frac{\partial}{\partial z}E_z+iw\mu(\nabla_s\times H_z)]\\\bar H_s=\frac{1}{w^2\mu\epsilon-k_z^2}[\nabla_s\frac{\partial}{\partial z}H_z-iw\epsilon(\nabla_s\times E_z)]\end{cases}</script><p>代回原方程</p><script type="math/tex; mode=display">(\nabla_s+w^2\mu\epsilon-k_z^2)\begin{bmatrix}E_z\\H_z\end{bmatrix}=0</script><ul><li>$E_z=0,H_z=\not0$<h4 id="TE-wave"><a href="#TE-wave" class="headerlink" title="TE wave"></a>TE wave</h4><script type="math/tex; mode=display">H_z=cosk_xcosk_yye^{ik_zz}</script><script type="math/tex; mode=display">H_x=\frac{-ik_xk_z}{w^2\mu\epsilon-k_z^2}sink_xxcosk_yye^{ik_zz}</script><script type="math/tex; mode=display">H_y=\frac{-ik_yk_z}{w^2\mu\epsilon-k_z^2}cosk_xxsink_yye^{ik_zz}</script><script type="math/tex; mode=display">E_x=\frac{-iw\epsilon k_y}{w^2\mu\epsilon-k_z^2}cosk_xxsink_yye^{ik_zz}</script><script type="math/tex; mode=display">E_y=\frac{iw\epsilon k_x}{w^2\mu\epsilon-k_z^2}sink_xxcosk_yye^{ik_zz}</script><script type="math/tex; mode=display">E_z=0</script></li></ul><hr><ul><li>$E_z= \not0,H_z=0$<h4 id="TM-wave"><a href="#TM-wave" class="headerlink" title="TM wave"></a>TM wave</h4></li></ul><script type="math/tex; mode=display">E_z=sink_xsink_yye^{ik_zz}</script><script type="math/tex; mode=display">E_x=\frac{ik_xk_z}{w^2\mu\epsilon-k_z^2}cosk_xxsink_yye^{ik_zz}</script><script type="math/tex; mode=display">E_y=\frac{ik_yk_z}{w^2\mu\epsilon-k_z^2}sink_xxcosk_yye^{ik_zz}</script><script type="math/tex; mode=display">H_x=\frac{-iw\epsilon k_y}{w^2\mu\epsilon-k_z^2}sink_xxcosk_yye^{ik_zz}</script><script type="math/tex; mode=display">H_y=\frac{iw\epsilon k_x}{w^2\mu\epsilon-k_z^2}cosk_xxsink_yye^{ik_zz}</script><script type="math/tex; mode=display">H_z=0</script><h4 id="波导条件"><a href="#波导条件" class="headerlink" title="波导条件"></a>波导条件</h4><script type="math/tex; mode=display">k_xa=m\pi</script><script type="math/tex; mode=display">k_yb=n\pi</script><script type="math/tex; mode=display">(\frac{m\pi}{a})^2+(\frac{n\pi}{b})^2+k_z^2=k^2</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621162300.png" alt=""></p><p><strong>cut off spatial frequence</strong></p><script type="math/tex; mode=display">k_{cmn}=\sqrt{(m\pi/a)^2+(n\pi/b)^2}</script><h2 id="Cylindrical-Circular-Waveguides圆柱波导"><a href="#Cylindrical-Circular-Waveguides圆柱波导" class="headerlink" title="Cylindrical Circular Waveguides圆柱波导"></a>Cylindrical Circular Waveguides圆柱波导</h2><h3 id="Bessel-Function"><a href="#Bessel-Function" class="headerlink" title="Bessel Function"></a>Bessel Function</h3><p>把亥姆霍兹方程化为柱坐标系</p><script type="math/tex; mode=display">(\nabla_s+w^2\mu\epsilon-k_z^2)\begin{bmatrix}E_z\\H_z\end{bmatrix}=0</script><script type="math/tex; mode=display">\to(\frac{1}{\rho}\frac{\partial}{\partial\rho}(\rho\frac{\partial}{\partial\rho})+\frac{1}{\rho^2}\frac{\partial^2}{\partial\phi^2}+k_\rho^2)\begin{bmatrix}E_z\\H_z\end{bmatrix}=0</script><blockquote><p>其中$k_\rho^2=w^2\mu\epsilon-k_z^2$</p></blockquote><p>令$\xi=k_p\rho$</p><p>我们知道以下方程的解为Bessel Function$J_m(\xi)$和Neumann function$N_m(\xi)$和他们的线性组合</p><script type="math/tex; mode=display"> [\frac{1}{\xi}\frac{d}{d\xi}(\xi\frac{d}{d\xi})+(1-\frac{m^2}{\xi^2})]B(\xi)=0</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621190508.png" alt=""></p><p>其中</p><script type="math/tex; mode=display">J_m(\xi) \to cos(\xi)</script><script type="math/tex; mode=display">N_m(\xi) \to sin(\xi)</script><script type="math/tex; mode=display">H_m^{(1)}=J_m(\xi)+iN_m(\xi) \to e^{i\xi}</script><script type="math/tex; mode=display">H_m^{(2)}=J_m(\xi)-iN_m(\xi) \to e^{-i\xi}</script><blockquote><p>往水里扔石头的时候，水波向远处传播的时候，波浪为Bessel函数，在无穷远时接近cos</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621190507.png" alt=""></p><h3 id="Circular-Metallic-Waveguides"><a href="#Circular-Metallic-Waveguides" class="headerlink" title="Circular Metallic Waveguides"></a>Circular Metallic Waveguides</h3><h4 id="TM"><a href="#TM" class="headerlink" title="TM"></a>TM</h4><script type="math/tex; mode=display">H_z=0</script><script type="math/tex; mode=display">E_z=J_m(k_\rho\rho)\begin{bmatrix}sinm\phi\\cosm\phi\end{bmatrix}e^{ik_zz}</script><h5 id="色散关系"><a href="#色散关系" class="headerlink" title="色散关系"></a>色散关系</h5><script type="math/tex; mode=display">k_z^2+k_\rho^2=w^2\mu\epsilon</script><h5 id="水平方向电磁场"><a href="#水平方向电磁场" class="headerlink" title="水平方向电磁场"></a>水平方向电磁场</h5><script type="math/tex; mode=display">E_\rho=\frac{ik_zk_\rho}{w^2\mu\epsilon-k_z^2}J_m^{'}(k_\rho\rho)\begin{bmatrix}sinm\phi\\cosm\phi\end{bmatrix}e^{ik_zz}</script><script type="math/tex; mode=display">E_\phi=\frac{ik_z}{w^2\mu\epsilon-k_z^2}\frac{m}{\rho}J_m(k_\rho\rho)\begin{bmatrix}cosm\phi\\-sinm\phi\end{bmatrix}e^{ik_zz}</script><script type="math/tex; mode=display">H_\rho=\frac{-iw\epsilon}{w^2\mu\epsilon-k_z^2}\frac{m}{\rho}J_m(k_\rho\rho)\begin{bmatrix}cosm\phi\\-sinm\phi\end{bmatrix}e^{ik_zz}</script><script type="math/tex; mode=display">H_\phi=\frac{iw\epsilon k_\rho}{w^2\mu\epsilon-k_z^2}J_m^{'}(k_\rho\rho)\begin{bmatrix}sinm\phi\\cosm\phi\end{bmatrix}e^{ik_zz}</script><h5 id="导波条件"><a href="#导波条件" class="headerlink" title="导波条件"></a>导波条件</h5><script type="math/tex; mode=display">J_m(k_\rho a)=0</script><script type="math/tex; mode=display">\to k_z=\sqrt{w^2\mu\epsilon-(\xi_{mn}/a)^2}</script><script type="math/tex; mode=display">\to k_{cmn}=\xi_{mn}/a</script><h4 id="TE"><a href="#TE" class="headerlink" title="TE"></a>TE</h4><script type="math/tex; mode=display">E_z=0</script><script type="math/tex; mode=display">H_z=J_m(k_\rho\rho)\begin{bmatrix}sinm\phi\\cosm\phi\end{bmatrix}e^{ik_zz}</script><h5 id="色散关系-1"><a href="#色散关系-1" class="headerlink" title="色散关系"></a>色散关系</h5><script type="math/tex; mode=display">k_z^2+k_\rho^2=w^2\mu\epsilon</script><h5 id="水平方向电磁场-1"><a href="#水平方向电磁场-1" class="headerlink" title="水平方向电磁场"></a>水平方向电磁场</h5><script type="math/tex; mode=display">H_\rho=\frac{ik_zk_\rho}{w^2\mu\epsilon-k_z^2}J_m^{'}(k_\rho\rho)\begin{bmatrix}sinm\phi\\cosm\phi\end{bmatrix}e^{ik_zz}</script><script type="math/tex; mode=display">H_\phi=\frac{ik_z}{w^2\mu\epsilon-k_z^2}\frac{m}{\rho}J_m(k_\rho\rho)\begin{bmatrix}cosm\phi\\-sinm\phi\end{bmatrix}e^{ik_zz}</script><script type="math/tex; mode=display">E_\rho=\frac{-iw\epsilon}{w^2\mu\epsilon-k_z^2}\frac{m}{\rho}J_m(k_\rho\rho)\begin{bmatrix}cosm\phi\\-sinm\phi\end{bmatrix}e^{ik_zz}</script><script type="math/tex; mode=display">E_\phi=\frac{iw\epsilon k_\rho}{w^2\mu\epsilon-k_z^2}J_m^{'}(k_\rho\rho)\begin{bmatrix}sinm\phi\\cosm\phi\end{bmatrix}e^{ik_zz}</script><h5 id="导波条件-1"><a href="#导波条件-1" class="headerlink" title="导波条件"></a>导波条件</h5><script type="math/tex; mode=display">J_m^{'}(k_\rho a)=0</script><script type="math/tex; mode=display">\to k_z=\sqrt{w^2\mu\epsilon-(\xi_{mn}^{'}/a)^2}</script><script type="math/tex; mode=display">\to k_{cmn}=\xi_{mn}^{'}/a</script><h1 id="Resonance谐振腔"><a href="#Resonance谐振腔" class="headerlink" title="Resonance谐振腔"></a>Resonance谐振腔</h1><h4 id="波导条件-1"><a href="#波导条件-1" class="headerlink" title="波导条件"></a>波导条件</h4><script type="math/tex; mode=display">k_xa=m\pi</script><script type="math/tex; mode=display">k_yb=n\pi</script><script type="math/tex; mode=display">k_zd=p\pi</script><script type="math/tex; mode=display">(\frac{m}{a\pi})^2+(\frac{n}{b\pi})^2+(\frac{m}{p\pi})^2=k_r^2</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621192844.png" alt=""></p><blockquote><p>其中m,n,p为整数，所以只有特定的频率可以传播</p></blockquote><p>比如，当m=n=1,p=0,$TM_{110}$</p><script type="math/tex; mode=display">E_z=E_0sin\frac{\pi x}{a}sin\frac{\pi y}{b}</script><script type="math/tex; mode=display">H_x=\frac{-i\pi}{w\mu b}E_0sin\frac{\pi x}{a}cos\frac{\pi y}{b}</script><script type="math/tex; mode=display">H_y=\frac{i\pi}{w\mu a}E_0cos\frac{\pi x}{a}sin\frac{\pi y}{b}</script><script type="math/tex; mode=display">k_r=\sqrt{(m/2a)^2+(n/2b)^2+(p/2d)^2}K_o</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621192845.png" alt=""></p><p>当谐振腔材料不是完全的PEC材料时，在谐振腔内存在损耗和泄露，能量会存在衰减的现象</p><p>设总能量函数为U(t)</p><script type="math/tex; mode=display">u(t)=U(0)e^{-2\alpha_rt}</script><script type="math/tex; mode=display">P_d=-\frac{dU}{dt}=2\alpha_rU</script><p><strong>质量因素</strong></p><script type="math/tex; mode=display">Q=w_r/2\alpha_r=w_rU/P_d=w_0L/R</script><script type="math/tex; mode=display">w_r=k_r/\sqrt{\mu\epsilon}</script>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手撕麦克斯韦 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reflection and Transmission</title>
      <link href="2020/06/21/Reflection-and-Transmission/"/>
      <url>2020/06/21/Reflection-and-Transmission/</url>
      
        <content type="html"><![CDATA[<h3 id="界面处场的关系"><a href="#界面处场的关系" class="headerlink" title="界面处场的关系"></a>界面处场的关系</h3><script type="math/tex; mode=display">\hat{n}\times(\bar{E}_1-\bar{E}_2)=0</script><script type="math/tex; mode=display">\hat{n}\times(\bar{H}_1-\bar{H}_2)=\bar{J}_s</script><script type="math/tex; mode=display">\hat{n}\cdot(\bar{B}_1-\bar{B}_2)=0</script><script type="math/tex; mode=display">\hat{n}\cdot(\bar{D}_1-\bar{D}_2)=0</script><span id="more"></span><p>任何圆极化和椭圆极化可以分解为两个线性极化波TE,TM波求解</p><blockquote><p>Js为界面处电荷密度</p></blockquote><p>为了解决反射和透射问题，我们把波分为两组类型，TE波和TM波，因为任何波都可以用着两种波线性组合</p><h2 id="Reflection-and-Transmission-of-TE-Waves"><a href="#Reflection-and-Transmission-of-TE-Waves" class="headerlink" title="Reflection and Transmission of TE Waves"></a>Reflection and Transmission of TE Waves</h2><p>电场场垂直入射平面的波，垂直极化<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621100430.png" alt=""></p><h3 id="incidence-wave-入射波"><a href="#incidence-wave-入射波" class="headerlink" title="incidence wave 入射波"></a>incidence wave 入射波</h3><script type="math/tex; mode=display">\bar{k}_i=\hat{x}k_x+\hat{z}k_z</script><script type="math/tex; mode=display">\bar{E}_i=\hat{y}e^{i\bar{k}_i\cdot\bar{r}}</script><script type="math/tex; mode=display">\bar{H}_i=\frac{1}{w\mu}\bar k_i\times\bar E_i</script><script type="math/tex; mode=display">\bar S_i=\bar k_i\frac{1}{w\mu}|\bar E_i|^2</script><h3 id="Reflected-Wave-反射波"><a href="#Reflected-Wave-反射波" class="headerlink" title="Reflected Wave 反射波"></a>Reflected Wave 反射波</h3><script type="math/tex; mode=display">\bar{k}_r=\hat{x}k_x+\hat{z}k_z</script><script type="math/tex; mode=display">\bar{E}_r=\hat{y}Re^{i\bar{k}_r\cdot\bar{r}}</script><script type="math/tex; mode=display">\bar{H}_r=\frac{1}{w\mu}\bar k_r\times\bar E_r</script><script type="math/tex; mode=display">\bar S_r=\bar k_r\frac{1}{w\mu}|\bar E_r|^2</script><h3 id="Transmission-Wave透射波"><a href="#Transmission-Wave透射波" class="headerlink" title="Transmission Wave透射波"></a>Transmission Wave透射波</h3><script type="math/tex; mode=display">\bar{k}_t=\hat{x}k_x+\hat{z}k_z</script><script type="math/tex; mode=display">\bar{E}_t=\hat{y}Te^{i\bar{k}_t\cdot\bar{r}}</script><script type="math/tex; mode=display">\bar{H}_i=\frac{1}{w\mu}\bar k_t\times\bar E_t</script><script type="math/tex; mode=display">\bar S_i=\bar k_t\frac{1}{w\mu}|\bar E_t|^2</script><h3 id="反射界面关系"><a href="#反射界面关系" class="headerlink" title="反射界面关系"></a>反射界面关系</h3><script type="math/tex; mode=display">\hat{y}e^{i\bar{k}_i\cdot\bar{r}}+\hat{y}Re^{i\bar{k}_r\cdot\bar{r}}=\hat{y}Te^{i\bar{k}_t\cdot\bar{r}}</script><p>令x=0</p><script type="math/tex; mode=display">e^{k_{iz}z}+Re^{k_{rz}z}=Te^{k_{tz}z}</script><p>对于任意z成立</p><h2 id="相位连续方程"><a href="#相位连续方程" class="headerlink" title="相位连续方程"></a>相位连续方程</h2><script type="math/tex; mode=display">k_{iz}=k_{rz}=k_{tz}</script><blockquote><p><strong>相位连续方程</strong>phase matching condition代表入射波和折射波反射波的切向分量是连续的</p></blockquote><script type="math/tex; mode=display"> k_isin\theta_i=k_rsin\theta_r=k_tsin\theta_t</script><script type="math/tex; mode=display"> \theta_i=\theta_r</script><script type="math/tex; mode=display"> \sqrt{\mu\epsilon}sin\theta_i=\sqrt{\mu_t\epsilon_t}sin\theta_t</script><script type="math/tex; mode=display"> n_isin\theta_i=n_tsin\theta_t</script><script type="math/tex; mode=display">\frac{sin\theta_i}{sin\theta_t}=\frac{k_t}{k_i}=\frac{n_t}{n_i}</script><h4 id="斯涅耳定理Snell’s-law"><a href="#斯涅耳定理Snell’s-law" class="headerlink" title="斯涅耳定理Snell’s law"></a>斯涅耳定理Snell’s law</h4><script type="math/tex; mode=display">n_t=c\sqrt{\mu_t\epsilon_t}</script><p>所以在之后解决折射问题可以利用相位连续方程画出k surface图直接解决</p><h2 id=""><a href="#" class="headerlink" title=""></a><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621101742.png" alt=""></h2><p>上述方程化简后</p><script type="math/tex; mode=display">1+R=T</script><p>为了求解上述二元方程，还需要另一个方程，利用界面处磁场关系（无电流情况）</p><script type="math/tex; mode=display">\frac{1}{w\mu}k_{ix}e^{k_{iz}z}+\frac{1}{w\mu}Rk_{rx}e^{k_{rz}z}=\frac{1}{w\mu_t}k_{tx}e^{k_{tz}z}</script><script type="math/tex; mode=display">\frac{k_x}{\mu}(1-R)=\frac{k_{tx}}{\mu_t}T</script><p>联立1+R=T</p><script type="math/tex; mode=display">\begin{cases}R=\frac{1-p_{0t}}{1+p_{0t}}\\T=\frac{2}{1+p_{0t}}\end{cases}</script><script type="math/tex; mode=display">p_{0t}=\frac{\mu k_{tx}}{\mu_t k_{ix}}</script><blockquote><p>$p_{0t}$:Fresnd refletion菲涅尔反射系数,安培的好朋友,这表明折射波和反射波的能量由界面两边的介质和入射角度决定</p><h3 id="能量关系"><a href="#能量关系" class="headerlink" title="能量关系"></a>能量关系</h3></blockquote><script type="math/tex; mode=display"><\bar S_i>=\frac{1}{2}Re\{\bar k\frac{1}{w\mu}\}</script><script type="math/tex; mode=display"><\bar S_r>=\frac{1}{2}Re\{\bar k_r\frac{1}{w\mu}|R|^2\}</script><script type="math/tex; mode=display"><\bar S_t>=\frac{1}{2}Re\{\bar k_t^*\frac{1}{w\mu_t^*}|T|^2e^{i({k_{tx}-k_{tx}^*)x}}\}</script><h2 id="Reflection-and-Transmission-of-TM-Waves"><a href="#Reflection-and-Transmission-of-TM-Waves" class="headerlink" title="Reflection and Transmission of TM Waves"></a>Reflection and Transmission of TM Waves</h2><p>磁场垂直入射平面的波<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621100429.png" alt=""><br>和TE波完全相同，作以下变化</p><script type="math/tex; mode=display">E \to H</script><script type="math/tex; mode=display">H\to E</script><script type="math/tex; mode=display">\mu\to \epsilon</script><p>则</p><script type="math/tex; mode=display">p_{0t}=\frac{\epsilon k_{tx}}{\epsilon_t k_{ix}}</script><h2 id="Total-Reflection-and-Critical-Angle-全反射和临界角"><a href="#Total-Reflection-and-Critical-Angle-全反射和临界角" class="headerlink" title="Total Reflection and Critical Angle 全反射和临界角"></a>Total Reflection and Critical Angle 全反射和临界角</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621100432.png" alt=""></p><p>透射光和入射光满足</p><script type="math/tex; mode=display">k_isin\theta_i=k_tsin\theta_t</script><p>临界条件</p><script type="math/tex; mode=display">k_isin\theta_c=k_t</script><p>当</p><script type="math/tex; mode=display">k_{iz}>k_tk_{tx}=\sqrt{k_t^2-k_{iz}^2}=ik_{txI}</script><p>此时</p><script type="math/tex; mode=display">\bar E_t=\hat{y}Te^{ik\cdot r}=\hat{y}Te^{-k_{txI}x}e^{ik_zz}</script><blockquote><p>得到沿着z方向传播，延x方向衰减的波：倏逝波</p></blockquote><h3 id="能量去哪了？"><a href="#能量去哪了？" class="headerlink" title="能量去哪了？"></a>能量去哪了？</h3><p>全反射</p><script type="math/tex; mode=display">R=\frac{1-p_{0t}}{1+p_{0t}}=\frac{1-ip_{0tI}}{1+ip_{0tI}}=e^{i2\phi_t}</script><script type="math/tex; mode=display">\phi_t=-tan^{-1}p_{0tI}=-tan^{-1}\frac{\epsilon k_{txI}}{\epsilon_tk_x}</script><script type="math/tex; mode=display">T=1+R=2cos\phi_te^{i\phi_t}</script><h4 id="定义r，t-能量反射系数，能量折射系数"><a href="#定义r，t-能量反射系数，能量折射系数" class="headerlink" title="定义r，t,能量反射系数，能量折射系数"></a>定义r，t,能量反射系数，能量折射系数</h4><script type="math/tex; mode=display">r=\frac{s_{rx}}{s_{ix}}</script><script type="math/tex; mode=display">t=\frac{s_{tx}}{s_{ix}}</script><script type="math/tex; mode=display">r+t=1</script><script type="math/tex; mode=display"><S_t(r,t)>=\hat{z}k_z\frac{2cos^2\phi}{w\epsilon_t}e^{-2k_{txI}x}</script><blockquote><p>能量只延z方向传播，没有x的分量，为表面波</p></blockquote><h5 id="TE，TM波的另一种说法"><a href="#TE，TM波的另一种说法" class="headerlink" title="TE，TM波的另一种说法"></a>TE，TM波的另一种说法</h5><blockquote><p>perpendicularly polaried wave<br>垂直极化波<br>paralally polarized wave，水平极化波 </p><p>以电场方向为准</p></blockquote><h2 id="Backward-Waves-and-Negative-Refraction-反向波和负折射"><a href="#Backward-Waves-and-Negative-Refraction-反向波和负折射" class="headerlink" title="Backward Waves and Negative Refraction 反向波和负折射"></a>Backward Waves and Negative Refraction 反向波和负折射</h2><p>对于21世纪Media$\mu_t&lt;0,\epsilon_t&lt;0$</p><p><strong>这个时候能量如何传播？</strong></p><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621100431.png" alt=""><br>此时k和s的方向相反，即波的传播方向和能量传播方向相反，且入射波和折射波在同一界面</p><h2 id="Total-Transmission-and-Brewster-Angle全透射和布鲁斯特角"><a href="#Total-Transmission-and-Brewster-Angle全透射和布鲁斯特角" class="headerlink" title="Total Transmission and Brewster Angle全透射和布鲁斯特角"></a>Total Transmission and Brewster Angle全透射和布鲁斯特角</h2><p><strong>存在全反射波，那么是否存在全透射波？</strong></p><p>对于TM波</p><script type="math/tex; mode=display">R^{TM}=0</script><p>则</p><script type="math/tex; mode=display">p_{0t}=\frac{\epsilon k_{tx}}{\epsilon_t k_{ix}}=1</script><script type="math/tex; mode=display">\epsilon k_{tx}=\epsilon_t k_{ix}</script><script type="math/tex; mode=display">\epsilon k_{t}cos\theta_t=\epsilon_t k_{i}cos\theta_i</script><script type="math/tex; mode=display">\epsilon w\sqrt{\mu_t\epsilon_t}cos\theta_t=\epsilon_t w\sqrt{\mu\epsilon}cos\theta_i</script><script type="math/tex; mode=display">k_{i}cos\theta_t= k_{t}cos\theta_i</script><p>又由</p><script type="math/tex; mode=display">k_{iz}=k_{tz}</script><script type="math/tex; mode=display"> k_{i}sin\theta_t= k_{t}sin\theta_i</script><ul><li><p>$\theta_i=\theta_t$</p><script type="math/tex; mode=display">k_i=k_t\epsilon_t=\epsilon</script><blockquote><p>介质相同，全透射</p></blockquote></li><li><p>$\theta_i+\theta_t=\pi$</p><script type="math/tex; mode=display">tan\theta_i=\frac{k_t}{k_i}=\sqrt{\frac{\epsilon_t}{\epsilon}}\theta_i=tan^{-1}\frac{k_t}{k_i}</script><blockquote><p>布鲁斯特角和全反射角区别</p></blockquote></li></ul><script type="math/tex; mode=display">\theta_C=sin^{-1}\frac{k_t}{k_i}\theta_B=tan^{-1}\frac{k_t}{k_i}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621100433.png" alt=""></p><h2 id="Double-Refraction-in-Uniaxial-Media-双折射"><a href="#Double-Refraction-in-Uniaxial-Media-双折射" class="headerlink" title="Double Refraction in Uniaxial Media 双折射"></a>Double Refraction in Uniaxial Media 双折射</h2><p>对于k in uniaxial media</p><script type="math/tex; mode=display">k^e=\frac{w}{\sqrt{v(\kappa cos^2\theta+\kappa_zsin^2\theta)}}</script><script type="math/tex; mode=display">k^o=\frac{w}{\sqrt{\nu\kappa}}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621100434.png" alt=""></p><h3 id="光轴旋转后？"><a href="#光轴旋转后？" class="headerlink" title="光轴旋转后？"></a>光轴旋转后？</h3><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621112424.png" alt=""></p><h2 id="Reflection-and-Transmission-by-a-Layered-Medium-多层介质的折射和反射"><a href="#Reflection-and-Transmission-by-a-Layered-Medium-多层介质的折射和反射" class="headerlink" title="Reflection and Transmission by a Layered Medium 多层介质的折射和反射"></a>Reflection and Transmission by a Layered Medium 多层介质的折射和反射</h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200621100435.png" alt=""><br>下面推导TM波的情况</p><h3 id="Regiom-0"><a href="#Regiom-0" class="headerlink" title="Regiom #0"></a>Regiom #0</h3><script type="math/tex; mode=display">\bar H_0=\bar H_i+\bar H_r</script><script type="math/tex; mode=display">=\hat{y}(e^{i(k_{0x}x+k_zz)}+Re^{i(k_{rx}x+k_zz)})</script><script type="math/tex; mode=display">\bar k_i=\hat{x}k_{0x}+\hat{z}k_z</script><script type="math/tex; mode=display">k_{0x}^2+k_z^2=k_0^2=w^2\mu_0\epsilon_0</script><script type="math/tex; mode=display">\bar k_r=-\hat{x}k_{0x}+\hat{z}k_z</script><script type="math/tex; mode=display">\bar E_0=-\frac{1}{w\epsilon_0}[\hat zk_{0x}e^{i(k_{0x}x+k_zz)}-\hat{z}k_{0x}Re^{i(-k_{0x}x+k_zz)}-\hat{x}k_ze^{i(k_{0x}x+k_zz)}-\hat{x}k_zRe^{i(-k_{0x}x+k_zz)}]</script><h3 id="Regiom-l"><a href="#Regiom-l" class="headerlink" title="Regiom #l"></a>Regiom #l</h3><script type="math/tex; mode=display">\bar H_l=\hat{y}(A_le^{i(k_{lx}x+k_zz)}+B_le^{i(k_{lx}x+k_zz)})</script><script type="math/tex; mode=display">\bar E_l=-\frac{1}{w\epsilon_0}[\hat z A_lk_{lx}e^{i(k_{lx}x+k_zz)}-\hat{z}k_{lx}B_le^{i(-k_{lx}x+k_zz)}-\hat{x}A_lk_ze^{i(k_{lx}x+k_zz)}-\hat{x}k_zB_le^{i(-k_{lx}x+k_zz)}]</script><h3 id="At-x-dl"><a href="#At-x-dl" class="headerlink" title="At x=dl"></a>At x=dl</h3><script type="math/tex; mode=display">A_le^{ik_{lx}d_l}+B_le^{-ik_{lx}d_l}=A_{l+1}e^{ik_{(l+1)x}d_{l}}+B_{l+1}e^{-ik_{(l+1)x}d_{l}}</script><script type="math/tex; mode=display">A_le^{ik_{lx}d_l}-B_le^{-ik_{lx}d_l}=p_{l(l+1)}[A_{l+1}e^{ik_{(l+1)x}d_{l}}+B_{l+1}e^{-ik_{(l+1)x}d_{l}}]</script><script type="math/tex; mode=display">p_{l(l+1)}=\frac{\epsilon_l k_{(l+1)x}}{\epsilon_{l+1}k_{lx}}</script><p>列出每层边界的方程，求解</p><p>用矩阵表示</p><script type="math/tex; mode=display">\begin{bmatrix}A_{l+1}\\B_{l+1}\end{bmatrix}=\bar V_{(l+1)l}\cdot\begin{bmatrix}A_{l}\\B_{l}\end{bmatrix}=\frac{1+P_{(l+1)l}}{2}\begin{bmatrix}e^{-i(k_{(l+1)x}-k_{lx})d_l}&R_{(l+1)l}e^{-i(k_{(l+1)x}+k_{lx})d_l}\\R_{(l+1)l}e^{i(k_{(l+1)x}+k_{lx})d_l}&e^{i(k_{(l+1)x}-k_{lx})d_l}\end{bmatrix}\cdot\begin{bmatrix}A_{l+1}\\B_{l+1}\end{bmatrix}</script>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手撕麦克斯韦 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KDB</title>
      <link href="2020/06/19/KDB/"/>
      <url>2020/06/19/KDB/</url>
      
        <content type="html"><![CDATA[<h2 id="wave-vector-k的引入"><a href="#wave-vector-k的引入" class="headerlink" title="wave vector k的引入"></a>wave vector k的引入</h2><p>由亥姆霍兹方程</p><script type="math/tex; mode=display">(\nabla^2+w^2\mu\epsilon)\overrightarrow{E}=0</script><span id="more"></span><p>解得</p><script type="math/tex; mode=display">\overrightarrow{E}(\overrightarrow{r})=\overrightarrow{E}e^{i(k_xx+k_yy+k_zz)}</script><script type="math/tex; mode=display">k_x^2+k_y^2+k_z^2=w^2\mu\epsilon=k^2</script><p>定义wave vector k</p><script type="math/tex; mode=display">\overrightarrow{k}=\hat{x}k_x+\hat{y}k_y+\hat{z}k_z</script><p>position vector</p><script type="math/tex; mode=display">\overrightarrow{r}=\hat{x}x+\hat{y}y+\hat{z}z</script><p>则</p><script type="math/tex; mode=display">\overrightarrow{k}\cdot\overrightarrow{r}=k_xx+k_yy+k_zz</script><p>则</p><script type="math/tex; mode=display">\overrightarrow{E}(\overrightarrow{r})=\overrightarrow{E}e^{i\overrightarrow{k}\cdot\overrightarrow{r}}</script><p>代回麦克斯韦方程</p><script type="math/tex; mode=display">    \begin{cases}    \overrightarrow{k}\times\overrightarrow{E}=w\mu\overrightarrow{H}\\    \overrightarrow{k}\times\overrightarrow{H}=-w\epsilon\overrightarrow{E}\\    \overrightarrow{k}\cdot\overrightarrow{E}=0\\    \overrightarrow{k}\cdot\overrightarrow{H}=0    \end{cases}</script><p>波印廷矢量</p><script type="math/tex; mode=display">        \overrightarrow{S}=\frac{1}{2}Re        \begin{cases}\frac{\overrightarrow{k}}{w\epsilon}|\overrightarrow{H}|^2\\\frac{\overrightarrow{k^*}}{w\mu^*}|\overrightarrow{E}|^2        \end{cases}</script><p><strong>k矢量的意义</strong></p><ul><li>波是平面波</li><li>波的方向确定</li><li>$k=2\pi/\lambda$可以计算波长，频率<h2 id="KDB坐标系"><a href="#KDB坐标系" class="headerlink" title="KDB坐标系"></a>KDB坐标系</h2></li></ul><p>对于平面波</p><script type="math/tex; mode=display">\overrightarrow{k}\times\overrightarrow{E}=w\overrightarrow{B}</script><script type="math/tex; mode=display">\overrightarrow{k}\times\overrightarrow{H}=-w\overrightarrow{D}</script><script type="math/tex; mode=display">\overrightarrow{k}\cdot\overrightarrow{B}=0\overrightarrow{k}\cdot\overrightarrow{D}=0</script><blockquote><p>可得k垂直于BD构成的平面，BD不一定互相垂直，EH也不一定在BD平面内(垂直：perpendicular)</p></blockquote><p>定义三个单位向量$\hat{e_1},\hat{e_2},\hat{e_3}$,$\hat{e_3}$和k方向相同<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200619201317.png" alt=""></p><p><strong>转换矩阵</strong>（将xyz坐标系转换到KDB坐标系）</p><script type="math/tex; mode=display">    \bar{T}=\begin{bmatrix}    sin\varPhi&-cos\varPhi&0，\\    cos\theta cos\varPhi&cos\theta sin\varPhi&-sin\theta\\    sin\theta cos\varPhi&sin\theta sin\varPhi &cos\theta    \end{bmatrix}</script><script type="math/tex; mode=display">    \bar{T}^{-1}=\begin{bmatrix}    sin\varPhi&cos\theta cos\varPhi&sin\theta cos\varPhi\\    -cos\varPhi&cos\theta sin\varPhi&sin\theta sin\varPhi\\    0&-sin\theta&cos\theta    \end{bmatrix}</script><p>对于任意矢量</p><script type="math/tex; mode=display">        \overrightarrow{A}=        \begin{bmatrix}        A_x\\        A_y\\        A_z        \end{bmatrix}</script><script type="math/tex; mode=display">        \overrightarrow{A_k}=        \begin{bmatrix}        A_1\\        A_2\\        A_3        \end{bmatrix}</script><p>存在</p><script type="math/tex; mode=display">\overrightarrow{A_k}=\bar{T}\cdot\overrightarrow{A}</script><p>对于任意场量</p><script type="math/tex; mode=display">\overrightarrow{E}=\bar\kappa\cdot\overrightarrow{D}+\bar\chi\cdot\overrightarrow{B}</script><script type="math/tex; mode=display">\overrightarrow{H}=\bar\nu\cdot\overrightarrow{B}+\bar\gamma\cdot\overrightarrow{D}</script><script type="math/tex; mode=display">\overrightarrow{D}=\bar\epsilon\overrightarrow{E}+\bar\xi\overrightarrow{H}</script><script type="math/tex; mode=display">\overrightarrow{B}=\bar\mu\overrightarrow{H}+\bar\zeta\overrightarrow{E}</script><p>转化为KDB坐标系</p><script type="math/tex; mode=display">\overrightarrow{E}=(\bar{T}\cdot\bar\kappa\cdot \bar{T}^{-1})\cdot\overrightarrow{D}+(\bar{T}\cdot\bar\chi\cdot \bar{T}^{-1})\cdot\overrightarrow{B}</script><script type="math/tex; mode=display">\overrightarrow{H}=(\bar{T}\cdot\bar\nu\cdot \bar{T}^{-1})\overrightarrow{D}+(\bar{T}\cdot\bar\gamma\cdot \bar{T}^{-1})\cdot\overrightarrow{B}</script><script type="math/tex; mode=display">\overrightarrow{D}=(\bar{T}\cdot\bar\epsilon\cdot \bar{T}^{-1})\cdot\overrightarrow{E}+(\bar{T}\cdot\bar\xi\cdot \bar{T}^{-1})\cdot\overrightarrow{H}</script><script type="math/tex; mode=display">\overrightarrow{B}=(\bar{T}\cdot\bar\mu\cdot \bar{T}^{-1})\cdot\overrightarrow{H}+(\bar{T}\cdot\bar\zeta\cdot \bar{T}^{-1})\cdot\overrightarrow{E}</script><p>我们可以得到</p><script type="math/tex; mode=display">\bar\kappa_k=\bar{T}\cdot\bar\kappa\cdot \bar{T}^{-1}</script><script type="math/tex; mode=display">\bar\chi_k=\bar{T}\cdot\bar\chi\cdot \bar{T}^{-1}</script><script type="math/tex; mode=display">\bar\nu_k=\bar{T}\cdot\bar\nu\cdot \bar{T}^{-1}</script><script type="math/tex; mode=display">\bar\gamma_k=\bar{T}\cdot\bar\gamma\cdot \bar{T}^{-1}</script><script type="math/tex; mode=display">\bar\epsilon_k=\bar{T}\cdot\bar\epsilon\cdot \bar{T}^{-1}</script><script type="math/tex; mode=display">\bar\xi_k=\bar{T}\cdot\bar\xi\cdot \bar{T}^{-1}</script><script type="math/tex; mode=display">\bar\mu_k=\bar{T}\cdot\bar\mu\cdot \bar{T}^{-1}</script><script type="math/tex; mode=display">\bar\zeta_k=\bar{T}\cdot\bar\zeta\cdot \bar{T}^{-1}</script><p>在KDB坐标系的物质本构方程</p><script type="math/tex; mode=display">\overrightarrow{E}=\bar\kappa_k\cdot\overrightarrow{D}+\bar\chi_k\cdot\overrightarrow{B}</script><script type="math/tex; mode=display">\overrightarrow{H}=\bar\nu_k\cdot\overrightarrow{D}+\bar\gamma_k\cdot\overrightarrow{B}</script><script type="math/tex; mode=display">\overrightarrow{D}=\bar\epsilon_k\overrightarrow{E}+\bar\xi_k\overrightarrow{H}</script><script type="math/tex; mode=display">\overrightarrow{B}=\bar\mu_k\overrightarrow{H}+\bar\zeta_k\overrightarrow{E}</script><blockquote><p><strong>为什么引入KDB坐标系？</strong></p><blockquote><p>把微分方程转化为线性方程，讲话运算，解决双各向异性这样最复杂的介质的麦克斯韦方程，将所有的介质归一化</p></blockquote></blockquote><h2 id="Maxwell-Equation-in-KDB-System"><a href="#Maxwell-Equation-in-KDB-System" class="headerlink" title="Maxwell Equation in KDB System"></a>Maxwell Equation in KDB System</h2><p>下面我们将运用KDB坐标系解决麦克斯韦方程</p><p>KDB坐标系麦克斯韦方程组</p><script type="math/tex; mode=display">        \begin{cases}        \overrightarrow{k}\times\overrightarrow{E}_k=w\overrightarrow{B}_k\\        \overrightarrow{k}\times\overrightarrow{H}_k=-w\overrightarrow{D}_k\\        \overrightarrow{k}\cdot\overrightarrow{B}_k=0\\        \overrightarrow{k}\cdot\overrightarrow{D}_k=0        \end{cases}</script><p>由</p><script type="math/tex; mode=display">\overrightarrow{k}\cdot\overrightarrow{B}_k=0\\\overrightarrow{k}\cdot\overrightarrow{D}_k=0</script><p>知</p><script type="math/tex; mode=display">B_3,D_3=0</script><p>因为</p><script type="math/tex; mode=display">\bar k=\hat{e_3}k</script><p>所以</p><script type="math/tex; mode=display">w\bar B_k=k\hat{e_3}\times(\hat{e_1}E_1+\hat{e_2}E_2+\hat{e_3}E_3)</script><script type="math/tex; mode=display">-w\bar D_k=k\hat{e_3}\times(\hat{e_1}H_1+\hat{e_2}H_2+\hat{e_3}H_3)</script><p>全部转化为DB的线性方程</p><script type="math/tex; mode=display">wB_2=kE_1=k(\kappa_{11} D+\kappa_{22} D+\chi _{11}B+\chi_{12} B)</script><script type="math/tex; mode=display">wB_1=kE_2=-k(\kappa_{21} D+\kappa_{22} D+\chi_{21} B+\chi_{22} B)</script><script type="math/tex; mode=display">wD_2=-kH_1=-k(\nu_{11}B_1+\nu_{12}B_2+\gamma_{11}D_1+\gamma_{12}D_2)</script><script type="math/tex; mode=display">wD_1=-kH_2=-k(\nu_{21}B_1+\nu_{22}B_2+\gamma_{21}D_1+\gamma_{22}D_2)</script><p>令u=w/k<br>移项可以化简为</p><script type="math/tex; mode=display">        \begin{pmatrix}        \kappa_{11}&\kappa_{22}\\        \kappa_{21}&\kappa_{22}        \end{pmatrix}        \begin{pmatrix}        D_1,\\        D_2        \end{pmatrix}=-        \begin{pmatrix}        \chi_{11}&\chi_{12}-u\\        \chi_{21}+u&\chi_{22}        \end{pmatrix}        \begin{pmatrix}        B_1,\\        B_2        \end{pmatrix}</script><script type="math/tex; mode=display">        \begin{pmatrix}        \nu_{11}&\nu_{22}\\        \nu_{21}&\nu_{22}        \end{pmatrix}        \begin{pmatrix}        B_1,\\        B_2        \end{pmatrix}        =-\begin{pmatrix}        \gamma_{11}&\gamma_{12}+u\\        \gamma_{21}-u&\gamma_{22}        \end{pmatrix}        \begin{pmatrix}        D_1,\\        D_2        \end{pmatrix}</script><blockquote><p>可以得到均匀介质色散关系</p></blockquote><h2 id="各向同性介质中的波"><a href="#各向同性介质中的波" class="headerlink" title="各向同性介质中的波"></a>各向同性介质中的波</h2><p>现在用KDB坐标系去推导最简单的介质，各向同性介质的麦克斯韦方程</p><script type="math/tex; mode=display">\bar E=\kappa\bar D</script><script type="math/tex; mode=display">\bar H=\nu\bar B</script><p>在KDB坐标系下</p><script type="math/tex; mode=display">\bar E_k=\kappa\bar D_k</script><script type="math/tex; mode=display">\bar H_k=\nu\bar B_k</script><p>代入KDB矩阵方程</p><script type="math/tex; mode=display">        \kappa        \begin{pmatrix}        D_1\\        D_2        \end{pmatrix}=        \begin{pmatrix}        0&u\\        -u&0        \end{pmatrix}        \begin{pmatrix}        B_1\\        B_2        \end{pmatrix}</script><script type="math/tex; mode=display">        \nu        \begin{pmatrix}        B_1\\        B_2        \end{pmatrix}        =\begin{pmatrix}        0&-u\\        u&0        \end{pmatrix}        \begin{pmatrix}        D_1\\        D_2        \end{pmatrix}</script><p>消去B得到</p><script type="math/tex; mode=display">        \begin{pmatrix}        u^2-\kappa\nu&0\\        0&u^2-\kappa\nu        \end{pmatrix}        \begin{pmatrix}        D_1,\\        D_2        \end{pmatrix}=0</script><p>解得</p><script type="math/tex; mode=display">u^2-\kappa\nu=0</script><ul><li>$D_1=D_2=0$<ul><li>没有场</li></ul></li><li>$D_ 1 \not= 0, D_2=0$<ul><li>e1方向极化波</li></ul></li><li>$D_1=0,D_2\not= 0$<ul><li>e2方向极化波</li></ul></li><li>$D_1\not= 0,D_2\not= 0$<ul><li>任意方向极化波</li></ul></li></ul><h2 id="Wave-in-uniaxial-Media单各向异性介质"><a href="#Wave-in-uniaxial-Media单各向异性介质" class="headerlink" title="Wave in uniaxial Media单各向异性介质"></a>Wave in uniaxial Media单各向异性介质</h2><p>加入磁场后的plasma media，手机通讯</p><p>物质本构方程</p><script type="math/tex; mode=display">\bar E=\bar\kappa\bar D</script><script type="math/tex; mode=display">\bar H=\nu\bar B</script><p>其中</p><script type="math/tex; mode=display">\bar\kappa=        \begin{pmatrix}        \kappa&0&0\\        0&\kappa&0\\        0&0&\kappa_z        \end{pmatrix}</script><script type="math/tex; mode=display">\bar\kappa_k=\bar{T}\cdot\bar\kappa\cdot \bar{T}^{-1}=\bar\kappa=    \begin{pmatrix}    \kappa&0&0\\    0&\kappa cos^2\theta+\kappa_zsin^2\theta&(\kappa-\kappa_z)sin\theta cos\theta\\    0&(\kappa-\kappa_z)sin\theta cos\theta&\kappa sin^2\theta+\kappa_zcos^2\theta    \end{pmatrix}</script><script type="math/tex; mode=display">    \begin{pmatrix}    u^2-\kappa_{11}\nu&0\\    0&u^2-\kappa_{22}\nu    \end{pmatrix}    \begin{pmatrix}    D_1\\    D_2    \end{pmatrix}=0</script><p>解得</p><ul><li>$D_1=D_2=0$<ul><li>没有场</li></ul></li><li>$D<em> 1  \not= 0, D_2=0，u^2-\kappa</em>{11}\nu=0$<ul><li>e1方向极化波</li><li>$u=\pm \sqrt{\nu\kappa_{11}}=\pm \sqrt{\nu\kappa}$</li></ul></li></ul><script type="math/tex; mode=display">\bar D_k=\hat{e}_1D_1</script><script type="math/tex; mode=display">\bar B_k=\hat{e}_2\frac{u}{\nu}D_1</script><script type="math/tex; mode=display">\bar H_k=\hat{e}_2uD_1</script><script type="math/tex; mode=display">\bar E_k=\hat{e}_1\kappa D_1</script><ul><li>$D<em>1=0,D_2\not=0,u^2-\kappa</em>{22}\nu=0$<ul><li>e2方向极化波</li><li>$u=\pm \sqrt{\nu\kappa_{22}}=\pm \sqrt{\nu(\kappa cos^2\theta+\kappa_zsin^2\theta)}$<blockquote><p>速度和角度相关，称为非寻常波,相位和k传播方向相反</p></blockquote></li></ul></li></ul><script type="math/tex; mode=display">\bar D_k=\hat{e}_2D_2</script><script type="math/tex; mode=display">\bar B_k=\hat{e}_1\frac{u}{\nu}D_2</script><script type="math/tex; mode=display">\bar H_k=-\hat{e}_2uD_2</script><script type="math/tex; mode=display">\bar E_k=\hat{e}_2\kappa_{22} D_2+\hat{e}_3(\kappa-\kappa_{z})sin\theta cos\theta D_2</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200619213636.png" alt=""></p><ul><li>$D<em>1\not=0,D_2\not=0,u^2-\kappa</em>{11}\nu=u^2-\kappa_{22}\nu=0$<ul><li>则$\kappa<em>{11}=\kappa</em>{22}$,不满足</li></ul></li></ul><blockquote><p>双折射现象</p><blockquote><p>当电磁波进入单轴介质中时，将会分解为两种速度不同的线性极化特征波，分别对应2,3两种情况</p></blockquote></blockquote><h2 id="wave-in-Gyrotropic-Media-陀螺介质中的波"><a href="#wave-in-Gyrotropic-Media-陀螺介质中的波" class="headerlink" title="wave in Gyrotropic Media 陀螺介质中的波"></a>wave in Gyrotropic Media 陀螺介质中的波</h2><p>物质本构方程</p><script type="math/tex; mode=display">\bar E=\bar\kappa\bar D</script><script type="math/tex; mode=display">\bar H=\nu\bar B</script><p>其中</p><script type="math/tex; mode=display">        \bar\kappa=        \begin{pmatrix}        \kappa&ik_g&0\\        -ik_g&\kappa&0\\        0&0&\kappa_z        \end{pmatrix}</script><script type="math/tex; mode=display">\bar\kappa_k=\bar{T}\cdot\bar\kappa\cdot \bar{T}^{-1}=\bar\kappa=        \begin{pmatrix}        \kappa&ik_gcos\theta&ik_gsin\theta\\        -ik_gcos\theta&\kappa cos^2\theta+\kappa_zsin^2\theta&(\kappa-\kappa_z)sin\theta cos\theta\\        ik_gsin\theta&(\kappa-\kappa_z)sin\theta cos\theta&\kappa sin^2\theta+\kappa_zcos^2\theta        \end{pmatrix}</script><script type="math/tex; mode=display">        \begin{pmatrix}        u^2-\kappa\nu&-i\nu\kappa_gcos\theta\\        i\nu\kappa_gcos\theta&u^2-\nu(\kappa cos^2\theta+\kappa_zsin^2\theta)        \end{pmatrix}        \begin{pmatrix}        D_1\\        D_2        \end{pmatrix}=0</script><h3 id="Faraday-Rotation"><a href="#Faraday-Rotation" class="headerlink" title="Faraday Rotation"></a>Faraday Rotation</h3><p>当波向z方向传播时，$\theta$=0</p><script type="math/tex; mode=display">        \begin{pmatrix}        u^2-\kappa\nu&-i\nu\kappa_g\\        i\nu\kappa_g&u^2-\nu\kappa         \end{pmatrix}        \begin{pmatrix}        D_1\\        D_2        \end{pmatrix}=0</script><p>我们得到</p><script type="math/tex; mode=display">\frac{D_2}{D_1}=\frac{u^2-\nu\kappa}{i\nu\kappa_g}=-\frac{i\nu\kappa_g}{u^2-\nu\kappa}</script><script type="math/tex; mode=display">u^2-\nu\kappa=\pm\nu\kappa_g</script><script type="math/tex; mode=display">u=w/k=\sqrt{\nu(\kappa\pm\kappa_g)}</script><script type="math/tex; mode=display">\frac{D_2}{D_1}=\pm i</script><p>波可以分解为两个垂直的线性极化波，所以波为圆极化波</p><ul><li>左旋</li></ul><script type="math/tex; mode=display">k^{I}=w/\sqrt{\nu(\kappa+\kappa_g)}</script><script type="math/tex; mode=display">\frac{D_2}{D_1}=-i</script><script type="math/tex; mode=display">\bar D=\hat{e_1}D_1-i\hat{e_2}D_1</script><script type="math/tex; mode=display">=\bar D_R+iD_I</script><ul><li>右旋</li></ul><script type="math/tex; mode=display">k^{II}=w/\sqrt{\nu(\kappa-\kappa_g)}</script><script type="math/tex; mode=display">\frac{D_2}{D_1}=i</script><p>当线性极化波$\bar D=\hat{e}_12D_0$,z的正方向入射该介质时，将波分解成</p><script type="math/tex; mode=display">\bar D=D_0(\hat{e}_1+\hat{e}_2i)+D_0(\hat{e}_1-\hat{e}_2i)</script><p>假设介质厚度为d</p><script type="math/tex; mode=display">\bar D=D_0(\hat{e}_1+\hat{e}_2i)e^{ik^Iz}+D_0(\hat{e}_1-\hat{e}_2i)e^{ik^{II}z}</script><script type="math/tex; mode=display">    =\hat{e}_1D_0(e^{i\Phi_I}+e^{i\Phi_{II}})+\hat{e}_2D_0(e^{i\Phi_I}-e^{i\Phi_{II}})</script><script type="math/tex; mode=display">\Phi_I=k^Iz_0=\frac{wz_0}{\sqrt{v(\kappa+\kappa_g)}}</script><script type="math/tex; mode=display">\Phi_{II}=k^Iz_0=\frac{wz_0}{\sqrt{v(\kappa-\kappa_g)}}</script><script type="math/tex; mode=display">\frac{D_2}{D_1}=i\frac{e^{i\Phi_I}-e^{i\Phi_{II}}}{e^{i\Phi_I}+e^{i\Phi_{II}}}=-tan\frac{\Phi_{II}-\Phi_I}{2}</script><blockquote><p>经过该介质后线极化波方向发生改变，称为法拉第旋转，z方向+负号</p></blockquote><h2 id="Wave-in-Bianisotropic-Media"><a href="#Wave-in-Bianisotropic-Media" class="headerlink" title="Wave in Bianisotropic Media"></a>Wave in Bianisotropic Media</h2><script type="math/tex; mode=display">\bar E=        \begin{pmatrix}        \kappa&0&0\\        0&\kappa&0\\        0&0&\kappa_z        \end{pmatrix}        \cdot\bar D        +\begin{pmatrix}        \chi&0&0\\        0&\chi&0\\        0&0&\chi_z        \end{pmatrix}        \cdot\bar B</script><script type="math/tex; mode=display">\bar H=        \begin{pmatrix}        \gamma&0&0\\        0&\gamma&0\\        0&0&\gamma_z        \end{pmatrix}        \cdot\bar D        +\begin{pmatrix}        \nu&0&0\\        0&\nu&0\\        0&0&\nu_z        \end{pmatrix}        \cdot\bar B</script><p>其中</p><script type="math/tex; mode=display">\bar\kappa_k=\bar{T}\cdot\bar\kappa\cdot \bar{T}^{-1}=\bar\kappa=        \begin{pmatrix}        \kappa&0&0\\        0&\kappa cos^2\theta+\kappa_zsin^2\theta&(\kappa-\kappa_z)sin\theta cos\theta\\        0&(\kappa-\kappa_z)sin\theta cos\theta&\kappa sin^2\theta+\kappa_zcos^2\theta        \end{pmatrix}</script><h2 id="Chiral-Media（手性介质）"><a href="#Chiral-Media（手性介质）" class="headerlink" title="Chiral Media（手性介质）"></a>Chiral Media（手性介质）</h2><script type="math/tex; mode=display">\overrightarrow{E}=\kappa\overrightarrow{D}-i\chi\overrightarrow{B}</script><script type="math/tex; mode=display">\overrightarrow{B}=i\chi\overrightarrow{D}+\nu\overrightarrow{B}</script><script type="math/tex; mode=display">\frac{D_2}{D_1}=\frac{u^2-\nu\kappa+\chi^2}{2i\chi\mu}=-\frac{2i\chi\mu}{u^2-\nu\kappa+\chi^2}</script><script type="math/tex; mode=display">u=\sqrt{\nu\kappa}\pm\chi</script><script type="math/tex; mode=display">\frac{D_2}{D_1}=\pm i</script><ul><li>左旋</li></ul><script type="math/tex; mode=display">k^{I}=w/(\sqrt{\nu\kappa}+\chi)</script><script type="math/tex; mode=display">\frac{D_2}{D_1}=-i</script><ul><li>右旋</li></ul><script type="math/tex; mode=display">k^{II}=w/(\sqrt{\nu\kappa}-\chi)</script><script type="math/tex; mode=display">\frac{D_2}{D_1}=i</script><p>当线性极化波$\bar D=\hat{e}_12D_0$入射该介质时，将波分解成</p><script type="math/tex; mode=display">\bar D=D_0(\hat{e}_1+\hat{e}_2i)+D_0(\hat{e}_1-\hat{e}_2i)</script><p>假设介质厚度为d</p><script type="math/tex; mode=display">\bar D=D_0(\hat{e}_1+\hat{e}_2i)e^{ik^Iz}+D_0(\hat{e}_1-\hat{e}_2i)e^{ik^{II}z}</script><script type="math/tex; mode=display">=\hat{e}_1D_0(e^{i\Phi_I}+e^{i\Phi_{II}})+\hat{e}_2D_0(e^{i\Phi_I}-e^{i\Phi_{II}})</script><script type="math/tex; mode=display">\Phi_I=k^Iz_0=\frac{wz_0}{\sqrt{\nu\kappa}+\chi}</script><script type="math/tex; mode=display">\Phi_{II}=k^Iz_0=\frac{wz_0}{\sqrt{\nu\kappa}-\chi}</script><script type="math/tex; mode=display">\to \frac{D_2}{D_1}=i\frac{e^{i\Phi_I}-e^{i\Phi_{II}}}{e^{i\Phi_I}+e^{i\Phi_{II}}}=-tan\frac{\Phi_{II}-\Phi_I}{2}</script><blockquote><p>optical activity:旋光性，旋光性和法拉第旋转有本质区别</p></blockquote><h2 id="重点归纳"><a href="#重点归纳" class="headerlink" title="重点归纳"></a>重点归纳</h2><p>如何解任意介质的麦克斯韦方程？</p><ol><li><strong>根据物质本构关系确定对于KDB坐标系下的本构关系</strong><script type="math/tex; mode=display">\bar{T}=\begin{bmatrix}sin\varPhi&-cos\varPhi&0\\cos\theta cos\varPhi&cos\theta sin\varPhi&-sin\theta\\sin\theta cos\varPhi&sin\theta sin\varPhi &cos\theta\end{bmatrix}</script>2.<strong>代入以下矩阵方程</strong></li></ol><script type="math/tex; mode=display">        \begin{pmatrix}        \kappa_{11}&\kappa_{22}\\        \kappa_{21}&\kappa_{22}        \end{pmatrix}        \begin{pmatrix}        D_1\\        D_2        \end{pmatrix}=-        \begin{pmatrix}        \chi_{11}&\chi_{12}-u\\        \chi_{21}+u&\chi_{22}        \end{pmatrix}        \begin{pmatrix}        B_1\\        B_2        \end{pmatrix}</script><script type="math/tex; mode=display">        \begin{pmatrix}        \nu_{11}&\nu_{22}\\        \nu_{21}&\nu_{22}        \end{pmatrix}        \begin{pmatrix}        B_1\\        B_2        \end{pmatrix}        =-\begin{pmatrix}        \gamma_{11}&\gamma_{12}+u\\        \gamma_{21}-u&\gamma_{22}        \end{pmatrix}        \begin{pmatrix}        D_1\\        D_2        \end{pmatrix}</script><p>3.<strong>分类讨论</strong></p><ul><li>反对角元素为0直接解方程</li><li>反对角元素不为0用D1，D2之比解方程</li></ul>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手撕麦克斯韦 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bianisotropic media</title>
      <link href="2020/06/19/Bianisotropic-media/"/>
      <url>2020/06/19/Bianisotropic-media/</url>
      
        <content type="html"><![CDATA[<h2 id="Anisotropic-Media各向异性介质"><a href="#Anisotropic-Media各向异性介质" class="headerlink" title="Anisotropic Media各向异性介质"></a>Anisotropic Media各向异性介质</h2><p>物质本构关系</p><script type="math/tex; mode=display">\overrightarrow{D}=\bar{\epsilon}\overrightarrow{E}</script><script type="math/tex; mode=display">\overrightarrow{B}=\bar{\mu}\overrightarrow{H}</script><blockquote><p>各方向介电值，磁导率不同</p></blockquote><span id="more"></span><h3 id="最简单的z轴为光轴的单轴晶体"><a href="#最简单的z轴为光轴的单轴晶体" class="headerlink" title="最简单的z轴为光轴的单轴晶体"></a>最简单的z轴为光轴的单轴晶体</h3><p>石英，方解石，存在双折射现象</p><script type="math/tex; mode=display">\bar{\epsilon}=\begin{bmatrix}\epsilon&0&0\\0&\epsilon&0\\0&0&\epsilon_z\end{bmatrix}</script><blockquote><p>$\epsilon_z&gt;\epsilon$正单轴 positive uniaxial</p><p>$\epsilon_z&lt;\epsilon$ 负单轴 nagative uniaxial</p></blockquote><h3 id="外加直流磁场的plasma-media"><a href="#外加直流磁场的plasma-media" class="headerlink" title="外加直流磁场的plasma media"></a>外加直流磁场的plasma media</h3><p>磁场方向为z0方向时</p><script type="math/tex; mode=display">\bar{\epsilon}=\begin{bmatrix}\epsilon&i\epsilon_g&0\\i\epsilon_g&\epsilon&0\\0&0&\epsilon_z\end{bmatrix}</script><blockquote><p>$wc=qB_0/m$</p><h3 id="gyromagnetic-madium-回转磁性介质"><a href="#gyromagnetic-madium-回转磁性介质" class="headerlink" title="gyromagnetic madium 回转磁性介质"></a>gyromagnetic madium 回转磁性介质</h3><p>media characterized by hermitian permittivity tensor。铁氧体</p></blockquote><script type="math/tex; mode=display">\bar{\epsilon}=\begin{bmatrix}\mu&i\mu_g&0\\i\mu_g&\mu&0\\0&0&\mu_z\end{bmatrix}</script><h2 id="Biisotropic-双各向同性介质"><a href="#Biisotropic-双各向同性介质" class="headerlink" title="Biisotropic 双各向同性介质"></a>Biisotropic 双各向同性介质</h2><p>物质本构关系</p><script type="math/tex; mode=display">\overrightarrow{D}=\epsilon\overrightarrow{E}+\xi\overrightarrow{H}</script><script type="math/tex; mode=display">\overrightarrow{B}=\mu\overrightarrow{H}+\zeta\overrightarrow{E}</script><h4 id="Tellegen-medium"><a href="#Tellegen-medium" class="headerlink" title="Tellegen medium"></a>Tellegen medium</h4><p>非互易介质</p><script type="math/tex; mode=display">\overrightarrow{D}=\epsilon\overrightarrow{E}+\tau\overrightarrow{H}</script><script type="math/tex; mode=display">\overrightarrow{B}=\mu\overrightarrow{H}+\tau\overrightarrow{E}</script><blockquote><p>$\tau^2/\mu\epsilon\approx1$</p></blockquote><h4 id="Chiral-Media"><a href="#Chiral-Media" class="headerlink" title="Chiral Media"></a>Chiral Media</h4><p>手性介质</p><script type="math/tex; mode=display">\overrightarrow{D}=\epsilon\overrightarrow{E}+i\chi\overrightarrow{H}</script><script type="math/tex; mode=display">\overrightarrow{B}=\mu\overrightarrow{H}-i\chi\overrightarrow{E}</script><blockquote><p>DNA $\chi$为手性参数</p></blockquote><h2 id="Bianisotropic-media双各向异性介质"><a href="#Bianisotropic-media双各向异性介质" class="headerlink" title="Bianisotropic media双各向异性介质"></a>Bianisotropic media双各向异性介质</h2><p>物质本构关系</p><script type="math/tex; mode=display">\overrightarrow{D}=\bar\epsilon\overrightarrow{E}+\bar\xi\overrightarrow{H}</script><script type="math/tex; mode=display">\overrightarrow{B}=\bar\mu\overrightarrow{H}+\bar\zeta\overrightarrow{E}</script><blockquote><p>用36个复数或72个实数，2 <em> 9 </em> 4可以概括宇宙中所有介质</p></blockquote><h2 id="Stmmetry-condition-for-lossless-media无损介质"><a href="#Stmmetry-condition-for-lossless-media无损介质" class="headerlink" title="Stmmetry condition for lossless media无损介质"></a>Stmmetry condition for lossless media无损介质</h2><ul><li><p>无损介质的定义lossless</p><script type="math/tex; mode=display"><\nabla\cdot\overrightarrow{S}>=0</script><blockquote><p>散度的物理意义为流出的能量</p></blockquote></li><li><p>passive</p><script type="math/tex; mode=display"><\nabla\cdot\overrightarrow{S}><0</script></li><li>active<script type="math/tex; mode=display"><\nabla\cdot\overrightarrow{S}>>0</script></li></ul><p>其中</p><script type="math/tex; mode=display">\nabla\cdot\overrightarrow{S}=1/2Re[iw(\overrightarrow{H}^*\cdot\overrightarrow{B}-\overrightarrow{E}\cdot\overrightarrow{D})]</script><script type="math/tex; mode=display">    =1/4[iw(\overrightarrow{H}^*\cdot\overrightarrow{B}-\overrightarrow{E}\cdot\overrightarrow{D})-iw(\overrightarrow{H}^*\cdot\overrightarrow{B}-\overrightarrow{E}\cdot\overrightarrow{D})^*]</script><script type="math/tex; mode=display">\#\#Re\{i(a+bi)=1/2((a+bi)-(a-bi))\#\#</script><script type="math/tex; mode=display">    =\frac{iw}{4}\{\overrightarrow{E}^*\cdot(\bar\epsilon-\bar\epsilon^+)\cdot\overrightarrow{E}+\overrightarrow{H}^*\cdot(\bar\mu-\bar\mu^+)\cdot\overrightarrow{H}+\overrightarrow{E}^*\cdot(\bar\xi-\bar\zeta^+)\cdot\overrightarrow{H}+\overrightarrow{H}^*\cdot(\bar\zeta-\bar\xi^+)\cdot\overrightarrow{E}\}</script><h4 id="lossless无损条件"><a href="#lossless无损条件" class="headerlink" title="lossless无损条件"></a>lossless无损条件</h4><script type="math/tex; mode=display">\bar\epsilon=\bar\epsilon^+</script><script type="math/tex; mode=display">\mu=\bar\mu^+</script><script type="math/tex; mode=display">\bar\zeta=\bar\xi^+</script><script type="math/tex; mode=display">\bar\xi=\bar\zeta^+</script><blockquote><p>厄米矩阵：<br>将一矩阵A的行与列互换，并取各矩阵元素的共轭复数，得一新矩阵，称为厄米特共轭，以A+表之，对角线必为实数，如</p><script type="math/tex; mode=display">        \begin{bmatrix}        2&2+i\\        2-i&2        \end{bmatrix}</script></blockquote><p>可推导</p><script type="math/tex; mode=display">\bar\kappa=\bar\kappa^+</script><script type="math/tex; mode=display">\bar\nu=\bar\nu^+</script><script type="math/tex; mode=display">\bar\chi=\bar\gamma^+</script><hr><script type="math/tex; mode=display">\bar P=\bar P^+</script><script type="math/tex; mode=display">\bar Q=\bar Q^+</script><script type="math/tex; mode=display">\bar L=\bar M^+</script><h2 id="Reciprocity-Media互易介质"><a href="#Reciprocity-Media互易介质" class="headerlink" title="Reciprocity Media互易介质"></a>Reciprocity Media互易介质</h2><h4 id="reciprocity-condition"><a href="#reciprocity-condition" class="headerlink" title="reciprocity condition"></a>reciprocity condition</h4><p><strong>互易条件</strong></p><script type="math/tex; mode=display">\bar\epsilon=\bar\epsilon^T</script><script type="math/tex; mode=display">\mu=\bar\mu^T</script><script type="math/tex; mode=display">-\bar\zeta=\bar\xi^T</script><script type="math/tex; mode=display">\bar\kappa=\bar\kappa^T</script><script type="math/tex; mode=display">\bar\nu=\bar\nu^T</script><script type="math/tex; mode=display">\bar\chi=-\bar\gamma^T</script>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手撕麦克斯韦 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Media</title>
      <link href="2020/06/19/Media/"/>
      <url>2020/06/19/Media/</url>
      
        <content type="html"><![CDATA[<h1 id="3-1time-harmonic-field-正弦时变场"><a href="#3-1time-harmonic-field-正弦时变场" class="headerlink" title="3.1time-harmonic field 正弦时变场"></a>3.1time-harmonic field 正弦时变场</h1><h2 id="信号频域分析"><a href="#信号频域分析" class="headerlink" title="信号频域分析"></a>信号频域分析</h2><h3 id="基本公式"><a href="#基本公式" class="headerlink" title="基本公式"></a>基本公式</h3><script type="math/tex; mode=display">\overrightarrow{E}(\overrightarrow{r},t)=Re\{\overrightarrow{E}(\overrightarrow{r})e^{-iwt}\}</script><script type="math/tex; mode=display">\overrightarrow{B}(\overrightarrow{r},t)=Re\{\overrightarrow{B}(\overrightarrow{r})e^{-iwt}\}</script><script type="math/tex; mode=display">\overrightarrow{D}(\overrightarrow{r},t)=Re\{\overrightarrow{D}(\overrightarrow{r})e^{-iwt}\}</script><script type="math/tex; mode=display">\overrightarrow{H}(\overrightarrow{r},t)=Re\{\overrightarrow{H}(\overrightarrow{r})e^{-iwt}\}</script><script type="math/tex; mode=display">\overrightarrow{J}(\overrightarrow{r},t)=Re\{\overrightarrow{J}(\overrightarrow{r})e^{-iwt}\}</script><script type="math/tex; mode=display">\overrightarrow{\rho}(\overrightarrow{r},t)=Re\{\overrightarrow{\rho}(\overrightarrow{r})e^{-iwt}\}</script><blockquote><p>作用，去除麦克斯韦方程中微分，把微分方程化为代数方程</p></blockquote><span id="more"></span><h3 id="频域麦克斯韦"><a href="#频域麦克斯韦" class="headerlink" title="频域麦克斯韦"></a>频域麦克斯韦</h3><h4 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h4><script type="math/tex; mode=display">\nabla\times\overrightarrow{E}(\overrightarrow{r},t)=-\frac{\partial}{\partial t}\overrightarrow{B}(\overrightarrow{r},t)</script><script type="math/tex; mode=display">Re\{[\nabla\times\overrightarrow{E}(\overrightarrow{r})-iw\overrightarrow{B}(\overrightarrow{r})]e^{-iwt}\}=0</script><script type="math/tex; mode=display">\nabla\times\overrightarrow{E}(\overrightarrow{r})-iw\overrightarrow{B}(\overrightarrow{r})=0</script><h4 id="频域麦克斯韦方程组"><a href="#频域麦克斯韦方程组" class="headerlink" title="频域麦克斯韦方程组"></a>频域麦克斯韦方程组</h4><script type="math/tex; mode=display">\begin{cases} \nabla\times\overrightarrow{E}=iw\overrightarrow{B}\\\nabla\times\overrightarrow{H}=-iw\overrightarrow{D}+\overrightarrow{J}\\\nabla\cdot\overrightarrow{B}=0\\\nabla\cdot\overrightarrow{D}=\rho\end{cases}</script><h3 id="频域亥姆霍兹方程"><a href="#频域亥姆霍兹方程" class="headerlink" title="频域亥姆霍兹方程"></a>频域亥姆霍兹方程</h3><script type="math/tex; mode=display">(\nabla^2+w^2\mu\varepsilon)\overrightarrow{E}=0</script><h2 id="极化（Polarization-of-Monochromatic-Waves）"><a href="#极化（Polarization-of-Monochromatic-Waves）" class="headerlink" title="极化（Polarization of Monochromatic Waves）"></a>极化（Polarization of Monochromatic Waves）</h2><p>对于$\overrightarrow{E}(\overrightarrow{r},t)$当r=0，令频域</p><script type="math/tex; mode=display">\overrightarrow{E}=\overrightarrow{E}_R+i\overrightarrow{E}_I</script><p>由$\overrightarrow{E}_R$,$\overrightarrow{E}_I$定义的的平面称为极化平面</p><script type="math/tex; mode=display">\overrightarrow{E}(t)=Re\{(\overrightarrow{E}_R+i\overrightarrow{E}_I)e^{-iwt}\}=\overrightarrow{E}_Rcoswt+\overrightarrow{E}_Isinwt</script><script type="math/tex; mode=display">\frac{\partial\overrightarrow{E}(t)}{\partial t}=w[-\overrightarrow{E}_Rsinwt+i\overrightarrow{E}_Icoswt]</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200619163521.png" alt=""></p><blockquote><p>当$E_R，E_I$平行时,波线性极化，垂直时圆极化或椭圆极化</p></blockquote><hr><h2 id="频域功率计算"><a href="#频域功率计算" class="headerlink" title="频域功率计算"></a>频域功率计算</h2><p>定义<strong>频域波印廷矢量</strong></p><script type="math/tex; mode=display">\overrightarrow{S}=\overrightarrow{E}\times\overrightarrow{H}^*</script><p>定义</p><script type="math/tex; mode=display">\overrightarrow{E}(\overrightarrow{r})=\overrightarrow{E}_R(\overrightarrow{r})+i\overrightarrow{E}_I(\overrightarrow{r})</script><script type="math/tex; mode=display">\overrightarrow{H}(\overrightarrow{r})=\overrightarrow{H}_R(\overrightarrow{r})+i\overrightarrow{H}_I(\overrightarrow{r})</script><p>可得</p><script type="math/tex; mode=display">\overrightarrow{E}(\overrightarrow{r},t)=Re\{\overrightarrow{E}(\overrightarrow{r})e^{-iwt}\}=\overrightarrow{E}_Rcoswt+\overrightarrow{E}_Isinwt</script><p>同理</p><script type="math/tex; mode=display">\overrightarrow{H}(\overrightarrow{r},t)=Re\{\overrightarrow{H}(\overrightarrow{r})e^{-iwt}\}=\overrightarrow{H}_Rcoswt+\overrightarrow{E}_Isinwt</script><p>所以频域波印廷矢量</p><script type="math/tex; mode=display">\overrightarrow{S}=\overrightarrow{E}_R\times\overrightarrow{H}_R+\overrightarrow{E}_I\times\overrightarrow{H}_I+i(\overrightarrow{E}_I\times\overrightarrow{H}_R-\overrightarrow{E}_R\times\overrightarrow{H}_I)</script><p>时域波印廷矢量</p><script type="math/tex; mode=display">\overrightarrow{S}(\overrightarrow{r},t)=\overrightarrow{E}(\overrightarrow{r},t)\times\overrightarrow{H}(\overrightarrow{r},t)</script><p>代入得</p><script type="math/tex; mode=display">\overrightarrow{S}=\overrightarrow{E}_R\times\overrightarrow{H}_Rcos^2wt+\overrightarrow{E}_I\times\overrightarrow{H}_Isin^2wt+(\overrightarrow{E}_I\times\overrightarrow{H}_R+\overrightarrow{E}_R\times\overrightarrow{H}_I)sinwtcoswt</script><p><strong>在2$\pi$的时间内时域波印廷矢量的平均值</strong></p><script type="math/tex; mode=display"><\overrightarrow{S}(\overrightarrow{r},t)>=\frac{1}{2\pi}\int_0^{2\pi}d(wt)\overrightarrow{S}(\overrightarrow{r},t)=\frac{1}{2}[\overrightarrow{E}_R\times\overrightarrow{H}_R+\overrightarrow{E}_I\times\overrightarrow{H}_I]=\frac{1}{2}Re{\overrightarrow{S}(\overrightarrow{r})}</script><hr><h2 id="gt-计算时域能量可以用频域公式去除积分，化简运算"><a href="#gt-计算时域能量可以用频域公式去除积分，化简运算" class="headerlink" title="&gt; 计算时域能量可以用频域公式去除积分，化简运算"></a>&gt; 计算时域能量可以用频域公式去除积分，化简运算</h2><h2 id="Conducting-media介质中的波"><a href="#Conducting-media介质中的波" class="headerlink" title="Conducting media介质中的波"></a>Conducting media介质中的波</h2><p>在导电介质中，根据欧姆定律</p><script type="math/tex; mode=display">\overrightarrow{J}_c=\sigma\overrightarrow{E}</script><blockquote><p>$\overrightarrow{J}_c$为欧姆电流,$\overrightarrow{J}_f$为传导电流</p></blockquote><p>由</p><script type="math/tex; mode=display">\nabla\times\overrightarrow{H}=-iw\overrightarrow{D}+\overrightarrow{J}_c+\overrightarrow{J}_f</script><script type="math/tex; mode=display">\nabla\times\overrightarrow{H}=-iw[\varepsilon+\frac{i}{w}\sigma]\overrightarrow{E}+\overrightarrow{J}_f</script><p>令</p><script type="math/tex; mode=display">\varepsilon_c=\varepsilon+\frac{i}{w}\sigma</script><p>当没有自由电流时得</p><script type="math/tex; mode=display">(\nabla^2+w^2\mu\varepsilon_c)\overrightarrow{E}=0</script><p>取电场只有x方向最简单的解$\overrightarrow{E}=\hat{x}\overrightarrow{E}=\hat{x}\overrightarrow{E}_0e^{ikz}$</p><script type="math/tex; mode=display">k^2=w^2\mu\varepsilon_c=w^2\mu(\varepsilon+\frac{i}{w}\sigma)</script><p>解得</p><script type="math/tex; mode=display">k=w\sqrt{\mu\varepsilon}(\varepsilon+\frac{i}{w}\sigma)^{\frac{1}{2}}=k_R+ik_I</script><p>频域电场强度</p><script type="math/tex; mode=display">\overrightarrow{E}=\hat{x}E_0e^{-k_Iz+ik_Rz}</script><p>转化为时域</p><script type="math/tex; mode=display">\overrightarrow{E}=\hat{x}E_0e^{-k_Iz}cos(k_Rz-wt)</script><blockquote><p>可以看出波沿传导方向指数衰减</p><script type="math/tex; mode=display">k_I=w\sqrt{\mu_0\epsilon}[1/2(\sqrt{1+\frac{\sigma^2}{\epsilon^2w^2}}-1)]^{1/2}</script></blockquote><p>定义穿透深度</p><script type="math/tex; mode=display">d_p=\frac{1}{k_I}</script><blockquote><p>当波衰减为0时刻的1/e</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200619163522.png" alt=""></p><p>根据频率，介质特性分两种情况，系数定义为损耗正切</p><ul><li>$1&lt;&lt;\frac{\sigma}{w\varepsilon}$ 一般取0.1<ul><li>导电能力弱</li><li>穿透深度大<script type="math/tex; mode=display">k=k_R+ik_I=w\sqrt{\mu\varepsilon}(i\frac{\sigma}{w\epsilon})^{\frac{1}{2}}=\sqrt{\frac{w\mu\sigma}{2}}(1+i)</script><script type="math/tex; mode=display">d_p=\sqrt{\frac{2}{w\mu\sigma}}</script></li></ul></li><li>$1&gt;&gt;\frac{\sigma}{w\epsilon}$ 一般取10<ul><li>导电能力强</li><li>穿透深度小<script type="math/tex; mode=display">k=k_R+ik_I=w\sqrt{\mu\varepsilon}(\varepsilon+i\frac{\sigma}{2w\varepsilon})=w\sqrt{\mu\varepsilon}+i\frac{\sigma}{2}(\frac{\mu}{\varepsilon})^{\frac{1}{2}}</script><script type="math/tex; mode=display">d_p=\frac{2}{\sigma}(\frac{\varepsilon}{\mu})^{\frac{1}{2}}</script></li></ul></li></ul><hr><h3 id="Plasme-Media中的波"><a href="#Plasme-Media中的波" class="headerlink" title="Plasme Media中的波"></a>Plasme Media中的波</h3><p>等离子体介质由自由电子和正离子构成，存在极化电场$\overrightarrow{P}$<br>，因为离子体电子速度v远小于光速c</p><script type="math/tex; mode=display">f=q(\overrightarrow{E}+\overrightarrow{v}\times\overrightarrow{B})\approx q\overrightarrow{E}=-mw^2\overrightarrow{r}</script><script type="math/tex; mode=display">\overrightarrow{P}=Nq\overrightarrow{r}=-\frac{Nq^2}{mw^2}\overrightarrow{E}</script><script type="math/tex; mode=display">\nabla\times\overrightarrow{H}=-iw\overrightarrow{D}=-iw(\epsilon\overrightarrow{E}+\overrightarrow{P})=-iw\epsilon(1-\frac{Nq^2}{mw^2})\overrightarrow{E}</script><p>定义</p><script type="math/tex; mode=display">\epsilon_p(w)=\epsilon_0[1-\frac{w_p^2}{w^2}]</script><script type="math/tex; mode=display">w_p=\sqrt{\frac{Nq^2}{m\epsilon_0}}\approx 56.4\sqrt{N}</script><p>得到亥姆霍兹方程</p><script type="math/tex; mode=display">(\nabla^2+w^2\mu\varepsilon_p)\overrightarrow{E}=0</script><script type="math/tex; mode=display">k^2=w^2\mu\varepsilon_p=w^2\mu_0\varepsilon_0(1-\frac{w_p^2}{w^2})</script><p>根据$w_p$的大小，有两种解</p><ul><li>$w&gt;w_p$<script type="math/tex; mode=display">\begin{cases} k=\frac{w}{c}\sqrt{1-\frac{w_p^2}{w^2}}\\\overrightarrow{E}=\hat{x}E_0e^{ikz}\\\overrightarrow{H}=\hat{y}\frac{k}{w\mu}E_0e^{ikz}\\\overrightarrow{S}=\hat{z}\frac{k}{w\mu}|E_0|^2\\<\overrightarrow{S}>=\hat{z}\frac{k}{2w\mu}|E_0|^2\end{cases}</script></li><li>$w&lt;w_p$<script type="math/tex; mode=display">\begin{cases} k=i\frac{w}{c}\sqrt{\frac{w_p^2}{w^2}-1}\\\overrightarrow{E}=\hat{x}E_0e^{-k_Iz}\\\overrightarrow{H}=\hat{y}\frac{ik_I}{w\mu}E_0e^{k_Iz}\\\overrightarrow{S}=\hat{z}\frac{-ik_I}{w\mu}|E_0|^2e^{-k_Iz}\\<\overrightarrow{S}>=0\end{cases}</script><blockquote><p>按指数衰减而不传播时间平均功率的波称为倏逝波。在全反射章节中会再次出现，全反射时的透射波也是倏逝波，不传播能量</p></blockquote></li></ul><h3 id="phase-and-group-velocities相速度和群速度"><a href="#phase-and-group-velocities相速度和群速度" class="headerlink" title="phase and group velocities相速度和群速度"></a>phase and group velocities相速度和群速度</h3><script type="math/tex; mode=display">k(w)=\frac{w}{c}\sqrt{1-\frac{w_p^2}{w^2}}</script><p>当$w&gt;w_p$<br>令$w_1=w_0+\delta w$,$w_2=w_0-\delta w$</p><script type="math/tex; mode=display">\overrightarrow{E}_x(z,t)=cos(k_1z-w_1t)+cos(k_2z-w_2t)=2cos(k_0z-w_0t)cos(\delta kz-\delta wt)</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200619163523.png" alt=""></p><blockquote><p>两个波线性叠加，形成驻波</p></blockquote><p><strong>定义</strong></p><blockquote><p>phase velocity=w/k</p><p>group velocity=$\frac{1}{\partial k/\partial  w}$</p></blockquote><p>则$v_p=\frac{w}{k}=\frac{c}{\sqrt{1-w_p^2/w^2}}&gt;c$</p><p>$v_g=\frac{1}{\partial k/\partial  w}=c\sqrt{1-w_p^2/w^2}&lt;c$</p><p>$v_gv_p=c^2$</p><blockquote><p>相速度可以理解为简谐波在传播的过程中固定相位的点沿着波传播方向上的速度，例如，波峰的传播速度或者波谷的传播速度，也就是波长除以周期。这个概念比较好形象的解释。群速度可以解释为振幅值的变化的传播速度（对于简谐振动，我们可以理解为群速度不存在。因为不存在振幅值的变化，更别提这个变化的传播了。所以群速度都是针对非简谐振动而言的。对于折射系数大于1的介质，相速度大于群速度，对于折射系数小于1的介质，相速度小于群速度（反常介质，不存在）</p></blockquote><hr><h3 id="Dispersive-Media"><a href="#Dispersive-Media" class="headerlink" title="Dispersive Media"></a>Dispersive Media</h3><script type="math/tex; mode=display">q\overrightarrow{E}=m(\frac{d^2\overrightarrow{r}}{dt^2}+\gamma\frac{d\overrightarrow{r}}{dt}+w_0^2\overrightarrow{r})</script><script type="math/tex; mode=display">\overrightarrow{P}=Nq\overrightarrow{r}=-\frac{Nq^2}{m(w^2-w_0^2+iw\gamma)}\overrightarrow{E}</script><p>亥姆霍兹方程</p><script type="math/tex; mode=display">(\nabla^2+w^2\mu\varepsilon_d)\overrightarrow{E}=0</script><script type="math/tex; mode=display">\epsilon_d(w)=\epsilon_0[1-\frac{w_p^2}{w^2-w_0^2+iw\gamma}]</script><script type="math/tex; mode=display">w_p=\sqrt{\frac{Nq^2}{m\epsilon_0}}\approx 56.4\sqrt{N}</script><p>化简得</p><script type="math/tex; mode=display">\epsilon_d(w)=\epsilon_R+i\epsilon_I=\epsilon_0[1-\frac{(w^2-w_0^2)w_p^2}{(w^2-w_0^2)^2+(w\gamma)^2}]+i\frac{w\gamma w_p^2}{(w^2-w_0^2)^2+(w\gamma)^2}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200619163525.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手撕麦克斯韦 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从0开始手撕麦克斯韦</title>
      <link href="2020/06/19/maxwell/"/>
      <url>2020/06/19/maxwell/</url>
      
        <content type="html"><![CDATA[<h2 id="麦克斯韦——信电杀神，磁爆之王，掌控雷电的男人。为了能对抗邪恶的麦克斯韦，小编今天就带大家从0开始徒手撕麦克斯韦"><a href="#麦克斯韦——信电杀神，磁爆之王，掌控雷电的男人。为了能对抗邪恶的麦克斯韦，小编今天就带大家从0开始徒手撕麦克斯韦" class="headerlink" title="麦克斯韦——信电杀神，磁爆之王，掌控雷电的男人。为了能对抗邪恶的麦克斯韦，小编今天就带大家从0开始徒手撕麦克斯韦"></a><strong>麦克斯韦——信电杀神，磁爆之王，掌控雷电的男人。为了能对抗邪恶的麦克斯韦，小编今天就带大家从0开始徒手撕麦克斯韦</strong></h2><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200619161213.jpeg" alt=""></p><p><strong>开个玩笑，麦克斯韦还是一位非常伟大的物理学家，场波学的是MIT英文版的教材，据说上的内容是MIT研究生才学的，属实难顶，国内关于场波的教材和视频并没有找到特别满意的，对传输线和天线等应用层面关注的更多，对数学物理的原理内容较少，而MIT的教材更注重于理论，所以啃得十分吃力，以下为自己整理的笔记，希望对想获得雷电力量的年轻人有所帮助</strong></p><h3 id="笔记目录"><a href="#笔记目录" class="headerlink" title="笔记目录"></a>笔记目录</h3><p><a href="https://Fazziekey.github.io/2020/06/18/Maxwell-s-Theory/">Maxwell fundamental theory</a></p><p><a href="https://Fazziekey.github.io/2020/06/19/Media/">frequence analyse</a></p><p><a href="https://Fazziekey.github.io/2020/06/19/Bianisotropic-media/">Biantropic-Media</a></p><p><a href="https://Fazziekey.github.io/2020/06/19/KDB/">KDB coordinate system</a></p><p><a href="https://Fazziekey.github.io/2020/06/21/Reflection-and-Transmission/">Reflection and Transmission</a></p><p><a href="https://Fazziekey.github.io/2020/06/21/Wave-Guide/">Wave Guidance&amp;Resonance</a></p><p><a href="https://Fazziekey.github.io/2020/06/21/Radiation/">Radiation</a></p>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手撕麦克斯韦 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SIFT</title>
      <link href="2020/06/19/SIFT/"/>
      <url>2020/06/19/SIFT/</url>
      
        <content type="html"><![CDATA[<h1 id="图像检索原理概述"><a href="#图像检索原理概述" class="headerlink" title="图像检索原理概述"></a>图像检索原理概述</h1><p>首先，我们要明白，图像检索，简单的说，便是从图片检索数据库中检索出满足条件的图片，图像检索技术的研究根据描述图像内容方式的不同可以分为两类：</p><p>一类是基于文本的图像检索技术，简称TBIR，</p><p>一类为基于内容的图像检索技术，简称CBIR。<br><span id="more"></span></p><h2 id="两类图像检索技术"><a href="#两类图像检索技术" class="headerlink" title="两类图像检索技术"></a>两类图像检索技术</h2><p>基于文本的图像检索(TBIR)技术，其主要原理为利用文本描述，如文本描述图片的内容、作者等等的方式来检索图片；<br>基于图像的内容语义的图像检索技术（CBIR），利用图片的颜色、纹理及图片包含的物体、类别等信息检索图片，如给定检索目标图片，在图像检索数据库中检索出与它相似的图片。</p><p>基于图像的内容语义的图像检索包括相同物体图像检索和相同类别图像检索，检索任务分别为检索同一个物体地不同图片和检索同一个类别地图片。例如，行人检索中检索的是同一个人即同一个身份在不同场景不同摄像头下拍得的图片属于相同物体的图像检索，而在3D形状检索中则是检索属于同一类的物品，如飞机等。<br> 图像检索技术的步骤<br>图像检索技术主要包含几个步骤，分别为：输入图片、特征提取、度量学习、重排序。<br>特征提取：即将图片数据进行降维，提取数据的判别性信息，一般将一张图片降维为一个向量；<br>度量学习：一般利用度量函数，计算图片特征之间的距离，作为loss，训练特征提取网络，使得相似图片提取的特征相似，不同类的图片提取的特征差异性较大。<br>重排序：利用数据间的流形关系，对度量结果进行重新排序，从而得到更好的检索结果。</p><h1 id="基于SFIT的图像特征提取"><a href="#基于SFIT的图像特征提取" class="headerlink" title="基于SFIT的图像特征提取"></a>基于SFIT的图像特征提取</h1><p>尺度不变特征转换即SIFT (Scale-invariant feature transform)是一种计算机视觉的算法。它用来侦测与描述影像中的局部性特征，它在空间尺度中寻找极值点，并提取出其位置、尺度、旋转不变量，此算法由 David Lowe在1999年所发表，2004年完善总结。</p><p>其应用范围包含物体辨识、机器人地图感知与导航、影像缝合、3D模型建立、手势辨识、影像追踪和动作比对。</p><p>局部影像特征的描述与侦测可以帮助辨识物体，SIFT特征是基于物体上的一些局部外观的兴趣点而与影像的大小和旋转无关。对于光线、噪声、些微视角改变的容忍度也相当高。基于这些特性，它们是高度显著而且相对容易撷取，在母数庞大的特征数据库中，很容易辨识物体而且鲜有误认。使用 SIFT特征描述对于部分物体遮蔽的侦测率也相当高，甚至只需要3个以上的SIFT物体特征就足以计算出位置与方位。在现今的电脑硬件速度下和小型的特征数据库条件下，辨识速度可接近即时运算。SIFT特征的信息量大，适合在海量数据库中快速准确匹配。</p><p>SIFT算法的实质是在不同的尺度空间上查找关键点(特征点)，并计算出关键点的方向。SIFT所查找到的关键点是一些十分突出，不会因光照，仿射变换和噪音等因素而变化的点，如角点、边缘点、暗区的亮点及亮区的暗点等。</p><h1 id="opencv中的SIFT算法"><a href="#opencv中的SIFT算法" class="headerlink" title="opencv中的SIFT算法"></a>opencv中的SIFT算法</h1><p>下面为一个简单算法样例</p><pre><code>import cv2  import numpy as np  img = cv2.imread(&#39;home.jpg&#39;)  gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)  sift = cv2.SIFT()  kp = sift.detect(gray,None)  img=cv2.drawKeypoints(gray,kp)  cv2.imwrite(&#39;sift_keypoints.jpg&#39;,img)  </code></pre><p>函数 sift.detect() 可以在图像中找到关键点。如果只想在图像中的一个区域搜索的话，也可以创建一个掩模图像作为参数使用。返回的S关键点是一个<br>带有很多不同属性的特殊结构体，这些属性中包含它的坐标（x，y），有意义的<br>邻域大小，确定其方向的角度等。OpenCV 也提供了绘制关键点的函数：cv2.drawKeyPoints()，它可以在关键点的部位绘制一个小圆圈。如果设置参数为 cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS，就会绘制代表关键点大小的圆圈甚至可以绘制除关键点的方向。<br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200619084500.png" alt=""><br>关于计算关键点描述符，OpenCV 提供了两种方法。</p><ol><li>由于已经找到了关键点，可以使用函数 <code>sift.compute()</code> 来计<br>算这些关键点的描述符。例如：<code>kp, des = sif t.compute(gray, kp)</code>。</li><li>如果还没有找到关键点，可以使用函数 <code>sift.detectAndCompute()</code><br>一步到位直接找到关键点并计算出其描述符。<br>这里 kp 返回的是一个关键点列表。<code>des</code> 是一个 <code>Numpy</code> 数组，其大小是关键点数目乘以 <strong>128</strong>。</li></ol><h1 id="代码框架"><a href="#代码框架" class="headerlink" title="代码框架"></a>代码框架</h1><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>这里我选用了选用了101_ObjectCategories的图片作为本次模型训练数据。数据集下载地址：<a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/。">http://www.vision.caltech.edu/Image_Datasets/Caltech101/。</a> </p><h3 id="代码运行环境需要"><a href="#代码运行环境需要" class="headerlink" title="代码运行环境需要"></a>代码运行环境需要</h3><ul><li>python 3.7 (Anaconda python) </li><li>Numpy 1.16.2 </li><li>Matplotlib 3.1.1 </li><li>opencv-contrib-python 3.4.2.16 </li></ul><h3 id="运行方法"><a href="#运行方法" class="headerlink" title="运行方法"></a>运行方法</h3><p>把run模块中的<code>image_path</code>更改为对应的文件路径运行即可</p><h1 id="算法流程图"><a href="#算法流程图" class="headerlink" title="算法流程图"></a>算法流程图</h1><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200619084433.png" alt=""></p><h1 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h1><pre><code>    # Developer：Fazzie      # Time: 2020/6/1420:25      # File name: SIFT.py      # Development environment: Anaconda Python      import cv2      import numpy as np      import scipy      import _pickle as pickle      import random      import os      from matplotlib import pyplot as plt      import scipy.spatial      # 特征提取模块      def feature_extract(image_path,vector_size=32):          img = cv2.imread(image_path,1)#读取图片          # 显示图片          # cv2.imshow(&#39;image&#39;,img)          # cv2.waitKey(0)          # cv2.destroyAllWindows()          # SIFT特征提取          gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)          sift = cv2.xfeatures2d.SIFT_create()       kp = sift.detect(gray, None)        # 画出特征点并保存          img = cv2.drawKeypoints(gray, kp, img)          cv2.imwrite(&#39;sift_keypoints.jpg&#39;, img)          # 根据关键点的返回值进行排序（越大越好）          kp = sorted(kp, key=lambda x: -x.response)[:vector_size]          # 计算描述符向量          kp, des = sift.compute(gray, kp)          # 将其放在一个大的向量中，作为我们的特征向量          des = des.flatten()          # 使描述符的大小一致          # 描述符向量的大小为128          needed_size = (vector_size * 128)          if des.size &lt; needed_size:              # 如果少于32个描述符，则在特征向量后面补零              des = np.concatenate([des, np.zeros(needed_size - des.size)])          return des      #   数据存储      def batch_extractor(images_path, pickled_db_path=&quot;features.pck&quot;):          files = [os.path.join(images_path, p) for p in sorted(os.listdir(images_path))]          result = &#123;&#125;          for f in files:              print(&#39;Extracting features from image %s&#39; % f)              name = f.split(&#39;/&#39;)[-1].lower()              result[name] = feature_extract(f)          # 将特征向量存于pickled 文件          with open(pickled_db_path, &#39;wb&#39;) as fp:              pickle.dump(result, fp)      # 图像特征匹配模块      class Matcher(object):          def __init__(self, pickled_db_path=&quot;features.pck&quot;):              with open(pickled_db_path,&quot;rb&quot;) as fp:                  self.data = pickle.load(fp)              self.names = []              self.matrix = []              for k, v in self.data.items():                  self.names.append(k)                  self.matrix.append(v)              self.matrix = np.array(self.matrix)              self.names = np.array(self.names)          def cos_cdist(self, vector):              # 计算待搜索图像与数据库图像的余弦距离              v = vector.reshape(1, -1)              return scipy.spatial.distance.cdist(self.matrix, v, &#39;cosine&#39;).reshape(-1)          def match(self, image_path, topn=5):              features = feature_extract(image_path)              img_distances = self.cos_cdist(features)              # 获得前5个记录              nearest_ids = np.argsort(img_distances)[:topn].tolist()              nearest_img_paths = self.names[nearest_ids].tolist()              return nearest_img_paths, img_distances[nearest_ids].tolist()      def show_img(path):          img = cv2.imread(path,1)          plt.imshow(img)          plt.show()      # 主程序      def run():          images_path = &#39;E:/Code/SFIT/101_ObjectCategories/panda/&#39;          files = [os.path.join(images_path, p) for p in sorted(os.listdir(images_path))]          # 随机获取1张图          sample = random.sample(files, 1)          batch_extractor(images_path)          ma = Matcher(&#39;features.pck&#39;)          for s in sample:              print(&#39;Query image ==========================================&#39;)              show_img(s)              names, match = ma.match(s, topn=3)              print(&#39;Result images ========================================&#39;)              for i in range(3):                  # 我们得到了余弦距离，向量之间的余弦距离越小表示它们越相似，因此我们从1中减去它以得到匹配值                  print(&#39;Match %s&#39; % (1 - match[i]))                  show_img(os.path.join(images_path, names[i]))      run()  </code></pre><h1 id="可能遇见的问题和解决方案"><a href="#可能遇见的问题和解决方案" class="headerlink" title="可能遇见的问题和解决方案"></a>可能遇见的问题和解决方案</h1><ul><li><code>opencv</code>库<code>SFIT</code>函数不可用（<code>Error:cv2 has no attribute SIFT</code>）</li></ul><p><strong>原因</strong>：在<code>opencv3.4</code>之后的版本<code>SIFT</code>，<code>SURF</code>等图像特征提取算法申请了专利，不包含在<code>opencv</code>的基本库里</p><p><strong>解决方案</strong>：安装额外模块<code>opencv contrib</code>的，下载对应<code>whl</code>文件，在<code>Anaconda</code>对应目录的第三方包<code>site-packages</code>输入命令行：<code>pip install</code> 对应whl文件名</p><ul><li><code>cpickle</code>模块不可用<br>在python3中<code>cpikle</code>模块集成到了<code>_pickle</code>,<code>import _pickle</code>即可</li><li>可以考虑使用SURF算法代替SIFT，进一步提高运算速率</li></ul><blockquote><p>如果想下载源码，可以访问<a href="https://github.com/Fazziekey/opencv_SIFT">https://github.com/Fazziekey/opencv_SIFT</a></p><p>参考文章:<a href="https://towardsdatascience.com/feature-extraction-and-similar-image-search-with-opencv-for-newbies-3c59796bf774">https://towardsdatascience.com/feature-extraction-and-similar-image-search-with-opencv-for-newbies-3c59796bf774</a></p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Maxwell&#39;s Theory</title>
      <link href="2020/06/18/Maxwell-s-Theory/"/>
      <url>2020/06/18/Maxwell-s-Theory/</url>
      
        <content type="html"><![CDATA[<h1 id="Maxwell’s-Theory"><a href="#Maxwell’s-Theory" class="headerlink" title="Maxwell’s Theory"></a>Maxwell’s Theory</h1><h2 id="maxwell’s-Equations"><a href="#maxwell’s-Equations" class="headerlink" title="maxwell’s Equations"></a>maxwell’s Equations</h2><h3 id="微分形式麦克斯韦方程组"><a href="#微分形式麦克斯韦方程组" class="headerlink" title="微分形式麦克斯韦方程组"></a>微分形式麦克斯韦方程组</h3><script type="math/tex; mode=display">\begin{cases} \nabla\times\overrightarrow{E}=\frac{\partial}{\partial t}\overrightarrow{B},\\\nabla\times\overrightarrow{H}=\frac{\partial}{\partial t}\overrightarrow{D}+\overrightarrow{J},\\\nabla\cdot\overrightarrow{B}=0,\\\nabla\cdot\overrightarrow{D}=\rho\end{cases}</script><blockquote><ul><li>1为法拉第定律或法拉第磁感应定律。</li><li>2为安培定律或广义安培电路定律式</li><li>3为电场的库仑定律或高斯定律。</li><li>4为高斯定律或磁场的高斯定律。</li></ul></blockquote><span id="more"></span><h3 id="物质本构方程"><a href="#物质本构方程" class="headerlink" title="物质本构方程"></a>物质本构方程</h3><script type="math/tex; mode=display">\bar D=\epsilon_0\bar E</script><script type="math/tex; mode=display">\bar B=\mu_0 \bar H</script><p>其中 </p><script type="math/tex; mode=display">\epsilon_0=8.85\times10^{-12} F/m</script><script type="math/tex; mode=display">\mu_0=4\pi\times10^{-7} H/m</script><h2 id="Vector-Analysis"><a href="#Vector-Analysis" class="headerlink" title="Vector Analysis"></a>Vector Analysis</h2><h3 id="向量运算重要公式"><a href="#向量运算重要公式" class="headerlink" title="向量运算重要公式"></a>向量运算重要公式</h3><script type="math/tex; mode=display">\bar C\cdot (\bar A\times \bar B)=\bar A\cdot (\bar B\times \bar C)=\bar B\cdot (\bar C\times \bar A)</script><script type="math/tex; mode=display">\bar C\times (\bar A\times \bar B)=\bar A(\bar C\cdot \bar B)-(\bar C\cdot \bar A)\bar B</script><h3 id="nabla-del-Operatoe-del-算符"><a href="#nabla-del-Operatoe-del-算符" class="headerlink" title="$\nabla$:del Operatoe(del 算符)"></a>$\nabla$:del Operatoe(del 算符)</h3><p><strong>定义</strong></p><script type="math/tex; mode=display">\nabla=\hat{x}\frac{\partial}{\partial x}+\hat{y}\frac{\partial}{\partial y}+\hat{z}\frac{\partial}{\partial z}</script><p><strong>重要公式</strong></p><script type="math/tex; mode=display">\nabla \cdot(\bar E\times \bar H)=\bar H \cdot(\nabla \times \bar E)-\bar E \cdot(\nabla \times \bar H)</script><script type="math/tex; mode=display">\nabla\cdot(\nabla \times \bar A)=0</script><script type="math/tex; mode=display">\nabla \times(\nabla \phi)=0</script><script type="math/tex; mode=display">\nabla \times(\nabla\times\bar E)=\nabla(\nabla\cdot\bar E)-\nabla^2\bar E</script><h4 id="Laplaceian-Operator（拉普拉斯算符）"><a href="#Laplaceian-Operator（拉普拉斯算符）" class="headerlink" title="Laplaceian Operator（拉普拉斯算符）"></a>Laplaceian Operator（拉普拉斯算符）</h4><script type="math/tex; mode=display">\nabla^2=\nabla\cdot\nabla=\frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}+\frac{\partial^2}{\partial z^2}</script><h3 id="Gradient-梯度"><a href="#Gradient-梯度" class="headerlink" title="Gradient 梯度"></a>Gradient 梯度</h3><script type="math/tex; mode=display">\nabla\phi=\hat{x}\frac{\partial}{\partial x}\phi+\hat{y}\frac{\partial}{\partial y}\phi+\hat{z}\frac{\partial}{\partial z}\phi</script><h3 id="Divergence-散度"><a href="#Divergence-散度" class="headerlink" title="Divergence 散度"></a>Divergence 散度</h3><script type="math/tex; mode=display">\nabla\cdot\bar D=\frac{\partial}{\partial x}D_x+\frac{\partial}{\partial y}D_y+\frac{\partial}{\partial z}D_z</script><blockquote><p>通量是单位时间通过某个曲面的量，散度是通量的强度和流量</p></blockquote><h3 id="Curl-旋度"><a href="#Curl-旋度" class="headerlink" title="Curl 旋度"></a>Curl 旋度</h3><script type="math/tex; mode=display">\nabla\times\bar D=\hat{x}(\frac{\partial}{\partial y}H_z-\frac{\partial}{\partial z}H_y)+\hat{y}(\frac{\partial}{\partial z}H_x-\frac{\partial}{\partial x}H_z)+\hat{z}(\frac{\partial}{\partial x}H_y-\frac{\partial}{\partial y}H_x)</script><h1 id="1-2电磁波"><a href="#1-2电磁波" class="headerlink" title="1.2电磁波"></a>1.2电磁波</h1><h2 id="Wave-Equation-and-Wave-Solution-波动方程和波动解"><a href="#Wave-Equation-and-Wave-Solution-波动方程和波动解" class="headerlink" title="Wave Equation and Wave Solution(波动方程和波动解)"></a>Wave Equation and Wave Solution(波动方程和波动解)</h2><p>微分形式的麦克斯韦方程对空间中的每一点都是有效的。为了解出这个方程，我们将从研究在无源区域的麦克斯韦方程组的解开始。</p><script type="math/tex; mode=display">\begin{cases} \nabla\times\overrightarrow{E}=-\mu_0\frac{\partial}{\partial t}\overrightarrow{H},\\\nabla\times\overrightarrow{H}=\epsilon_0\frac{\partial}{\partial t}\overrightarrow{H},\\\nabla\cdot\overrightarrow{E}=0,\\\nabla\cdot\overrightarrow{H}=0\end{cases}</script><p>两边同取旋度</p><script type="math/tex; mode=display">\nabla\times\nabla\times\bar E=-\mu_0\epsilon_0\frac{\partial^2}{\partial t^2}\bar E</script><p><strong>亥姆霍兹方程</strong></p><script type="math/tex; mode=display">\nabla^2\bar E-\mu_0\epsilon_0\frac{\partial^2}{\partial t^2}\bar E=0</script><p>满足该方程的解为波动解</p><hr><p>对于该偏微分方程，最简单的解为电场方向在x方向，传播方向只延z方向的波</p><script type="math/tex; mode=display">\bar E=\hat{x}E_x(z,t)</script><script type="math/tex; mode=display">\bar E=\hat{x}E_0cos(kz-wt)</script><p>其中</p><script type="math/tex; mode=display">k^2=w^2\mu_0\epsilon_0</script><p>该关系称为<strong>色散关系</strong></p><blockquote><p><strong>dispersion relation</strong>:色散关系</p><p>色散关系提供了空间频率k和时间频率w之间的重要联系</p><p>在研究时空变化量，如E(z,t)时，有两种观点。时间的观点是研究不同时间在空间中固定点上的变化。空间视角是研究固定时间的空间变化。</p></blockquote><p><strong>重要关系</strong></p><script type="math/tex; mode=display">f=\frac{w}{2\pi}=\frac{k}{\sqrt{\mu_0\epsilon_0}}=kc</script><script type="math/tex; mode=display">c=\frac{w}{k}</script><p>对应磁场解</p><script type="math/tex; mode=display">\bar H=\hat{y}E_0\frac{1}{\eta_0}cos(kz-wt)</script><p>其中<br>$\frac{1}{\eta_0}=\sqrt{\frac{\epsilon_0}{\mu_0}}$,$\eta$为自由空间阻抗</p><h2 id="Unit-for-Spatial-Frequence-k"><a href="#Unit-for-Spatial-Frequence-k" class="headerlink" title="Unit for Spatial Frequence k"></a>Unit for Spatial Frequence k</h2><script type="math/tex; mode=display">k=\frac{2\pi}{\lambda}</script><p>k的单位定义为$K_0$</p><script type="math/tex; mode=display">K_0=2\pi rad/m</script><h4 id="电磁波谱"><a href="#电磁波谱" class="headerlink" title="电磁波谱"></a>电磁波谱</h4><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200618231538.png" alt=""></p><h3 id="相速度"><a href="#相速度" class="headerlink" title="相速度"></a>相速度</h3><script type="math/tex; mode=display">v_p=\frac{dz}{dt}=\frac{w}{k}=\frac{1}{\sqrt{\mu_o\epsilon_0}}</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200618231619.png" alt=""></p><blockquote><p>相速度可以理解为图中A点移动的速度，相位的传播，在真空中等于光速</p></blockquote><p><strong>相位延迟</strong></p><script type="math/tex; mode=display">A_p=\frac{k}{w}=\sqrt{\mu_o\epsilon_0}</script><h2 id="Polarization极化"><a href="#Polarization极化" class="headerlink" title="Polarization极化"></a>Polarization极化</h2><p>对于波动方程解</p><script type="math/tex; mode=display">\bar E(z,t)=\hat{x}E_x+\hat{y}E_y</script><script type="math/tex; mode=display">=\hat{x}cos(kz-wt)+\hat{y}Acos(kz-wt+\phi)</script><p>从时间域看</p><ul><li>线性极化</li></ul><p>$\phi=2m\pi$,$\phi=(2m+1)\pi$</p><script type="math/tex; mode=display">\bar E(z,t)=\hat{x}cos(wt)\pm\hat{y}Acos(wt)</script><ul><li>圆极化<br>$\phi=\pi/2,A=1$<script type="math/tex; mode=display">\bar E(z,t)=\hat{x}cos(wt)+\hat{y}sin(wt)</script></li><li>椭圆极化<br>$\phi=\pi/2$<script type="math/tex; mode=display">\bar E(z,t)=\hat{x}cos(wt)+\hat{y}Asin(wt)</script><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200618231620.png" alt=""><blockquote><p>$\phi=+\pi/2$右旋，<br>$\phi=-\pi/2$左旋</p></blockquote></li></ul><blockquote><p>$0&lt;\phi&lt;\pi$右旋，<br>$\pi&lt;\phi=2\pi$左旋</p></blockquote><p><strong>空间角度看极化</strong><br><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200618231621.png" alt=""></p><h1 id="Herztian-Waves-赫兹天线"><a href="#Herztian-Waves-赫兹天线" class="headerlink" title="Herztian Waves 赫兹天线"></a>Herztian Waves 赫兹天线</h1><p>一个赫兹偶极子是由两个相对的电荷($\pm q$)组成，它们之间隔着无穷小的距离。偶极矩p= ql有一个角频率w，使得每个点电荷在2$\pi$/w的周期内从+q变化到-q。p被定义为$l→0与q→无穷$的乘积，使p为常数。假设两个电荷位于z=±l/2处。赫兹解出了所有的电磁场用势函数称为赫兹势$\Pi$</p><script type="math/tex; mode=display">\Pi=\frac{ql}{4\pi r}cos(kr-wt)</script><p><img src="https://cdn.jsdelivr.net/gh/Fazziekey/image-bed/img/20200618231622.png" alt=""></p><ul><li>$kr\gg1$<script type="math/tex; mode=display">\bar E=-\hat{\theta}\frac{k^2ql}{4\pi\epsilon_0r}sin\theta cos(kr-wt)</script><script type="math/tex; mode=display">\bar H=-\hat{\phi}\frac{wkql}{4\pi\epsilon_0r}sin\theta cos(kr-wt)</script></li><li>$w=0$<script type="math/tex; mode=display">\bar E=\frac{ql}{4\pi\epsilon_0r^3}(\hat{\theta}sin\theta+\hat{r}2cos\theta)</script><script type="math/tex; mode=display">\bar H=0</script></li><li>在偶极子附近<script type="math/tex; mode=display">\bar H=\hat{\phi}\frac{wql}{4\pi r^2}sin\theta</script></li></ul><h1 id="Constitutive-Relations-物质本构关系"><a href="#Constitutive-Relations-物质本构关系" class="headerlink" title="Constitutive Relations 物质本构关系"></a>Constitutive Relations 物质本构关系</h1><h3 id="Isotropic-Media各向同性介质"><a href="#Isotropic-Media各向同性介质" class="headerlink" title="Isotropic Media各向同性介质"></a>Isotropic Media各向同性介质</h3><script type="math/tex; mode=display">\bar D=\epsilon \bar E</script><script type="math/tex; mode=display">\bar B=\mu\bar H</script><h3 id="Anisotropic-Media各向异性介质"><a href="#Anisotropic-Media各向异性介质" class="headerlink" title="Anisotropic Media各向异性介质"></a>Anisotropic Media各向异性介质</h3><script type="math/tex; mode=display">\bar D=\bar \epsilon \bar E</script><script type="math/tex; mode=display">\bar B=\bar \mu\bar H</script><h3 id="Bianisotropic-Media双各向异性介质"><a href="#Bianisotropic-Media双各向异性介质" class="headerlink" title="Bianisotropic Media双各向异性介质"></a>Bianisotropic Media双各向异性介质</h3><script type="math/tex; mode=display">\bar D=\bar \epsilon \bar E+\bar \xi\bar H</script><script type="math/tex; mode=display">\bar B=\bar\zeta\bar E+ \bar \mu\bar H</script><h3 id="Biisotropic-Media手性介质"><a href="#Biisotropic-Media手性介质" class="headerlink" title="Biisotropic Media手性介质"></a>Biisotropic Media手性介质</h3><script type="math/tex; mode=display">\bar D= \epsilon \bar E+\tau\bar H</script><script type="math/tex; mode=display">\bar B=\tau\bar E+ \mu\bar H</script><h1 id="Boundary-Condition边界条件"><a href="#Boundary-Condition边界条件" class="headerlink" title="Boundary Condition边界条件"></a>Boundary Condition边界条件</h1><script type="math/tex; mode=display">\hat{n}\cdot(\bar{E_1}-\bar{E_2})=0</script><script type="math/tex; mode=display">\hat{n}\cdot(\bar{H_1}-\bar{H_2})=\bar J_s</script><script type="math/tex; mode=display">\hat{n}\cdot(\bar{B_1}-\bar{B_2})=0</script><script type="math/tex; mode=display">\hat{n}\cdot(\bar{D_1}-\bar{D_2})=\rho_s</script>]]></content>
      
      
      <categories>
          
          <category> 《行于黑暗，侍奉光明，我们是电工》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手撕麦克斯韦 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用pytorch进行梯度运算</title>
      <link href="2020/04/22/grad/"/>
      <url>2020/04/22/grad/</url>
      
        <content type="html"><![CDATA[<p>torch.Tensor是程序包的中心类。如果将其属性设置 .requires_grad为True，它将开始跟踪对其的所有操作。完成计算后，可以调用.backward()并自动计算所有梯度。该张量的梯度将累加到.grad属性中。</p><h3 id="创建一个张量并设置requires-grad-True为跟踪张量"><a href="#创建一个张量并设置requires-grad-True为跟踪张量" class="headerlink" title="创建一个张量并设置requires_grad=True为跟踪张量"></a>创建一个张量并设置requires_grad=True为跟踪张量</h3><pre><code>import torchx = torch.ones(2, 2, requires_grad=True)print(x)</code></pre><span id="more"></span>    <h3 id="运算"><a href="#运算" class="headerlink" title="运算"></a>运算</h3><pre><code>y = x + 2print(y)print(y.grad_fn)</code></pre><p>输出</p><pre><code>tensor([[3., 3.],[3., 3.]], grad_fn=&lt;AddBackward0&gt;)</code></pre><p>y是运算产生，存在梯度</p><pre><code>z = y * y * 3out = z.mean()print(z, out)</code></pre><p>结果</p><pre><code>tensor([[27., 27.],[27., 27.]], grad_fn=&lt;MulBackward0&gt;)tensor(27., grad_fn=&lt;MeanBackward0&gt;)</code></pre><p>.requires<em>grad</em>( … )requires_grad 可以更改现有Tensor的标志。如果未给出输入，则默认为False。</p><pre><code>a = torch.randn(2, 2)a = ((a * 3) / (a - 1))print(a.requires_grad)a.requires_grad_(True)print(a.requires_grad)b = (a * a).sum()print(b.grad_fn)</code></pre><p>输出</p><pre><code>FalseTrue&lt;SumBackward0 object at 0x7fc4a70d31d0&gt;</code></pre><h2 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h2><p>使用反向传播</p><pre><code>out.backward()</code></pre><p>$out=\frac{1}{4}\sum<em>{i=1}^4 z_i,z=3(x+2)^2,\frac{\partial out}{\partial x}=\frac{3}{2}(x+2)=4.5|</em>{x=1},$<br>数学上，如果具有向量值函数 $\overrightarrow{y} =f(\overrightarrow{x})$，然后是 $\overrightarrow{y}$关于$\overrightarrow{x}$是雅可比矩阵：</p><script type="math/tex; mode=display">J=\begin{pmatrix}  \frac{\partial y_1}{\partial x_1} & ...& \frac{\partial y_1}{\partial x_n} \ \ ...& &...\ \    \frac{\partial y_1}{\partial x_n} & ... & \frac{\partial y_n}{\partial x_n}\end{pmatrix}</script><p>一般来说，torch.autograd是用于计算向量雅可比积的引擎。给定向量$v=(v_1,v_2…v_n)^T$，计算$J*v^T$,如果v 恰好是标量函数的梯度$l=g(\overrightarrow{y})$,则$v=( \frac{\partial l}{\partial y_1}… \frac{\partial l}{\partial y_n})^T$</p><script type="math/tex; mode=display">J^T\cdot v=\begin{pmatrix}  \frac{\partial l}{\partial x_1} \ \ ...\ \     \frac{\partial l}{\partial x_n}\end{pmatrix}</script><pre><code>x = torch.randn(3, requires_grad=True)y = x * 2while y.data.norm() &lt; 1000:y = y * 2print(y)</code></pre><p>输出</p><pre><code>tensor([1030.3691,  653.3950, -535.1644], grad_fn=&lt;MulBackward0&gt;)</code></pre><p>输入</p><pre><code>v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)y.backward(v)print(x.grad)</code></pre><p>输出</p><pre><code>tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])</code></pre><p><code>.requires_grad=True</code>可以通过将代码块包装在 <code>with torch.no_grad():</code></p><pre><code>print(x.requires_grad)print((x ** 2).requires_grad)with torch.no_grad():print((x ** 2).requires_grad)</code></pre><p>输出</p><pre><code>TrueTrueFalse</code></pre>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>“pytorch矩阵基本操作”</title>
      <link href="2020/04/21/%E2%80%9Cpytorch%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E2%80%9D/"/>
      <url>2020/04/21/%E2%80%9Cpytorch%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E2%80%9D/</url>
      
        <content type="html"><![CDATA[<h2 id="导入-pytorch库"><a href="#导入-pytorch库" class="headerlink" title="导入 pytorch库"></a>导入 pytorch库</h2><pre><code>from __future__ import print_functionimport torch</code></pre><p>需要配置好环境</p><h2 id="创建矩阵"><a href="#创建矩阵" class="headerlink" title="创建矩阵"></a>创建矩阵</h2><h3 id="构造5-3未初始化的矩阵"><a href="#构造5-3未初始化的矩阵" class="headerlink" title="构造5*3未初始化的矩阵"></a>构造5*3未初始化的矩阵</h3><pre><code>x = torch.empty(5, 3)print(x)</code></pre><p>输出<br>        tensor([[2.1030e-18, 4.5559e-41, 4.0441e+10],<br>        [3.0767e-41, 0.0000e+00, 1.4013e-45],<br>        [0.0000e+00, 0.0000e+00, 0.0000e+00],<br>        [0.0000e+00, 0.0000e+00, 0.0000e+00],<br>        [0.0000e+00, 0.0000e+00, 0.0000e+00]])<br><span id="more"></span></p><h3 id="构建一个随机初始化矩阵"><a href="#构建一个随机初始化矩阵" class="headerlink" title="构建一个随机初始化矩阵"></a>构建一个随机初始化矩阵</h3><pre><code>x = torch.rand(5, 3)print(x)</code></pre><h3 id="构造一个填充零且dtype-long的矩阵，长整形"><a href="#构造一个填充零且dtype-long的矩阵，长整形" class="headerlink" title="构造一个填充零且dtype long的矩阵，长整形"></a>构造一个填充零且dtype long的矩阵，长整形</h3><pre><code>x = torch.zeros(5, 3, dtype=torch.long)print(x)</code></pre><h3 id="从数据构造张量"><a href="#从数据构造张量" class="headerlink" title="从数据构造张量"></a>从数据构造张量</h3><pre><code>x = torch.tensor([5.5, 3])print(x)</code></pre><h3 id="基于现有张量创建张量。这些方法将重用输入张量的属性，例如dtype，除非用户提供新值"><a href="#基于现有张量创建张量。这些方法将重用输入张量的属性，例如dtype，除非用户提供新值" class="headerlink" title="基于现有张量创建张量。这些方法将重用输入张量的属性，例如dtype，除非用户提供新值"></a>基于现有张量创建张量。这些方法将重用输入张量的属性，例如dtype，除非用户提供新值</h3><pre><code>x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizesprint(x)x = torch.randn_like(x, dtype=torch.float)    # override dtype!print(x)                                     # result has the same size</code></pre><h3 id="矩阵大小函数"><a href="#矩阵大小函数" class="headerlink" title="矩阵大小函数"></a>矩阵大小函数</h3><pre><code>print(x.size())#torch.size 为一个元组</code></pre><h2 id="矩阵基本运算"><a href="#矩阵基本运算" class="headerlink" title="矩阵基本运算"></a>矩阵基本运算</h2><h3 id="加法"><a href="#加法" class="headerlink" title="加法"></a>加法</h3><h4 id="方法1"><a href="#方法1" class="headerlink" title="方法1"></a>方法1</h4><pre><code>y = torch.rand(5, 3)print(x + y)</code></pre><h4 id="方法2"><a href="#方法2" class="headerlink" title="方法2"></a>方法2</h4><pre><code>print(torch.add(x, y))</code></pre><h3 id="加法：提供输出张量作为参数"><a href="#加法：提供输出张量作为参数" class="headerlink" title="加法：提供输出张量作为参数"></a>加法：提供输出张量作为参数</h3><pre><code>result = torch.empty(5, 3)torch.add(x, y, out=result)print(result)</code></pre><h3 id="地板加"><a href="#地板加" class="headerlink" title="地板加"></a>地板加</h3><pre><code># adds x to yy.add_(x)print(y)</code></pre><p>注意<br>任何使张量就地变化的操作都用固定<em>。例如：x.copy</em>(y)，x.t_()，将改变x。</p><h2 id="矩阵索引"><a href="#矩阵索引" class="headerlink" title="矩阵索引"></a>矩阵索引</h2><pre><code>print(x[:, 1]) #索引第二列，所有矩阵从0算起冒号表示范围 如1:3索引第二行到第三行</code></pre><h2 id="矩阵大小调整"><a href="#矩阵大小调整" class="headerlink" title="矩阵大小调整"></a>矩阵大小调整</h2><pre><code>x = torch.randn(4, 4)y = x.view(16)z = x.view(-1, 8)  # the size -1 is inferred from other dimensionsprint(x.size(), y.size(), z.size())</code></pre><p>输出结果<br>torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</p><h3 id="特殊情况，只有一个元素"><a href="#特殊情况，只有一个元素" class="headerlink" title="特殊情况，只有一个元素"></a>特殊情况，只有一个元素</h3><pre><code>如果您具有一个元素张量，请使用.item()将该值作为Python数字获取</code></pre><h2 id="与numpy之间转换"><a href="#与numpy之间转换" class="headerlink" title="与numpy之间转换"></a>与numpy之间转换</h2><h3 id="torch转numpy"><a href="#torch转numpy" class="headerlink" title="torch转numpy"></a>torch转numpy</h3><pre><code>a = torch.ones(5) #元素全为1矩阵创建print(a)</code></pre><p>tensor([1., 1., 1., 1., 1.])</p><pre><code>b = a.numpy()print(b)</code></pre><p>[1. 1. 1. 1. 1.]</p><pre><code>a.add_(1)print(a)print(b)</code></pre><p>tensor([2., 2., 2., 2., 2.])<br>[2. 2. 2. 2. 2.]</p><h3 id="numpy转torch"><a href="#numpy转torch" class="headerlink" title="numpy转torch"></a>numpy转torch</h3><pre><code>import numpy as npa = np.ones(5)b = torch.from_numpy(a)np.add(a, 1, out=a)print(a)print(b)</code></pre><h2 id="CUDA张量"><a href="#CUDA张量" class="headerlink" title="CUDA张量"></a>CUDA张量</h2><h3 id="使用该-to方法可以将张量移动到任何设备上。"><a href="#使用该-to方法可以将张量移动到任何设备上。" class="headerlink" title="使用该.to方法可以将张量移动到任何设备上。"></a>使用该.to方法可以将张量移动到任何设备上。</h3><pre><code># let us run this cell only if CUDA is available# We will use ``torch.device`` objects to move tensors in and out of GPUif torch.cuda.is_available():     device = torch.device(&quot;cuda&quot;)          # a CUDA device object     y = torch.ones_like(x, device=device)  # directly create a tensor on GPU     x = x.to(device)                       # or just use strings ``.to(&quot;cuda&quot;)``     z = x + y     print(z)     print(z.to(&quot;cpu&quot;, torch.double))       # ``.to`` can also change dtype together!</code></pre>]]></content>
      
      
      <categories>
          
          <category> 《炼金术士修炼手册》 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mark Down语法从入门到放弃</title>
      <link href="2020/04/16/Mark-Down%E8%AF%AD%E6%B3%95%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%83/"/>
      <url>2020/04/16/Mark-Down%E8%AF%AD%E6%B3%95%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%83/</url>
      
        <content type="html"><![CDATA[<h2 id="一、标题"><a href="#一、标题" class="headerlink" title="一、标题"></a>一、标题</h2><p>1.使用#表示标题，其中#号必须在行首</p><h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><h3 id="三级标题"><a href="#三级标题" class="headerlink" title="三级标题"></a>三级标题</h3><h4 id="四级标题"><a href="#四级标题" class="headerlink" title="四级标题"></a>四级标题</h4><h5 id="五级标题"><a href="#五级标题" class="headerlink" title="五级标题"></a>五级标题</h5><h6 id="六级标题"><a href="#六级标题" class="headerlink" title="六级标题"></a>六级标题</h6><span id="more"></span><p>2.或者用’=====’ ‘——‘表示</p><h1 id="一级标题-1"><a href="#一级标题-1" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题-1"><a href="#二级标题-1" class="headerlink" title="二级标题"></a>二级标题</h2><hr><h2 id="二、分割线"><a href="#二、分割线" class="headerlink" title="二、分割线"></a>二、分割线</h2><p>使用三个以上’-‘或‘ * ’ 表示</p><h2 id="三、斜体和粗体"><a href="#三、斜体和粗体" class="headerlink" title="三、斜体和粗体"></a>三、斜体和粗体</h2><p>使用‘ <em> ’和‘</em> ’分别表示斜体和粗体<br><em>斜体</em> <strong>粗体</strong> <strong><em>又斜有粗</em></strong></p><h2 id="四、超链接和图片"><a href="#四、超链接和图片" class="headerlink" title="四、超链接和图片"></a>四、超链接和图片</h2><p>写法1：[第一种写法]（www.baidu.com)<br>写法2：<a href="www.baidu.com">第二种写法</a></p><p>图片插入：在超链接前+一个‘！’<br><img src="http://i2.tiimg.com/716502/c981609de1b23f6e.jpg" alt="echo"></p><h2 id="五、无序列表"><a href="#五、无序列表" class="headerlink" title="五、无序列表"></a>五、无序列表</h2><p>使用‘+ ’ ‘- ’ ‘* ’</p><ul><li>一层<ul><li>两层<ul><li>三层<ul><li>四层</li></ul></li></ul></li></ul></li></ul><h2 id="六、有序列表"><a href="#六、有序列表" class="headerlink" title="六、有序列表"></a>六、有序列表</h2><p>使用‘ 1. ’点号后有空格</p><ol><li>你在第一层<ol><li>我在第五层</li></ol></li></ol><h2 id="七、文字引用"><a href="#七、文字引用" class="headerlink" title="七、文字引用"></a>七、文字引用</h2><p>使用‘&gt; ’ 表示</p><blockquote><p>第一层</p><blockquote><p>第二层</p><blockquote><p>第三层</p></blockquote></blockquote></blockquote><h2 id="八、行内代码块"><a href="#八、行内代码块" class="headerlink" title="八、行内代码块"></a>八、行内代码块</h2><p>使用`表示<code>\</code>为转义字符</p><h2 id="九、代码块"><a href="#九、代码块" class="headerlink" title="九、代码块"></a>九、代码块</h2><p>使用四个空格缩进表示代码块</p><pre><code>str=&#39;hello world&#39;for i in range(11):     print(str[i])</code></pre><h2 id="十、表格"><a href="#十、表格" class="headerlink" title="十、表格"></a>十、表格</h2><p>第二行表示对齐方式<br><code>|---|---:|:---:|</code><br>默认左对齐</p><div class="table-container"><table><thead><tr><th>X</th><th>Y</th><th>与</th><th style="text-align:right">或</th><th style="text-align:center">非X</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td style="text-align:right">0</td><td style="text-align:center">1</td></tr><tr><td>0</td><td>1</td><td>0</td><td style="text-align:right">1</td><td style="text-align:center">1</td></tr><tr><td>1</td><td>0</td><td>0</td><td style="text-align:right">1</td><td style="text-align:center">0</td></tr><tr><td>1</td><td>1</td><td>1</td><td style="text-align:right">1</td><td style="text-align:center">0</td></tr></tbody></table></div><h2 id="十一、流程图"><a href="#十一、流程图" class="headerlink" title="十一、流程图"></a>十一、流程图</h2><p>主要的语法为 <code>name=&gt;type: describe，</code>其中 type 主要有以下几种：<br>1.开始和结束：<code>start end</code><br>2.输入输出：<code>inputoutput</code><br>3.操作：<code>operation</code><br>4.条件：<code>condition</code><br>5.子程序：<code>subroutine</code></p><script type="math/tex; mode=display">st=>start: Startio=>inputoutput: input your nuberop=>operation: operatecond=>condition: right or nosub=>subroutine: you choicee=>endst->io->op->condcond(yes)->econd(no)->sub->io</script><h2 id="十二、数学公式"><a href="#十二、数学公式" class="headerlink" title="十二、数学公式"></a>十二、数学公式</h2><p>使用 $ 表示，其中一个 $ 表示在行内，两个 $ 表示独占一行。</p><p>`eg : $\sum_{i=1}^n a_i=0$</p><script type="math/tex; mode=display">\sum_{i=1}^n a_i=0</script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>“Mark dowm语法从入门到放弃”</title>
      <link href="2020/04/14/%E2%80%9C%E4%B8%8D%E7%9F%A5%E9%81%93%E8%AF%A5%E5%86%99%E4%BB%80%E4%B9%88%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87blog%E2%80%9D/"/>
      <url>2020/04/14/%E2%80%9C%E4%B8%8D%E7%9F%A5%E9%81%93%E8%AF%A5%E5%86%99%E4%BB%80%E4%B9%88%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87blog%E2%80%9D/</url>
      
        <content type="html"><![CDATA[<h2 id="一、标题"><a href="#一、标题" class="headerlink" title="一、标题"></a>一、标题</h2><p>1.使用#表示标题，其中#号必须在行首</p><h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><h3 id="三级标题"><a href="#三级标题" class="headerlink" title="三级标题"></a>三级标题</h3><h4 id="四级标题"><a href="#四级标题" class="headerlink" title="四级标题"></a>四级标题</h4><h5 id="五级标题"><a href="#五级标题" class="headerlink" title="五级标题"></a>五级标题</h5><h6 id="六级标题"><a href="#六级标题" class="headerlink" title="六级标题"></a>六级标题</h6><p>2.或者用===== ——表示</p><h1 id="一级标题-1"><a href="#一级标题-1" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题-1"><a href="#二级标题-1" class="headerlink" title="二级标题"></a>二级标题</h2><h2 id="二、分割线"><a href="#二、分割线" class="headerlink" title="二、分割线"></a>二、分割线</h2><h2 id="使用三个以上———或-表示"><a href="#使用三个以上———或-表示" class="headerlink" title="使用三个以上———或*表示"></a>使用三个以上———或*表示</h2>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
